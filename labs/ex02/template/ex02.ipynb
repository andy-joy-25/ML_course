{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_loss` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE\n",
    "    # ***************************************************\n",
    "    \n",
    "    e = y - np.matmul(tx,w)\n",
    "    mse = 0.5*(1/y.shape[0])*np.matmul(e.T,e)\n",
    "    return mse\n",
    "    #return (0.5*(1/y.shape[0])*np.matmul((y - np.matmul(tx,w)).T,(y - np.matmul(tx,w)))).item() # MSE\n",
    "    #return (1/y.shape[0])*np.sum(np.abs(y - np.matmul(tx,w))) #MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "\n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    # ***************************************************\n",
    "    for i in range(grid_w0.shape[0]):\n",
    "        for j in range(grid_w1.shape[0]):\n",
    "            losses[i,j] = compute_loss(y,tx,np.array([grid_w0[i],grid_w1[j]]))\n",
    "            \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=42.42448314678246, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.021 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA14AAAITCAYAAAAXac30AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAADJqUlEQVR4nOzdeVyVZf7/8ddhVVE0LUWSymom21SyhigtK9PMGpsys2y3nAprhBa1xE5BqZVLpem0at+01LZf02KRWmqilmFjjjktTmqGVqYIKhzg/P64us/GAQ/rWXg/Hw8eB8593TfXuUU9Hz6f63PZnE6nExEREREREWk0UcGegIiIiIiISKRT4CUiIiIiItLIFHiJiIiIiIg0MgVeIiIiIiIijUyBl4iIiIiISCNT4CUiIiIiItLIFHiJiIiIiIg0MgVeIiIiIiIijUyBl4iIiIiISCNT4CUiIiIiItLIwirwWr58OZdeeinJycnYbDbefvttr+M33ngjNpvN6+Oiiy7yGrN7926GDx9OYmIi7dq1Y8SIERQXFzfhqxARaX5mzZpF9+7dSUxMJDExkfT0dD744APA/Lt85513csIJJ9CyZUuOOuoo7rrrLvbu3et1ja1btzJo0CBatWpFx44duffeeykvL/ca88knn3DaaacRHx/P8ccfz5w5c6rMZebMmRxzzDG0aNGCtLQ01q5d22ivW0RExBJWgVdJSQk9evRg5syZ1Y656KKL+Pnnn10fr776qtfx4cOHs3HjRvLy8nj33XdZvnw5I0eObOypi4g0a126dGHSpEmsW7eOL774gvPPP5/BgwezceNGduzYwY4dO3jiiSf4+uuvmTNnDosXL2bEiBGu8ysqKhg0aBBlZWWsWrWKuXPnMmfOHCZMmOAas2XLFgYNGsR5553H+vXrGT16NLfccgsffviha8yCBQvIysriwQcf5Msvv6RHjx4MGDCAXbt2Nen9EBGR5sfmdDqdwZ5EXdhsNt566y0uu+wy13M33ngje/bsqZIJs2zatImTTjqJzz//nNNPPx2AxYsXc/HFF7N9+3aSk5ObYOYiIgLQvn17Hn/8ca8Ay7Jo0SKuvfZaSkpKiImJ4YMPPuCSSy5hx44ddOrUCYDZs2czZswYfvnlF+Li4hgzZgzvvfceX3/9tes6w4YNY8+ePSxevBiAtLQ0zjjjDGbMmAFAZWUlKSkp3HnnnYwdO7YJXrWIiDRXMcGeQEP75JNP6NixI4cddhjnn38+ubm5dOjQAYD8/HzatWvnCroA+vXrR1RUFGvWrOFvf/ub32uWlpZSWlrq+rqyspLdu3fToUMHbDZb474gEWl2nE4n+/btIzk5maio+hUmHDx4kLKysgaamTen01nl38D4+Hji4+NrPK+iooJFixZRUlJCenq63zF79+4lMTGRmBjz31R+fj6nnnqqK+gCGDBgALfffjsbN24kNTWV/Px8+vXr53WdAQMGMHr0aADKyspYt24d48aNcx2PioqiX79+5OfnB/y6Q1FlZSU7duygTZs2+n9JRKSJBfr/dkQFXhdddBGXX345Xbt25fvvv+f+++9n4MCB5OfnEx0dTWFhIR07dvQ6JyYmhvbt21NYWFjtdSdOnMhDDz3U2NMXEfGybds2unTpUufzDx48SJeWLfmtAefkqXXr1lXWyD744IPY7Xa/4zds2EB6ejoHDx6kdevWvPXWW5x00klVxv3666/k5OR4lYEXFhZ6BV2A62vr3+/qxhQVFXHgwAF+//13Kioq/I755ptvAnvRIWrHjh2kpKQEexoiIs3aof7fjqjAa9iwYa7PTz31VLp3785xxx3HJ598wgUXXFDn644bN46srCzX13v37uWoo45i22BIvLdeUz6k9089v3G/gR8vcFOTf89AffzZX4M9BQlh/c5+J9hTqNEIXgpo3P6ickakLKdNmzb1+n5lZWX8BrwJJNTrSlWVAJcXF7Nt2zYSExNdz9eU7TrhhBNYv349e/fu5fXXX+eGG27g008/9Qq+ioqKGDRoECeddFK1AZxUZf2s+P55BMrhcPDRRx/Rv39/YmNjG3p6zYLuYcPQfaw/3cP6q+09LCoqIiUl5ZD/b0dU4OXr2GOP5fDDD+e7777jggsuICkpqcoC6vLycnbv3k1SUlK116mudCbxXkhs3eDTdnmnR39aNd7l/ZrN3wnVv6IfLL+84d89SkT5eP21DDznzWBPo1ovk8Ft/DPg8Q1VMpZA4/3VsboUBiIuLo7jjz8egF69evH555/z5JNP8s9/mnuyb98+LrroItq0acNbb73l9Z9dUlJSle6DO3fudB2zHq3nPMckJibSsmVLoqOjiY6O9jumpv8DwoH1s1KbPw9PDoeDVq1akZiYqDdqdaR72DB0H+tP97D+6noPD/X/dlh1Nayt7du389tvv9G5c2cA0tPT2bNnD+vWrXONWbp0KZWVlaSlpQVrmhKAD5ZfHuwpSJjQz0r4qKysdK2fLSoqon///sTFxfHOO+/QokULr7Hp6els2LDB65dneXl5JCYmujJm6enpLFmyxOu8vLw81zqyuLg4evXq5TWmsrKSJUuWVLvWTEREpKGEVeBVXFzM+vXrWb9+PWBaB69fv56tW7dSXFzMvffey+rVq/nf//7HkiVLGDx4MMcffzwDBgwA4MQTT+Siiy7i1ltvZe3atXz22WeMGjWKYcOGhVxHw3d69G/y7zmbvzf59wyE3khLbYXyz0yo/j1rbOPGjWP58uX873//Y8OGDYwbN45PPvmE4cOHu4KukpISXnjhBYqKiigsLKSwsJCKigoA+vfvz0knncR1113HV199xYcffsj48ePJyMhwVSTcdttt/PDDD9x333188803PPPMMyxcuJDMzEzXPLKysnjuueeYO3cumzZt4vbbb6ekpISbbgrdEmsREYkMYVVq+MUXX3Deeee5vrbWXd1www3MmjWLf//738ydO5c9e/aQnJxM//79ycnJ8SoTnDdvHqNGjeKCCy4gKiqKK664gqeeeqrJX0uoCdU3g6H8Blqkrmbz91qVHEaCXbt2cf311/Pzzz/Ttm1bunfvzocffsiFF17IJ598wpo1awBcpYiWLVu2cMwxxxAdHc27777L7bffTnp6OgkJCdxwww08/PDDrrFdu3blvffeIzMzkyeffJIuXbrw/PPPu375BnDVVVfxyy+/MGHCBAoLC+nZsyeLFy+u0nBDRESkoYVV4NW3b19q2nbMc5PM6rRv35758+c35LREJAR9sPzykF7v1dy88MIL1R471L/tlqOPPpr333+/xjF9+/aloKCgxjGjRo1i1KhRh/x+IiIiDSmsSg2bi6YuM1S2SyJVKP8MherfOxEREWkcCrwkJIXyG2YJL6H8s6TgS0REpPlQ4BVilO0K7TfKEp70MyUiIiLBpsBLQoreIEtjCdWfrVD85YeIiIg0PAVeIUTZLpHGpeBLREREgkWBVzMVim/0QvVNsUQW/ZyJiIhIMCjwChHB2DA5lOjNsDSlUPx5C8VfhoiIiEjDUeDVDIXaG7xQfBMskS8Uf+5C7e+miIiINBwFXiGgOWe7QvHNrzQf+vkTERGRpqLAq5nRb9RFvIVa8KW/oyIiIpFJgVeQNWW2K9Te0IXaG15pvkLtZ/EFbgr2FERERKSBKfCSoAi1N7ohz/7HhzQa/UyKiIhIY4oJ9gSas+aa7dIb3ADZA3wukGMSkA+WX87Ac94M9jRERESkKTkcEBvb6N9GgZc0KQVdh2BvpHPrc91mRsGXiIhIM/LTT9CnD+TmwjXXNOq3UuDVDIRKtktBlx/2EPk+hzrezCj4EhERaQYcDrjqKtiyBR5/HK68slEzXwq8gqSpygxDJegSD/ZgT8APex2PRTAFXyIiIhHu/vvhs88gMRFef73Ryw0VeEmTaPbZLnuwJ1APdp9HERERkXD39tvwxBPm8zlz4LjjGv1bqqthEDS3bFezDbrsRFY3QnuwJ9D0mu3ProiISCT74Qe48UbzeVYW/O1vTfJtFXhJo2p2b1ztRFaw5ctO5L62ajS7n2EREZEIlZ0NHRIOsuPsIbB3L5x1Fkya1GTfX6WGTaw5ZbuazRtWe7AnEAR2n0cRERGREDdtGjyxfzTJ+wvg8MNhwYImaSNvUcYrAoVC0BXx7DTL7E8VdprFPWg2v0QQERGJYC/1m8dt/JNKbDBvHnTp0qTfX4FXE2rKDZODLSLfqNppNoFGrdmDPYHGF5E/0yIiIs3Ff/7DlXkjAYiakA39m/59uQKvCBMK2a6IfINqD/YEwoAd3SdplpYvX86ll15KcnIyNpuNt99+23XM4XAwZswYTj31VBISEkhOTub6669nx44dXtfYvXs3w4cPJzExkXbt2jFixAiKi4ub+JWIiESo4mIYMgT274d+/WDChKBMQ4FXE2ku2a6IC7rsKJioLTsRe88i7udbGkRJSQk9evRg5syZVY7t37+fL7/8kuzsbL788kvefPNNNm/ezF//+levccOHD2fjxo3k5eXx7rvvsnz5ckaOHNlUL0FEJHI5nfD3v8OmTZCcbEoMo6ODMhU114ggwc52RdybUnuwJxDm7ETkPdTGyuJr4MCBDBw40O+xtm3bkpeX5/XcjBkz+Mtf/sLWrVs56qij2LRpE4sXL+bzzz/n9NNPB+Dpp5/m4osv5oknniA5ObnRX4OISMT65z9h/nwTbC1YAB07Bm0qyng1gabIdgU76Io49mBPIELY0b0U8bF3715sNhvt2rUDID8/n3bt2rmCLoB+/foRFRXFmjVrgjRLEZEIsG4d/OMf5vNJk6B376BORxkvaRARle2yB3sCEcju8xjmlPWSujp48CBjxozh6quvJjExEYDCwkI6+vwGNiYmhvbt21NYWOj3OqWlpZSWlrq+LioqAsyaMofDUet5WefU5VwxdA8bhu5j/eke/uH334kZMgRbWRmVf/0rFXfdBQHek9rew0DHKfBqZM0h2xUxQZc92BNoBuxEzH1W8CW15XA4GDp0KE6nk1mzZtXrWhMnTuShhx6q8vxHH31Eq1at6nxd37JIqT3dw4ah+1h/zfoeOp38ZeJEOv/vf5R06sQnQ4dS/sEHtb5MoPdw//79AY1T4CX1oqBLas3u8yjSDFhB148//sjSpUtd2S6ApKQkdu3a5TW+vLyc3bt3k5SU5Pd648aNIysry/V1UVERKSkp9O/f3+vatZlfXl4eF154IbFNuJloJNE9bBi6j/WnewhRU6YQvXYtzvh44t55h/6pqbU6v7b30Ko6OBQFXmEumNkuBV1SL3afxzCkrJcEwgq6vv32W5YtW0aHDh28jqenp7Nnzx7WrVtHr169AFi6dCmVlZWkpaX5vWZ8fDzx8fFVno+Nja3XG636ni+6hw1F97H+mu09XLECxo8HwPbkk8T+5S91vlSg9zDQ+6zAqxFFcgv5iAi67MGegABhX36o4EuKi4v57rvvXF9v2bKF9evX0759ezp37syQIUP48ssveffdd6moqHCt22rfvj1xcXGceOKJXHTRRdx6663Mnj0bh8PBqFGjGDZsmDoaiojUxs6dcNVVUFEBw4dDiG3LocArjAV7bVdYswd7AuLF7vMoEka++OILzjvvPNfXVgngDTfcgN1u55133gGgZ8+eXuctW7aMvn37AjBv3jxGjRrFBRdcQFRUFFdccQVPPfVUk8xfRCQiVFTANdfAzz/DSSeZNvI2W7Bn5UWBVyNp7GyXSgzrwR7sCUi17D6PYUJZr+atb9++OJ3Oao/XdMzSvn175s+f35DTEhFpXh56CJYuhYQEeP118xhitI+X1EpYB112wu4NfbNlD/YEai+s/26IiIiEs8WLITfXfP7ss3DiicGdTzUUeDWCSM52hS17sCcgtWZHf24iIiJSs23b4NprwemE22835YYhSoGXBCxsf6NvD/YEpF7swZ5A4ML274iIiEg4KiuDoUPht9+gVy+YNi3YM6qRAq8GFqnZrrB9Q2kP9gSkQdiDPYHAhe3fFRERkXAzZgysXg3t2sGiReBnm41QosBLIpOdsHqzLgGwB3sCIiIi0liys6F1a/MYkDfegOnTzedz50LXro01tQajwKsBKdsVIuzBnoA0GnuwJxCYsPs7IyIiEmTTpkFJSYDVgt9+CzfdZD6/916yP/9r7YK2IFHgJZHFHuwJSKOzB3sCgVHwJSIiErjMTNMB/o+tEKt34AAMGQL79kHv3vDII7UL2oJIgVcDUbYryOyEzRtyaQD2YE9AREREGlJODhQXw8MPH6Ls8M474d//hiOOgNdeg9hYv0FbrUsXm4ACL6lWWAVdIiEobP4OiYiIhJBqM1hz58ILL4DNBvPnw5FHAt5B2yGvEUQKvBpApGa7woI92BOQoLEHewKBUfAlIiJSO37LDjdsoOyW2wFYcs5D0K9f7a8RZAq8xK+weLNoD/YEJOjswZ6AiIiINLQqGax9++DKK4krP8BiBnDZ5w/U/hohQIGXhB87esMtbvZgT+DQwuIXGSIiIiHEtUZrvBNuvRU2b2Zvmy78vdUrZN4dniFMeM46hERimWFIv0m0B3sCEpLswZ7AoYX03ysREZEG0lBNLaw1WiWPPwMLFkBMDG0/XMiPJYeHVBarNhR4iUhksAd7AiIiIs1HdQGWv6YWdQnGMjPhnBZreaw80zzx2GOQnl7/iQeRAq96eP/U8xv1+sp2+bAHewIS8uzBnkDNQvrvVyObOHEiZ5xxBm3atKFjx45cdtllbN682WtMYWEh1113HUlJSSQkJHDaaafxxhtveI3ZvXs3w4cPJzExkXbt2jFixAiKi4u9xvz73/+mT58+tGjRgpSUFB577LEq81m0aBHdunWjRYsWnHrqqbz//vsN/6JFRCKYb4BlBVepqVWbWtSlw2BO5m7ejh9KTKWDjd0uh9GjG3T+waDAS8KDPdgTEJH6+PTTT8nIyGD16tXk5eXhcDjo378/JSUlrjHXX389mzdv5p133mHDhg1cfvnlDB06lIKCAteY4cOHs3HjRvLy8nj33XdZvnw5I0eOdB0vKiqif//+HH300axbt47HH38cu93Os88+6xqzatUqrr76akaMGEFBQQGXXXYZl112GV9//XXT3AwRkQjg2zXQCq4KCqo2tah1h8HKSrj+eg7b+yPfcjwXbn3RtJAPcwq8xKU5/zZeIog92BMQfxYvXsyNN97IySefTI8ePZgzZw5bt25l3bp1rjGrVq3izjvv5C9/+QvHHnss48ePp127dq4xmzZtYvHixTz//POkpaXRu3dvnn76aV577TV27NgBwLx58ygrK+PFF1/k5JNPZtiwYdx1111MnTrV9X2efPJJLrroIu69915OPPFEcnJyOO2005gxY0bT3hQRkTDm2zWwpuCqug6D1ZYgTp4M772HIzqe61ss4pa72zbKa2hqCrxClPbu8mAP9gQk7NiDPYHqRdovOIqKirw+SktLAzpv7969ALRv39713FlnncWCBQvYvXs3lZWVvPbaaxw8eJC+ffsCkJ+fT7t27Tj99NNd5/Tr14+oqCjWrFnjGnPOOecQFxfnGjNgwAA2b97M77//7hrTz2f/lwEDBpCfn1/7GyAiIkDd2rf7LUH85BMYPx6A2NkzyD/QM2ybafiKCfYERESkcZ05BBJjG/aaRQ7gdUhJSfF6/sEHH8Rut9d4bmVlJaNHj+bss8/mlFNOcT2/cOFCrrrqKjp06EBMTAytWrXirbfe4vjjjwfMGrCOHTt6XSsmJob27dtTWFjoGtO1a1evMZ06dXIdO+ywwygsLHQ95znGuoaIiDSNzEwTdKWm/pH5uuVnxrw2zFVqyIgR9bp+dra5fmamCQyDTRkvAUL4t/D2YE9AwpY92BOoXsj+fauDbdu2sXfvXtfHuHHjDnlORkYGX3/9Na+99prX89nZ2ezZs4ePP/6YL774gqysLIYOHcqGDRsaa/oiIhJEVpasoAAOlpRz1tNXw86dcPLJ8Mwz9V7XVZemHo1JGa8QpDLDP9iDPQEJe3b0c9TIEhMTSUxMDHj8qFGjXE0xunTp4nr++++/Z8aMGXz99decfPLJAPTo0YMVK1Ywc+ZMZs+eTVJSErt27fK6Xnl5Obt37yYpKQmApKQkdu7c6TXG+vpQY6zjIiLStDIzIXHyBPo4PjWprzfeMAvGGuC606bVoqlHI1PGSyLqt+8i4aK5/b1zOp2MGjWKt956i6VLl1YpB9y/fz8AUVHe/y1FR0dTWVkJQHp6Onv27PFqyLF06VIqKytJS0tzjVm+fDkOh8M1Ji8vjxNOOIHDDjvMNWbJkiVe3ycvL4/0MN8fRkQkXOWc+R73OiaaL55/Hk44oWGuW4d1Z41JgZeEJnuwJyARwx7sCQiY8sJXXnmF+fPn06ZNGwoLCyksLOTAgQMAdOvWjeOPP56///3vrF27lu+//54pU6aQl5fHZZddBsCJJ57IRRddxK233sratWv57LPPGDVqFMOGDSM5ORmAa665hri4OEaMGMHGjRtZsGABTz75JFkev+78xz/+weLFi5kyZQrffPMNdrudL774glGjRjX5fRERafZ+/BGuuw6Af8ZkkP31VUGeUONR4BVimrrMsLn91l2aKXuwJ+Bfc/r7N2vWLPbu3Uvfvn3p3Lmz62PBggUAxMbG8v7773PEEUdw6aWX0r17d15++WXmzp3LxRdf7LrOvHnz6NatGxdccAEXX3wxvXv39tqjq23btnz00Uds2bKFXr16cffddzNhwgSvvb7OOuss5s+fz7PPPkuPHj14/fXXefvtt70afYiISBMoLYWhQ+H33/ki6gzuKp8SMuuxGoPWeEnosQd7AiLS0JxO5yHH/OlPf+KNN96ocUz79u2ZP39+jWO6d+/OihUrahxz5ZVXcuWVVx5yTiIizVWTdAS85x5YuxYOO4xPhy8k9qV4V4fD6r5vqHUqrA1lvCS02IM9AYlY9mBPwL/mlPUSEZHw4dkR0HOj42o3Pa5GteMXLABr4/qXX+bup49xdTisqRNhqHUqrA0FXiFEZYYijcwe7AmIiIiEh8xM01gwK8s72Akk8PEMtvyNf/KOzewbdov5YuxYuOQSr+8bG2uqEPv0qRq0ec4r3CjwktBhD/YEpFmwB3sCVemXICIiEmo8OwJ6Bju+gY+/jJZnsJWaap6zHtm/n36zh9CGYlZEnQs5OWRnm2ArLs4MiYuD8nJYubJq0BZqnQprQ4FXM6U3eiIiIiISiJwcE3BNnWq+9gx8/GW0PIOzggLznPVIRgYnO79mp60T+Xe9CjExTJtmAi2Hw71+KyEBevcO3+yWPwq8QkSz3zTZHuwJSLNiD/YEqtIvQ0REJJT4ZrKqKzH0V/pXXbaMF1+EOXMgKopOS17lvmmdXdeIiTFZr6ws9/krVoRvdssfBV7NkN7giRCSwZeIiEio8A20qltbZQVJTqf/JhquIOyKryAjAwB7dA7ZS8/zGuNwQFlZ5ARZ/ijwkuCzB3sCIqFBvxQREZFgszJdqanegdah1lbV2HRj714YMgQOHmRx9MU87Bgbll0J60uBVwho1mWG9mBPQJo1e7AnICIiElqsAKqgIPAyv+xs04XQKhX04nTCiBHw3Xdw1FE8ffrLOIlyN9vwuU5tWtWHm7AKvJYvX86ll15KcnIyNpuNt99+2+u40+lkwoQJdO7cmZYtW9KvXz++/fZbrzG7d+9m+PDhJCYm0q5dO0aMGEFxcXETvorg0m/URXzYgz0Bb/o7KiIiwVSXdu1Wc4y4OD+B2lNPwRtvUEYs/7xgIR+t6wDA6tVVrzN5sgn6Jk+u+/xDWVgFXiUlJfTo0YOZM2f6Pf7YY4/x1FNPMXv2bNasWUNCQgIDBgzg4MGDrjHDhw9n48aN5OXl8e6777J8+XJGjhzZVC9BPNmDPQERERER8RRou3bP7FS1wdrq1XDPPQDczRTuXpiGzWYOWY+enE7vx0gTVoHXwIEDyc3N5W9/+1uVY06nk+nTpzN+/HgGDx5M9+7defnll9mxY4crM7Zp0yYWL17M888/T1paGr179+bpp5/mtddeY8eOHU38aoymLDPUb9JFqmEP9gS8ffzZX4M9BRERkRp5runyG6z99hsMHQrl5Xx94pW81GoUWVkwZowJ0saOrVpaOHasOTZuXFBeUqMLq8CrJlu2bKGwsJB+/fq5nmvbti1paWnk5+cDkJ+fT7t27Tj99NNdY/r160dUVBRr1qyp9tqlpaUUFRV5fUg92YM9gTCzrPqfT2kg9mBPQEREJHzUuJFyZSVcdx1s2wZ/+hOnrH6e4hIbDz/sHaRV15BDGa8QV1hYCECnTp28nu/UqZPrWGFhIR07dvQ6HhMTQ/v27V1j/Jk4cSJt27Z1faSkpDTw7BtfSGW77MGeQBhZtsYddFmfe36IiIiI1JEVLPXp45158tfkwvc5K4BatsyUDT7yiEcQNXEifPABtGgBr79O9uOJrnNrKlGssTNiNfMKJxETeDWmcePGsXfvXtfHtm3bGuS6zbqbodQs0MBKwVjDsgd7AiIiIk3HCnRWrvQOeKwgKjf30Bsor1xpHp1OE0Q9/belMGGCeXLWLOje3etc69qPPFK1RPFQjT0OFZiFuogJvJKSkgDYuXOn1/M7d+50HUtKSmLXrl1ex8vLy9m9e7drjD/x8fEkJiZ6fUgd2YM9gRDXEMGTgrH6sQd7AiIiIk3jsMPMY5s23gGPZ6mfFeRY7d9928D37m0e+/SB4v/u4KaPrjalhjffTPb3N7r2BIuNNS3n/TXQsDJZUHNjj7p0XAwlERN4de3alaSkJJYsWeJ6rqioiDVr1pCeng5Aeno6e/bsYd26da4xS5cupbKykrS0tCafc1MJqTJDqaopAiR/wZgCMhERkYiXnW3avMfGVi3R277dPO7b5x3wWMGUzeYOcgoKvB+taxcUwPjxsHxpOVx9NezaBd27w4wZXnuCOZ2m5bylTx/354FmsgLtuBiqwirwKi4uZv369axfvx4wDTXWr1/P1q1bsdlsjB49mtzcXN555x02bNjA9ddfT3JyMpdddhkAJ554IhdddBG33nora9eu5bPPPmPUqFEMGzaM5OTkJn0tzbLM0B7sCYSYUAh+FIz5Zw/2BERERGpW3Tos37e006aBw2GCnmnTvM/zzFZ5XqOgwBxr1cqdmcrMhJgYKCurWn44aRJMaTUeli+niDZM6/06tGzplaGy2sfHxpprLl/unotnRixc128FIqwCry+++ILU1FRS/8hxZmVlkZqayoQ/6kjvu+8+7rzzTkaOHMkZZ5xBcXExixcvpkWLFq5rzJs3j27dunHBBRdw8cUX07t3b5599tmgvJ6moGxXCAr1AEeBmGEP9gRERET8y8426698s0RWIOTJKicEEwB5ZpdWrDBB0LnnuoOx6tZ95eSY4MnhcK/9sgKrS53vcLfD7Hp8My+S9cyfyM72zlB5tpH3ne+aNd7Boe9rDeeGGp5igj2B2ujbty/OGvpL2mw2Hn74YR6uIf/Yvn175s+f3xjTk5rYgz2BIAvnAMaa+3mRW44rIiISTjyDE8/1TpmZMHu291irnBBMAOR0mvM9z/MMxjIzYfJkEwiBCdzi4sx5FRXucx55xCzlyrl5CwdOugEqYGHyP3hjxxCvOVrXzMkxH54yM83x0lL/r8d3br7nh5uwynhFiqYqMwyZbJc92BMIokjKGkXK66gNe7AnICIiUpWVacrO9l7vlJMDO3aYz3Nz3Q0rwF3q52+dlGdJYE6OCbQs27e7s1HR0e7nnU6It5Xy+bFDaXlwD5x5JkO3POYqXzxwwJQgembNqmtJf+aZ5uvevc11PcfUpaFGqGbJFHiJNIZICrg8ReJrEhERCTOBNJl45hkT9MTGmsBl/PjqAxJ/bd192Wwwbhx06eJ+bipZnMEX/EZ7njhjAa3bx7F6tTlWWWnOsYImz/JIzzb14N24w7fRRl0aaoRq23kFXtK47MGeQBOL1IDLU6S/Pl/2YE9ARESak4bK1txxhwl6rMbdTqc7AzVpkv/va3U/BBOoxXgsSmrVygQ/v/9uvr6a+WTwDJXYeO/qedhfPIqSEhNsxcSY64wda4K4qVNN+aInz6DIM6vVEC3jQ7XtvAKvJtbsygybg+bYiKI5vVYREZFG5BtoNVS2Zvx4kykqKHBnmaw1Wjab9/ft08cct0oKc3PNOIfDHYDt328Cs9RUOCV6E88yEoCJtgd4bttFlJa6gy2Hw3Q/fPhh9+uxNlju3btqUOSZ1WqIlvGh2nZegZc0HnuwJ9DImluw5as5vX57sCcgIiKRyjfQ8s3WVNc2PpCsWJ8+3l0OrR51SUnusr9Jk0wHQ1+TJrnXiMXHm3MdDtj8ZQkLK4fQmhKWcD4TnHZWrjQBm8PhvTGy5+sZN84EQ337es+lOVHgFYFCIttlD/YEGlFzCjgCoXshIiJSZ76Blm+2xl8GLNCsmL+ACmDbNvfnVtMNS2ysea683J0pS021xjl5hts50fkfdtCZ4cynkmiv83NzTcBnBYY5Oe5yQyuzForrr5qCAq8m1Cw3TY4kCriq1xzuiz3YExARkUh0qLI4f+uVAsmKgXuDZF+9e5vywagoE2B58pe1WrnSPHcrzzFk//9RTjQ3tXiNW8d3co3xXA/muweY595gllBbf9UUFHhFGGW7GklzCCzqS/dImqnly5dz6aWXkpycjM1m4+233/Y67nQ6mTBhAp07d6Zly5b069ePb7/91mvM7t27GT58OImJibRr144RI0ZQXFzchK9CREKVv8AskKwYmA2SExLcX1tru/r2NeWDUVGBl/yl8iVPcRcA46MeJe3ec8jJMWvAEhJMS3grWxYVZR7Lyrw3WrbWd/m2wW8uFHiJHIoCisBFelbQHuwJSCgqKSmhR48ezJw50+/xxx57jKeeeorZs2ezZs0aEhISGDBgAAcPHnSNGT58OBs3biQvL493332X5cuXM3LkyKZ6CSISBA2511RqqvejJ8/W8E4nLFvmLverrAzs+smt9rCIK2lBKe9wKTNb3OMKnKwgcM0ad7asstK9Jsza+Li42ASCodj0oqko8GoizabM0B7sCTSwSA4iGpPumzQjAwcOJDc3l7/97W9VjjmdTqZPn8748eMZPHgw3bt35+WXX2bHjh2uzNimTZtYvHgxzz//PGlpafTu3Zunn36a1157jR3WTqgiEnECXadVU3MNay2VtXfW6tWQnGw+99xA2XMdl2e5XyCBV0oXJz/1v4nj+IEtHMMNzKXkQFSVgNG3ZNHiLxhsrmIOPUTCRdDLDO3B/fYNTsFD/SxbA+elBXsWDc9O5P2sS6PZsmULhYWF9OvXz/Vc27ZtSUtLIz8/n2HDhpGfn0+7du04/fTTXWP69etHVFQUa9as8RvQlZaWUlpa6vq6qKgIAIfDgcPhqPU8rXPqcq4YuocNozndx7vvNpsct2tn9shKT4fFi73H5ObClCnm89mzYcIE7+fWrTOPsbHu/bcqK829mzXLQWWlOS87Gx5/PLB52Wze5YfDf5kOb79NeXQcN8a/SqmzNS1weM3nmWegZUtzns1mXk9Zmcl4ffONeQwntf05DHScAi8RfxR0NYxIDb5EAlRYWAhAp06dvJ7v1KmT61hhYSEdO3b0Oh4TE0P79u1dY3xNnDiRhx56qMrzH330Ea1atarzfPPy8up8rhi6hw2jOdzH006D55/3fu7996uOefVV7+O+z1Xnuee872Eg5/g67Jtv6P3AAwBsHHEjWRfvBNyTtObj+zp8+b6ucBHoz+H+/fsDGqfAqwk0RZmhsl0NSEFXw4rE4MtOZP3MS9gZN24cWR4twYqKikhJSaF///4kJibW+noOh4O8vDwuvPBCYq1fm0ut6B42jEi9j7m57oxTQgJYVcTJye59ts46Cz744NDn5ebC9Okmu2St35oyxV02mJDg4Pnn87j55guJiop1fa+LLoL8fPe1jzwSfvrJfe127dxfAxzu/IX80gyinBUsjB7KjS8/Df/n3XveZjPdDJ1O06XQ6TSlk5WV7vkceSTs2QN33GEacYSD2v4cWlUHh6LAS0QanxXMRloAJnIISUlJAOzcuZPOnTu7nt+5cyc9e/Z0jdm1a5fXeeXl5ezevdt1vq/4+Hji4+OrPB8bG1uvN6v1PV90DxtKpNzH7GwTiJSWutdA3XOPKQvMzoa9e02wMm6caThhjbcCqmnToEMH2L4devUy51VWgvU+/9FHTXfCzEz45BOzfis93Rw7eDCW/ftjiYsz5zmd3uuwvvvO/XmvXt5rv6Ko4Dlu4kh+4htOYETF8xyoiKv2dSYkgN1u1pR5btjs+X2mTAE/iXqv+5SZaRpxhIpAfw4D/VlVc40IoGxXA1K2q3Hp/koz07VrV5KSkliyZInruaKiItasWUP6H++O0tPT2bNnD+usxRrA0qVLqaysJC1Nv6wQCWdWAw2bzbuNena2yVw5HCZwcjpN0DJpkhk/aZK78+D27eZaa9ZAXJx53lJR4W7QYTXYsLJanuu0HI6qGyV78gy6bDYYb3uEAXxECa24gjcopo3ruGeM0aWL935imZnu/cF81dRkI9BGI+FOgVcjazbdDCOBgoKmESn32R7sCUioKC4uZv369axfvx4wDTXWr1/P1q1bsdlsjB49mtzcXN555x02bNjA9ddfT3JyMpdddhkAJ554IhdddBG33nora9eu5bPPPmPUqFEMGzaMZKs9mYiEJWv/qrFj3W3UraDLkprqDrIqKsz4igr38d69TbDjcFRtUuF0miAnK6vmwMpmA3+/x/Hc48ty18kf86DTDsBtzOY/nOx1PM4j8bVzp3d7+JwcM8eKCjM3z9LCgoLq5+dvk+hIpMBL6sce7Ak0kEgJBsKF7rdEkC+++ILU1FRS//h1blZWFqmpqUyYMAGA++67jzvvvJORI0dyxhlnUFxczOLFi2nRooXrGvPmzaNbt25ccMEFXHzxxfTu3Ztnn302KK9HRBqOv82PPbM62dneAUlMjAlCrGxVVJTJZHkGXDabCcYslZUmQ1ZdO3cw1/PMall8ywKT+Yn7v76GKJw8y628wnVex9u0qbovWE08N1iuKajyd58ikdZ4hbmglhnag/etG5SCgOCIhKYbdiLn74HUWd++fXHW8O7DZrPx8MMP83AN7yjat2/P/PnzG2N6IhJiMjNN8JWVZQINp9METjabyYxNmuQeGxVVNaCKjoa+fb0DqYZo1x6DgwVcRUd+4UtSuYunqozZt8+9BmvaNJOti401cx8zxv/6rJyc0Fq3FUzKeDUilRmGAQVdwbVsjf4MRESkWcjONmV6kyaZ4Mu3PK+szARhnkGUv/46VoarLmJj/a+/stng5SPvpzefsYe2XMkiSmlRZWxKinvOxcUmW1debuY8aZL3ps6+GyyLAq+wpmxXPekNf+gI5z8Le7AnICIi4WDaNBOglJdXbSLRp48JfjzXfoG7sYanysqaywr96dLFPCYludu8e/qr822u/ukJAG7iJX7gOMC9IbJl9273XKOiTMYrJsad9SopMZm42jbKyM5uHsGaAi9pnsL5jX6k0p+JiIhEgOqCiMxME6DExLjXO1ljfddfWcGOlWGqLyuA27at6rGu/MAcbgTg2dZZvM3fXN/f6fRex5Wa6p6rtW7MaqSRlmbWcvXuXftGGepqKFIde7AnICIiIhKa/AURffqYbFZamsl6WWWGVvt4T336mHVcYAKmhtzOzLfzYTwHWcSVtGMvq2xnMarY1DBWt2z1s8+qXsPaH6ygwJQfrljh3cExkEyWuhpKvWh9VwhTZiV0heufjT3YExARkVDhL4iwskQrV5osls1mHq228Z6dCj/7zF0O6Lvmq758A6rpjKYXX/ILhzPUuQAHVaM8z8DPMwNmBWA2m3cWz1OgmSzfroaRWnqowCtMBX3TZJHGEq7Bl4iIyB9KS002ywocrKAqJcVd9rd9uwlYwDxawVllpVk/5RnwWMFaQxrOK9zGP6nExnDm8RNd/I7zbWVvlUuefbYJMB94wHRjnDrVf3llXTJZkVp6qMBLmhe9qQ8P+nMSEZEwNW2au9PftGkmGFm92gQsP//sHpeSYlqwJySYEkQrsIqKgjPP9A54du9u2MDrJDbyzz+qsx5mAnn0dzXIqI4VCFoNQqwmGpMnuzeA9g2U6ro/V6SWHirwktqxB3sCIiHKHuwJiIhIsGVnm7bwVmYoK8s7ECsvdx+74QYTmGRmmiDG6TSZpJYtTbmhp5IS/90I6yKBYhZxJQnsJ49+5GDSVOXlVUsRPcsJx4zxv/bLM0BsqEApUjdUVuAlzYeyKOFFf14iIhJmrJbxMTFmzy6n0wRWnqx1W7m5JlDz3JOrstIEWTXsyV4vbVo7eZaRnMQmtnMk1zCfSqJd8/JlPWdt8uzJKjm0REX5LzcUNwVejaCxG2tofVcd6E18eNKfW8SYOHEiZ5xxBm3atKFjx45cdtllbN682e9Yp9PJwIEDsdlsvP32217Htm7dyqBBg2jVqhUdO3bk3nvvpdxnQ5tPPvmE0047jfj4eI4//njmzJlT5XvMnDmTY445hhYtWpCWlsbatWsb6qWKSBioS/MG33Os/az69HEfLy01AUl5ubsMLyen+mvm5nrvyVVTVqshSg2HF8/mGl6lnGiuYgG/ckTA55aXw/jxpgTQyuydeaZ7blFRkbkuqyEp8JLA2YM9AZEQZw/2BELXp59+SkZGBqtXryYvLw+Hw0H//v0p8e2jDEyfPh2bn3cYFRUVDBo0iLKyMlatWsXcuXOZM2cOEyZMcI3ZsmULgwYN4rzzzmP9+vWMHj2aW265hQ8//NA1ZsGCBWRlZfHggw/y5Zdf0qNHDwYMGMCuXbsa58WLSMipS/MG33M8OxVax8vLTabLagfvcJggxWquYQkkiPLXtr0+evEF0xkNwBgms4qzaz0n3xLAVavc544dG5nrshqSAi+JfMqahDf9+UWExYsXc+ONN3LyySfTo0cP5syZw9atW1m3bp3XuPXr1zNlyhRefPHFKtf46KOP+M9//sMrr7xCz549GThwIDk5OcycOZOysjIAZs+eTdeuXZkyZQonnngio0aNYsiQIUzzeHc1depUbr31Vm666SZOOukkZs+eTatWrfx+TxGJTHVp3uB7Tpc/mgBamxynprofx451n5eba4Izzy6FrVsf+vs1ZLlhO35nEVcSTxlvcRlTqfrCzz675j3DrMyeJytDV1npf11WpLaFrysFXiIi0uT27t0LQPv27V3P7d+/n2uuuYaZM2eSlJRU5Zz8/HxOPfVUOnXq5HpuwIABFBUVsXHjRteYfv36eZ03YMAA8vPzASgrK2PdunVeY6KioujXr59rjIhEvro0b/A95/ffzeO2bSbLteaP3xMWFJix48d7n+/ZhGLfvrrPvfaczOFGuvI/vudYbuIloGp6a+VK8PjnlZgYE2iCeVy+vGogZWXy/AVlELlt4etKgVcD0/quEKNsiUijKioq8vooLS095DmVlZWMHj2as88+m1NOOcX1fGZmJmeddRaDBw/2e15hYaFX0AW4vi4sLKxxTFFREQcOHODXX3+loqLC7xjrGiIigfBsmuFwmAyVZ0asprVdTekenmAw73CQeK5kEXtpV+1Ya48xgKQk95o16zX5BlIrVpjXvXy5/+tFalv4uoo59BARtHZFgmvZGjgvLdizCIyd0Pv7MhoIoKylVoqB1yHFqrH5w4MPPojdbq/x1IyMDL7++mtWWgsjgHfeeYelS5dSUFDQwBMVEWk8UVHuDY/HjXNnw7Kz4dFHa389m61hSwzPZiUTGQfAP3iSAk4L+FwrCEtIMHOKjYWKCvOcVVZ5KDk5oROAhgJlvCRyKdsl0ui2bdvG3r17XR/jxo2rcfyoUaN49913WbZsGV2sBRLA0qVL+f7772nXrh0xMTHE/NGj+IorrqBv374AJCUlsXPnTq/rWV9bpYnVjUlMTKRly5YcfvjhREdH+x3jr7xRRJqP7GxTMhgb639Nkm+Z3bRp7jVOLVua4MQ67nkMAu9I2JBB1xHOXSzgKmKo4BWG8ywjaxwfE4NrE+WoKP97kVnz0+/I6kaBl0QmBV2RR3+mISkxMdHrIz4+3u84p9PJqFGjeOutt1i6dCldu3b1Oj527Fj+/e9/s379etcHwLRp03jppZcASE9PZ8OGDV7dB/Py8khMTOSkk05yjVmyZInXtfPy8khPTwcgLi6OXr16eY2prKxkyZIlrjEi0jxZe3CVl7v32PI9brWIb90aDjvMfezAAXjkEXP8kUdMiZ6nhmgFXysVFbxUdj1HsoP/cCK3MRt/67o8WZs8W5s4O50mEH34YVMyGBNjArKYGJUO1pUCLxGRhmYP9gRCT0ZGBq+88grz58+nTZs2FBYWUlhYyIEDBwCTqTrllFO8PgCOOuooV5DWv39/TjrpJK677jq++uorPvzwQ8aPH09GRoYr4Lvtttv44YcfuO+++/jmm2945plnWLhwIZkeizGysrJ47rnnmDt3Lps2beL222+npKSEm266qYnviog0hUA762Vmenf1820IYZXXWXt0ea6Hqqz03my4vNx7c+Ga9udqDN0WLOD8yqUUk8AVvEFJLerNHQ7zWhMSzKPVgdHhMKWG1no2dSusPQVeDShiG2vYg/Nt60yZEZGQM2vWLPbu3Uvfvn3p3Lmz62PBggUBXyM6Opp3332X6Oho0tPTufbaa7n++ut52KMtWdeuXXnvvffIy8ujR48eTJkyheeff54BAwa4xlx11VU88cQTTJgwgZ49e7J+/XoWL15cpeGGiESGQDvr5eSYTYGtTYJ9szpWeZ2V+fFklebFxECbNua5YFUv96v4iD8vWgTASJ7lG050HfOo8K5RQYHp4FhQYO6dbwZQ3QrrRoGXiIQPBdVhy+l0+v248cYbazznsssu83ru6KOP5v3332f//v388ssvPPHEE671YJa+fftSUFBAaWkp33//vd/vMWrUKH788UdKS0tZs2YNaWlh0rxFRGqttp31rLbxVqldVJR5tLJAY8dWXb9lZbxsNnereM+MWFPpwjZeLLsBm9PJc9EjeZVrvI4fak42m/e98uzcOHmy+3N1K6wbBV4SWfTGXERERDzUZc8ucK/5cjrNo5UF8r2OZ0MMz726mlosZSzgKg7nN/Yceyz3xT5R62s4nSbAtF5jTo57fVp5uXtcXe9pc6fAS0TCS7gE1/ZgT0BERALluw4sO9s0yPDs7metdwrVdU2TGcNZ5PM77fh8zBhKbS0OeU7v3qa00nNt28qV5uu4OPNao6PN89aj1J0CL6mZPdgTqIVweUMuIiIiIcVzzVJ2tlnTZLVPr6w0a7+s9U7TptWuS6FnUNNYLucNMpkOwMi4F9hfw5pVz7kXFJjsVVyc9xirw+Hkyaa0MiHB7FMm9aPAq4FEbGONcKGgq3nRn7eIiNSRvy6HVsfC1FTvhhFWGWGfPiboiooy65oeeKDqdWNj3Y01PDV2+eFxfMeL3AzAY9zLe9GX1jjec1ms51quhARzTzwDxfJylRU2JAVeIiKNxR7sCYiIiC9/HfmsjoUFBe4gDEzAlZ1tyu/AZL8efhg++aTqdePi3I01mkoLDvA6Q2hLESvozQM8UuP4Nm1gzBh3kOW5lssKrsaMcY/3DNICbcsv1VPgJeFP2Q8REREJkGd2y+LZpc+zbXx+vik79NSnjzsQ81RS0jjzrcnT3ElPvmInHbmKBZTjTlf5lkOOHw9FRe6vly3zH0jl5Lhb6o8d635eLeTrT4GXVM8e7AmI1EABt4iI1IEVWK1c6Q46PDM+mZkm6Cov9+7kZ/EXdAXD9czlFl6gEhvXMJ+fSfY67tltEdzt4K0AauVK8/joo+4xVlYLqpYXqoV8/SnwkvCmN98S6uzBnoCIiHjy3JvKaqbhuV/XJ59UDbhsNtMBMFScwgZmcTsAD/IQS7ngkOc4ne5ujZ7ruCor3QHXpEneTUasjFh2tnkuM1NrvepDgVcDUGMNkSBR4C0iIrXkWUqXlVV1vy5/GS2nE1avbvq5+tOafbzOEFpxgMUM4BH8dPrwY9w4E1hZmTyrFLFPH3cWzHMDZc/SQpUZNgwFXuKfPdgTCIDedIuIiEgteJbSZWbC1KlmrVeUn3fEvhkuf2WHTc/J89zCCfyXbXThWl7B6fN23l/7epvNZKqsYMvpNB82G6xYAYcd5l7T5VlyaQVhKjNsGAq8JDwp6BKLfhZERMSHb5mc9bm/LE5BAbRsWfUaobKWy1MGM7mKhTiIYSgL+Y3Dq4zx175+/HjzaHU07N3brGOz1oFt3+4ORP2te1NL+YahwEtEpLHZgz0BEZHmpboyOc+OhlYWJzUV9u8P7nwDcQZrmYpJOd3L46wmPeBzH37YlBTm5prXu2IFxMe7j3uWG6qcsPEo8JLwowyHiIiI1MA3wIqJMcGVlcVavdrdLKKgwJ35SUjwX6oXbO35jUVcSRwOXucKnuQf1Y71LZvs08c8Wq/devTcNHn5cpUTNgUFXvUUkY017E3/LUXqRcG4iIh48NwQOSfHZHc826tXVprszuTJ3vt5paa6y/E8Nw8OJhuVvMz1HM1WvuV4RvACYKt2vGfg1aePCarAvWbNCsQs1n1ROWHjU+Al4UVvsCVc2YM9ARGRyGWt4+rTxzympnpnb6ysV2ysGWsFG+Xl7iANYNUqmDPHBGX+1n0FwxgmM4j3OUg8V7KIItrWON5qArJ3rzvoAlNeOH48fPmlu+zQs7TQcy2cNA4FXiISGRSUi4g0S9nZ7iDC2hS4oMA7e5OTY5pOlJW5O/qBefRc31VZaRpNAOzb17Svw59z+YRcTGeMUczgK3oGfG5urrk3UVGme2Fiovd9sljBqdZ4NT4FXhI+9MZaREREfHgGCl26mEfP8kFPVpDmybMEMZR0opDXGEY0lczlel5gRK3Of/xx81qt1+cZSNps7uyfFZxmZprnSkuV9WosCrzq4QVuatTra32XSC0pOBcRaXY8m0T8/rt5bs0a79JDK5CYNMl9ntVS3Vb9cqmgiaacV7maJHaygVO4g2eoaV2Xp0Bej9MJcXHe67lycsxz5eXKejUWBV4iIk3FHuwJiIhEHs+mEFYQ5nR6lx7m5pqSu8pKc05srFnzdOaZoZnxeogHOY9P2EdrhvA6+0moMiY21h1wWsGWzQb33OM9zmYzr3H8eBNoRkWZR3/dC9XZsHEp8JLwoEyGiIiIHEJOjgkebDb3h8XpNEFHQgKMHWueC8VNki/mPR7gUQBu4Xn+ywlVxsTGmtdgBZwPPGBe1/jx7s2SrWYi1tdgslmVlabLo7/uheps2LgUeIlIZFGQLiLSbPjrxDdtmmmk4dlEA0wQlpRkMmCPPBKa65iO4kf+j+sAmEEGC7nK7zjrtVmNM+bMcT9vKS/3Lif0LB9URis4FHiJmz3YExBpBuzBnoCISHDVp215drYJJqzGEJMmmUDKc+2WVS7nyen07lhYWendZMNqNhFMsZSxkKG053fWcgZ3M6XasdY6LCvQ2r7df0dCzyYjnmvhlNEKDgVeISoojTVClTIYUlv6mRERCVl1bVtudSR0ONyBh+faJiugA1MuZ20YDCZYy872fs5TZaXZODmYjTae4B7SWMtuDmMoCykjvsbxnm3wLb6ZrDVrqt4XBV3Bo8BLRERERJpMXRs4eAZqMTHubI4VLFnZL2tc377u8Q6HOd63r8ls+QZYKSkmqIuJqd2cGsoQFnEXTwNwHf/HjxxzyHN8m4L4y2Q5ndqfK5Qo8JLQpsyFRCJ7sCcgIhI8dW3gYAVavXubQKqgwL2Wy+EwwZRnQOcbaJSXuzNm0dHex6wSRIej9q+nvv7Ef117dE1kLO8zKOBzfffjys6G5GRzLCEBxo1Tp8JQosBLDHuwJyDSwBS0i4hElIIC70croOjd292psLjYBGKtW5tArbrSwfLyppnzobRkP68zhET28Qnnkk1OwOdGRUGrVqZE0nrNkyeb7BbAjh0mGFOnwtChwEtEREREQp4VaKWmeq9ZWrHCO7B45BETfHz2mVm7FazywUDMYBTd2UAhnbiaV6ng0JO1WuJHRblLCK1yQqfT3STk8MOrNjCpT2MTqT8FXiFIjTX+oIyF1Jd+hkREIoaVuSkocG+KHB3tLrWzWGufnE6T8WrZMjjzPZSbeJGbeYkKoriaVymkc0DnnXWWuQ9jx7pLCK2gdNw400gETNmkb7ml1nsFV8QFXna7HZvN5vXRrVs31/GDBw+SkZFBhw4daN26NVdccQU7d+4M4oxDgD3YExBphuzBnoCISHjKzHR/Xlnp7nAI/jM5+/Y1zbxqoztfMZMMALLJ4RPOC/hcq9TSsmyZef2ZmSbrd8cd5vnY2KrrurTeK7giLvACOPnkk/n5559dHys9tiXPzMzkX//6F4sWLeLTTz9lx44dXH65MkwiEUtZLxGRiBMTY7JZUVHm8+oaaoSiNhSxiCtpyUHe42ImMTbgc20292u1ujiuXOmdxRo/3jz++mvVdV1a7xVcERl4xcTEkJSU5Po4/PDDAdi7dy8vvPACU6dO5fzzz6dXr1689NJLrFq1itWrVwd51uJFb5ZFJEJUVFSQnZ1N165dadmyJccddxw5OTk4PXpBO51OJkyYQOfOnWnZsiX9+vXj22+/DeKsRULXtGkmy+V0wv33m5K6ZctMUGI1lqiLptnDy8kLjODPfMuPHMX1vIzzj7fj/lrcW0GU62ynu5TScw8zZbHCQ0QGXt9++y3Jyckce+yxDB8+nK1btwKwbt06HA4H/fr1c43t1q0bRx11FPn5+dVer7S0lKKiIq8PEZF6swd7AtIUJk+ezKxZs5gxYwabNm1i8uTJPPbYYzz99NOuMY899hhPPfUUs2fPZs2aNSQkJDBgwAAOHjwYxJmLhBarMYTVVh7MOq/sbJP1qS/ffbEaw108xZW8ThmxDGUhu+ngOlZZ6R18bdtmMlS+wZeV2RozxmT7YmLcZYb+qKFG6Ii4wCstLY05c+awePFiZs2axZYtW+jTpw/79u2jsLCQuLg42rVr53VOp06dKCwsrPaaEydOpG3btq6PlJSURpt/kzfWsDfttxMJCmVQJYhWrVrF4MGDGTRoEMcccwxDhgyhf//+rF27FjDZrunTpzN+/HgGDx5M9+7defnll9mxYwdvv/12cCcvEkS+AYPVrdA3yMrNdXfyq0nv3g0/x9pIYzVPcA8A9/AEa0lzHWvTxjyefbZ7vM0GffqYQKt3b/MaPcsqc3IgPr5qE43cXO9HNdQIHSHcYLNuBg4c6Pq8e/fupKWlcfTRR7Nw4UJa1rGtzbhx48jyyN8WFRU1avDV7OlNsohEkLPOOotnn32W//73v/z5z3/mq6++YuXKlUydOhWALVu2UFhY6FWN0bZtW9LS0sjPz2fYsGFVrllaWkppaanra6sSw+Fw4KjDDrDWOXU5Vwzdw4bheR9nzzZZoCefhKeeghYt3ONstqoZKqttfHo6+CtkWrcueB0OOzh/ZVHpUGKd5bwZdTnPx91GS5v7Z6W83JQLgvcc160zj998411Gaf2YpaWZ15qW5n7uxRcdnHaaeRw/Hu6+G555BjIygrNBdDiq7d/nQMdFXODlq127dvz5z3/mu+++48ILL6SsrIw9e/Z4Zb127txJUlJStdeIj48nPj6+CWYrIiKRZuzYsRQVFdGtWzeio6OpqKjgkUceYfjw4QCuiotOnTp5nVdTNcbEiRN56KGHqjz/0Ucf0apVqzrPNS8vr87niqF72DDy8vJ4/vm6n3/XXQ03l3qrrOTMnBw6FWyjODmZ+CeG8GqrD2p9mfffr/rcXXe5X6t1fMYM6zGP99+H007DdS/9XUOqF+jf5/379wc0LuIDr+LiYr7//nuuu+46evXqRWxsLEuWLOGKK64AYPPmzWzdupX09PQgz1RERCLRwoULmTdvHvPnz+fkk09m/fr1jB49muTkZG644YY6XbO6Soz+/fuTmJhY6+s5HA7y8vK48MILiQ2kZkuqaK73MDfXZFPuuKPqWqS6sO7j+vUXMnmyuY82m8lmOZ2mzO6BB+Cii0ymp3Vr06UvlI1xPMrg8gIO0ILzf3uHr0d0D+g8a+3W+PGQnFy1cYjV2dHzvoD3z+LkybE88wx07w7//nfD/TlFutr+fQ60/0PEBV733HMPl156KUcffTQ7duzgwQcfJDo6mquvvpq2bdsyYsQIsrKyaN++PYmJidx5552kp6dz5plnBnvqItKYlq2B89IOPU6kgd17772MHTvWVTJ46qmn8uOPPzJx4kRuuOEGV8XFzp076dzZvYHqzp076dmzp99rVleJERsbW683/fU9X5rfPZwyxQQEU6aAnyRswLKzzRqku+82GZoZM2I5cMDcR2s/qmnToKLCfL10qTnvwIEGeBGN6DyWMh7T9eIOnuHz0l61On/iRFNuuWePCbJatHDvSxYVZY7FxoLdXvXc2NhYpkyJpaTEfb/q++fU3AT69znQv/MR11xj+/btXH311ZxwwgkMHTqUDh06sHr1ao444ggApk2bxiWXXMIVV1zBOeecQ1JSEm+++WaQZy0iIpFq//79REV5/3cbHR1NZWUlAF27diUpKYklS5a4jhcVFbFmzRpVY0jIa6gNea0GEM88Y76+4w53M4mxY93Hc3MhLq7+824KndnBq1xNNJW8wM3M4aZDnuP7/t3pdLfPj4szgRaYe279s1JTN0brz6d3b7WcDwURl/F67bXXajzeokULZs6cycyZM5toRiIiNbCj7qIR7tJLL+WRRx7hqKOO4uSTT6agoICpU6dy8803A2Cz2Rg9ejS5ubn86U9/omvXrmRnZ5OcnMxll10W3MmLHEJOjvmor8xMmDwZrJ4x48d7Z2aWLXN3MwyHBhHRlPMaw+jELr6iO3fy9CHPiY01LeKtboQA48a5g6+srOo/r05D/flIw4i4wEtERCSUPP3002RnZ3PHHXewa9cukpOT+fvf/86ECRNcY+677z5KSkoYOXIke/bsoXfv3ixevJgWnm3cRCKAVVKYmekdEOTkuDM7/qxe7f95f90NQ8EjPMA5rKCINgzhdQ7gbnoTG2vm7Ptax441e3FZ2b2EBPfeXL73yt/nEvoirtRQwpxayYtIhGnTpg3Tp0/nxx9/5MCBA3z//ffk5uYS51EvZbPZePjhhyksLOTgwYN8/PHH/PnPfw7irEUaR017Slllcb6ys6sGKTabKZ8LxaDrUt5hDI8BcDMv8h1/8jrudHpvlAzmNVpBVkOVb0roUeDVnNmDPQERERFpTmoKKnJyYMeOqs/7BmkJCaaDn+9GyqHgGLYwF9Ot9Enu4g2GVBljs3mXS9ps7qALzH0oLvZ+TiKDAi8RERERaRL+gorsbNMWPjvb/VyHDqaZRHY2pKaa56zGE4cdVjUYC4VGknGUspChHMYeVpPGvTwOQJcu3uOsoMtmMw0yoqO9X7vF332R8KbAS0SaD5WyioiElOxs00yipAQeecTsVwWmtNDhgEcfrdpUY/t2dzBmSUqqWr7X1KaSxRl8wW+05yoW4MCUE2/f7n+802m6FJaX+y+9rKksU8KTAq8Q8sHyy4M9BREREZEmM2mS+3On071JsBVEWe3TffmWGW7bFtz1XlfxGhmYXvjX8gpbObrG8bGxh27zrrVekUeBl4iIiIg0mNqUyFWXpWrVyv/zNltolBV66sYmnucWAHJ5gMUMPOQ5VrOQvn29Sy89753WekUeBV4iIiIi0mBqUyKXlub/+TvucGeEYmPdAZrTGVr7eLWihNcZQmtKWMp5PMhDhz4Jd3Zv8mTo08e8vj59VF4Y6RR4iYgEmz3YE2h8EydO5IwzzqBNmzZ07NiRyy67jM2bN3uNOXjwIBkZGXTo0IHWrVtzxRVXsHPnTq8xW7duZdCgQbRq1YqOHTty7733Uu7TZ/qTTz7htNNOIz4+nuOPP545c+ZUmc/MmTM55phjaNGiBWlpaaxdu7bBX7NIc5WZCTExUFZ26KxXQYH/51euNNkeMIFWKLaNByezuJ2T+Q8/k8Q1zKeS6BrPaNPG5wpOd9nkypUqL4x0CrwkdKjxgUjE+vTTT8nIyGD16tXk5eXhcDjo378/JdaCDiAzM5N//etfLFq0iE8//ZQdO3Zw+eXuta8VFRUMGjSIsrIyVq1axdy5c5kzZ47XRsRbtmxh0KBBnHfeeaxfv57Ro0dzyy238OGHH7rGLFiwgKysLB588EG+/PJLevTowYABA9i1a1fT3AyRCJeTA/HxJmCaNs1/6aGV5TnsMP/XyM83j6HYMt5yC89zPf9HBVEM4zV2knTIcw4eNEFpVJR5HDfO3fUwJUXlhZFOgZeINC8K8INi8eLF3HjjjZx88sn06NGDOXPmsHXrVtatWwfA3r17eeGFF5g6dSrnn38+vXr14qWXXmLVqlWsXr0agI8++oj//Oc/vPLKK/Ts2ZOBAweSk5PDzJkzKSsrA2D27Nl07dqVKVOmcOKJJzJq1CiGDBnCNI+6nalTp3Lrrbdy0003cdJJJzF79mxatWrFiy++2PQ3RiRCeWZu/JXPWQFVdR3/rNJC31bsoaInBTzNnQDcz6Ms59xDnpOQYDJc5eXQsqUJTB9+GH7/3RzfvbsxZyyhQIGXiIg0ub179wLQvn17ANatW4fD4aBfv36uMd26deOoo44i/49ffefn53PqqafSqVMn15gBAwZQVFTExo0bXWM8r2GNsa5RVlbGunXrvMZERUXRr18/1xgRqZ/sbBNkZWaawMIzCLMyXVbJXZ8+Zh0XVG200adP9YFZMHVpvYfXGUILSvkXl/A49wZ0XmameY0xMd6lhJmZZh1baal5zdq7K3Ip8Gqu7MGegIhEgqKiIq+P0tLSQ55TWVnJ6NGjOfvssznllFMAKCwsJC4ujnbt2nmN7dSpE4WFha4xnkGXddw6VtOYoqIiDhw4wK+//kpFRYXfMdY1RKR+rAzX5MkmiAB3+ZyV6dq3z2R/zj3XrPMaPx4eeMB9Dc+1T6HFyVPFN3EcP/A/juYG5uIM4O20zWbui8NhMl6ea9Zycsxm0eXl5jWXlJi9zRR8RZ6YYE9AREQa1/unnk+rxIb9535/UTmwlJSUFK/nH3zwQex2e43nZmRk8PXXX7MyNN9ViUg9ZGebzE1srLtzn1ViOHmye5zVFr6iwox75BHTQv7II4Mz70BlMo2/8TalxHEli/id9gGdN348LFvmDiYnTTL3w+mEsWNN1mvaNLMxtDVm2jQTlEnkUMZLRETqbNu2bezdu9f1MW7cuBrHjxo1infffZdly5bRxWPxRlJSEmVlZezZs8dr/M6dO0lKSnKN8e1yaH19qDGJiYm0bNmSww8/nOjoaL9jrGuISN1Nm2YyN3FxJqDwXOfl2QbeWutkZX6sIO2nn4Iz70CcxWdMZgxgArAvOMN1zNpbzN8eY23awNSpsMZjiXFlpTv7ZQVYxcWwYoUJ0hISTBDmr+ywNvukSWhR4CUiInWWmJjo9REfH+93nNPpZNSoUbz11lssXbqUrl27eh3v1asXsbGxLFmyxPXc5s2b2bp1K+np6QCkp6ezYcMGr+6DeXl5JCYmctJJJ7nGeF7DGmNdIy4ujl69enmNqaysZMmSJa4xIlJVoG/2fduhl5XBxIkmiIiNNeubrL25YmKqtlcPVYfzCwu4iljKeZVhzOJ2wMzf6YQxY9zNMyy9e5sgat8+E1R6HouKct8D39bxVhBWUOC/7FB7fYUvBV4SGtRpTpqSft6aXEZGBq+88grz58+nTZs2FBYWUlhYyIEDBwBo27YtI0aMICsri2XLlrFu3Tpuuukm0tPTOfPMMwHo378/J510Etdddx1fffUVH374IePHjycjI8MV8N1222388MMP3HfffXzzzTc888wzLFy4kMzMTNdcsrKyeO6555g7dy6bNm3i9ttvp6SkhJtuuqnpb4xImAj0zX5Ojgm+pk41pXRWVqegwAQn8fHQt6/ZOLm83AQloS6KCuYxnC78xDecwEieBUwnkH373M1ESkrMa7IUFHjfr3Hj3NmsceNMUGp1NvTH458tr+tor6/wpTVeIeKD5ZcfepCIRC47Ed30ZtasWQD07dvX6/mXXnqJG2+8EYBp06YRFRXFFVdcQWlpKQMGDOCZZ55xjY2Ojubdd9/l9ttvJz09nYSEBG644QYe9njX0rVrV9577z0yMzN58skn6dKlC88//zwDBgxwjbnqqqv45ZdfmDBhAoWFhfTs2ZPFixdXabghIm7WGqRDvdnPzjYZGnCv43I6zXlTp7ozOOFkPLn0J4/9tGQIr1OMd5pu8mQTSFprs2JjTallVpZ57dZ98wywJk4067zGjKl+HZf1vO99z8nR2q9wpcBLREQandOzxqYaLVq0YObMmcycObPaMUcffTTvv/9+jdfp27cvBQUFNY4ZNWoUo0aNOuScRMQI9M2+Z2YmOtpkdSxOZ/gFXf3I40EeAuDv/JONnFJljJXRs4wdaxpp5OSYckMrA+h0muesdXBw6AYaCrIii0oNRURERKRBZGaadUuxsXDmme51YVY5nrVXl81myuX8NaOwjgfbkWxnPtcQhZNnuZVXuM51zHN+Tqe7/C8727tt/sqVJrPluVbL8x55ZrJ819GpiUbkUeAlIiIiIg0iJ8esWyorczeHmDbNlOOVlJgxCQlmrVNxsXenQ08BJMkbVQwOFnAVR/ArBfTkHzx5yHOszFZ2Nng0bfUK0qwMl3WPPMsPfdfRqYlG5FHgJSIiIiINzrMJhBVIRUeb5ydODI2sVnUmMo6zWcVeEhnC6xykpddx38Bw0iST0bICJc/92MeOdQdi7dtDnz7mtffp430N36YZaqIReRR4iYiIiDRjtSlpq81Yqy36ww+79/QaN84EKZ7d/0LNYN7mHqYAcBMv8QPHeR23WuInJJjHmBjv15Oa6l1SOXUqbN9uvt62zbsM0ZPn/fL3tYQ/BV7NkT3YE/Ch1t4SDPq5ExEBalfSVt/yN6czsExX69Z1u359deUH5nAjAFPJ5C2qdp0eO9aUUaammsfKSu/jBQWmy6GlpMT9mvv0McGa9bk0Lwq8RERERJqx2pS01bX8zVrjlZsLgezcUFxcu+s3hHgOsogracdeVpHOGCZXGRMV5W6WsXKlebQCL6thSFaWu8uhZ4ml0wnLl8OKFe7PpXlR4CUiIiLSzHiWDNampK26sTV15EtJ8W6iYZXdhZrpjKYXX/IrHbiKBZRTteViZWXVMsmUFPN49tnue2MFqFYpos2m7oSiwEtEJHTYgz0BEWkuAikZrGk9l+8x63qTJpnNg61GE5Mn+w+0qmsjHyzXMI/b+CeV2BjOPLaTEtB5bdqYdVtgMmApKSbIeuQRE3ytWAHx8SbwVHdCUeAlIiIi0swcqmQwO9u7S58vz8AtO9u0Ro+JMRkhz+yWv3bxMTEN8xoayon8h2cZCUAO2XzEAK/jNa1J27fP+2sryLQ2i+7TR90JxU2Bl4iIiEgzU1N5oRV0WfwFDJ7BxLRpJsCKjzdroPyJijJZrpgYSEqqfv+uppZAMa8zhAT2k0c/HmZClTE17SnmG5S1aeP99cqV6k4obgq8RKT5UmdDEZEqPDNc2dk1BwxOpwnCYmJM1uvMM72PWyWFlZUwZowJuEJnjZeTf/J3TmITP5HMcOZRSXStrjB+vLkHCQnm6337zLouz86FgbTgr02bfglfCrxCwAfLq7YqFREREQmG1FTz2Lu3CSr8BQSepYY5Oe51TCtXmixQTIw5xzOz9cgj5tHaTDjY/s4/Gc58yonmKhbwCx0PeY5nmaS1R1d2tgk+LStXencuDGQ9XX3b9Et4UOAlwaWMwyH1ZDMf8A968N9gT0VERJoBqxV6QYG7dfqkSe7j2dlQWmqyWVYZohWsgXuvrokTva9rlez99FPjzT1Qp7GOJ/kHAGOZxGf0Dug8q6NhTIx5PSUlJqDMyal+f65A1nhpHVjzoMBLJMQNZQkXsYahLAn2VEREpBnwDAKsNUyea5mmTTMBSHm5CTiiouCzz7yvYR33lZJS85qpptCO31nElcRTxtsMZgp31/oa8fHuz51OE2wVFJjSQ9/9uQJZ46V1YM2DAq/mxh7sCUht/Y1PvB5FREQak2cQMGaMu7yuTx9TdpiaagIzK4ByOgMPpoK/vsvJHG7kWLbwA125kTmArdqmIG3amNdqlUempLiD0t4eSTJrM2WVCkpNFHiJhLBj2EE3tgJwIj9yNDuCPCMREWlOfNdvlZSYx7Kyqh38PPl2+wuVdV338ASDeYeDxDOE19lLO8A0//Bn3z4TaFoB47ZtJiP48MNmHdf48e6NklUqKIeiwEskhF3CSiow/3tVYuMSPjvEGVJrobbO0B7sCYhIpKttlz2r9NAzw+NwVN3DypNvBiz4mS7ozQomMg6Af/AkBZx2yHNsNhNoerL258rONhkua6Nkf6WC6lYonhR4iYSwwbgLxZ0+X4uIiNRFbbvsWaWHVoanpg2FLYcqPQzkGg2pIztZwFXEUMErDHdtmOw5H6v1vafWrf2vc1u5Ut0KpfYUeImEqDaUcC4FRGP+94rGSV++pDUlQZ6ZiIiEs7p02bMyNwDRtdvqqgqbrWkbbERRwXyuIZmf2chJ3MZswDvyczr9b+q8b5/3WjZrLVhKStXOjv6oW6F4UuAlwRNqJV4hpj9riKXC67lYKuiP7puIiNRdXbrsWZmbyZP9dyusjabuavggD3EBSykmgSG8TgmtAz43JcX766gok/UrLHTfB3UrlEAp8BIJUZeyAgfev1Z0EM2lrKzmDBERkcZh7dNV36CrqQ1gMePJBWAkz/INJ9bq/KOPdjfQiI01r99qpw/hdz8kuGIOPUREGlIyu+jE7hrH2IC/stJvxmswKziNbzjULwx30p4ddKzfZEVERHBvqux0mvbyLVvW3FwjFHRhG69wLVE4mcVtvMo1tb7GypVmbVtOjruZRlaW2VDa4XC32hcJhH5cRJrYq2RzDl8dclwl/lcet6WYddx4yPM/pSd9mV3b6YmIiACmc9/KlaabYWqqu7tffDwcOOA9tqnXbR1KLGUsZCiH8xvrOI1M6tbdIiXFrG3LzDTBF8DUqZCWZoJRrd2S2lCpoUgTe57BHCCu2sDKElVNTqu65y2V2DhAHC/w1zrPsdnRekMRkSqsQGvlSnfGC0xTCd99r0Ip6AKYzBjSWc0e2nIliyilRcDnWlmshATYvdu7K+HkyebrNWu0dktqT4FXkH2w/PKm+2b2pvtWUr3/42J6MZdvSaGigf8KVhDFfzmKXszl/7i4Qa8tIiLhq7b7SWVne7dP37/f3dGvvLz6DYdDwRW8TibTAbiBuWzh2IDOs9nM6x471t2J0LcroRVgOhzam0tqT4GXSBBsoiunMZeXGQhAff//ss6fy8Wcxlw20bWeV5SgmhjsCYhIpAl0PykrQJs82TuL5XSGdrBlOZ5veZGbAXice3iHwX7Hea7NsgJKp9N8eHYi9O1KOHas+zztzSW1pcBLJEj205KbyeYGsiklrkoHw0A5iKaUOK5nAiMYz4FalFOIiEjky86GsjITbPiuSfLNhFkBmsNhuvj5tlOvTu/eDTvnumjBARZxJYnsYwW9uZ9Hqx175pkmk9W7t3dAmZvrvhf+soQ5Oe4uh1rfJbWlwEskyF5mEL2Yyw8cWevSwwqi+J4unKbSwsgyLtgTEJFwVF054bRpJpCKjzcZHX+BlpW9Oeww93lxcWaNUyBWhsBOJ09zJz35il0cwTBeo5zYascWFJhMlufaNYt1Lw6VJQy1dW0S+hR4BdnAc95sum9mb7pvJbVjlR6+ybm1Ou9NzuU05vKNSgtFRJq96gIFz3VKvmN81zBt3+4+LzXVHPc0frzJhNlq7g/V5K5nLrfwApXYuIb57ODIGseXlJgSw9RU8/q7dDHP22zue2HtXZaa6h3UBlq2KeJLgZdIiNhPS37m8IBLDh1Es4MjVFooIiJA1SDK4rlOyXeM57E+fbzPs7JBnkHWpEnQqVNoZXtOYQOzuB0AO3aW0C+g85xOk6lLTYVt29zr2Kys4OrVZlxBgXewVd19FjkUBV4iIcJGJVfxcZVNk6sTSwXDyMNW79YcwnlpwZ6BiEi9+TaCONQYK4uTkmKCK99ywdTUqk02ysu9s2LB1pp9LOJKWnGAD+lPLuNrfY2VK73LM60gy2bz390wkPss4o8CL5EQcRb/phO/V3m+0ufRUyd+J50NjTovEREJL4G2jrcCjOoCqYKC0MpsVeXkOW6lG5vZzpFcyys46/jW1rOphhVkjR1bfXdDkbpQ4CUSIoaypEqZodWxcCrD/HY+dBDNUJY05TRFRCTEBbIGKTvbbIRss7lLCX3XbbVvDxWBFWEExR08wzAW4CCGoSzkV46odqzVdbFNG/MY5ecdsHW/FGRJY1HgJRIC/JUZWh0LezGXuxntt/Ohyg1FwsNPP/3EtddeS4cOHWjZsiWnnnoqX3zxheu40+lkwoQJdO7cmZYtW9KvXz++/fbbIM5Ywlkga5AmTzZlg9beVQBHHukdfFnrnkLRGaxlGqbzx308Rj5nVTs2NhZWrDCNQfbtM895Bl7Wa7aaaYg0FgVeIiHAs8ywus2Qq9t0WeWGIqHt999/5+yzzyY2NpYPPviA//znP0yZMoXDPPp2P/bYYzz11FPMnj2bNWvWkJCQwIABAzh48GAQZy7hKpCMjb+Aavv20A20PB3GbhYylDgcvMHlTGd0tWNtNvOarG6EljPPdO/HZW2mvHp1YCWaInWlwEuCRw0NXIayBCdQfojNkH03XS4nCucf54tIaJo8eTIpKSm89NJL/OUvf6Fr167079+f4447DjDZrunTpzN+/HgGDx5M9+7defnll9mxYwdvv/12cCcvYctznZf1eZ8+7ufGjg38Wp7liMFmo5KXuZ5j+JHvOI6beRGofnJOp8nsWd0ILWvWuJ8bM8YEYJWVpkRz8uTGfx3SPCnwEgkyq8zQBnz3R2nhoTZDtjZd/p4u2EDlhiIh7J133uH000/nyiuvpGPHjqSmpvLcc8+5jm/ZsoXCwkL69XO3wG7bti1paWnk5+cHY8oSATzXeVmfr1zpvfYrtpr9hX2DLM9yxGC7j8e4hPc4SDxDeJ0i2tY4vk0b77JLq8TQ4XDfCytDaB0LldcqkScm2BMQae5aUsr3HMl7nM0o7gl4Xy6r9HAGT3ACP9KSUvbTspFnKyK19cMPPzBr1iyysrK4//77+fzzz7nrrruIi4vjhhtuoLCwEIBOnTp5ndepUyfXMV+lpaWUlpa6vi4qKgLA4XDgcDhqPUfrnLqcK0ao3cO774ZnnoGMDBNIPPMMdO8O//63eZwyxZTYxcSYQCsmxuzPFexW8S1bOrwePfWuWM4jZQ8AkBX7JP+NOZmW+L/fsbEmuCovd2fsZs2C+HjvcZWVcMEFsHgxnHMO5OdDero5N1yF2s9iOKrtPQx0nM3pVFxfW0VFRbRt25Z+e/+P2MRW9b7eB8svb4BZBcjedN8qIMvWBHsGIcFGZZ1b4DbE+c1eqJW93lsEF7dl7969JCYm1vky1r9Vr+49n1aJDft7tv1F5Vzddmm959gcxMXFcfrpp7Nq1SrXc3fddReff/45+fn5rFq1irPPPpsdO3bQuXNn15ihQ4dis9lYsGBBlWva7XYeeuihKs/Pnz+fVq3q//+SSKiJ//13+mZm0mLPHraedx4Fd90VOvWP0uzt37+fa6655pD/JyrjJRIC6hs0KegSCV2dO3fmpJNO8nruxBNP5I033gAgKSkJgJ07d3oFXjt37qRnz55+rzlu3DiyPFrWFRUVkZKSQv/+/esUCDscDvLy8rjwwguJra7+TGrUVPcwOdmUyCUkwI4d/o9ZYmMhLg7KytwZnIQEOHDAZHpCUcuWDl58MY+bb76QAwfMfYx2lvNu2UBaVO5ho+0kzs1/k/2rE6qcm54On39uslxgujT+9JP/75OQ4L5XZ50FH3xg9vKysoQPPNAYr65p6O9z/dX2HlpVB4eiwEtEmrdQy3bZgZJDDZJwcvbZZ7N582av5/773/9y9NFHA9C1a1eSkpJYsmSJK9AqKipizZo13H777X6vGR8fT7xvzRQQGxtbrzda9T1fGv8e3nabWZd0++3ea7Sys2HPHpMESkszmx/v3w9FRe6ufZWVcPBgeKxhOnAg1hV45WLnXD5lH6253Pkmvx1s5/ecNWvMa7Ze388/m6/79DHr29q0Me3k+/SBc8819zE11XQztDZJ9pNIDlv6+1x/gd7DQO+zfk0uIiLSiDIzM1m9ejWPPvoo3333HfPnz+fZZ58lIyMDAJvNxujRo8nNzeWdd95hw4YNXH/99SQnJ3PZZZcFd/ISsnyDJ2tfLjB7VmVmutc22WymfXplpf+gKyWl6nOhUsV3Me/xAI8CcAvP819OqHZuJSXer699e9PFsW9f83xRkWkh/+WX5nhxsQlQD7XZtEhDUeAlIiLSiM444wzeeustXn31VU455RRycnKYPn06w4cPd4257777uPPOOxk5ciRnnHEGxcXFLF68mBYtAmu2I82HZ7dCT1bAYT1Om2bKC51O87hypff47Gx3t8I/kq9Vrte7d8PPvzaO4kf+j+sAmEEGC7kKCDxjt22buVeTJrnb6Pvev0A2mxZpKAq8mht7sCcgItL8XHLJJWzYsIGDBw+yadMmbr31Vq/jNpuNhx9+mMLCQg4ePMjHH3/Mn//85yDNVkJZdYHC2LHm+XHj3ONiYtwt0i1RUSabNXGie18v36DMUt3zTSHWWcZChtKe31nLGdzNlIDP7d3b3Avr0WZzB1u+9y+QzaZFGkqDBl5r1qhDnYiIiEhjqS5Q8H0+J8cEHJ5NNGw2aNnSvamwta9XKJroGEMaa9nNYQxlIWW41zR6lhp26eL+2mYzWa0VK8y9sB6tDZKzsqreJ8+NpkUaW4MGXldeeWVDXk5ERESk2bCCACsTVd9gwF9J3oED9btmU0heuZI7KmYCcD0v8yPHuI6NH+9uFgLw++9g7aDQqpV5zda969PHBGOffGIyXVOnVr2nkyeb4HPy5MZ9TSJQh66GQ4cO9fu80+lk9+7d9Z6QNDPnpWkvLxEREdzrj6wSv2nTTIamLvwFbdaaLn9sttDodvinys2kzpgBwCTG8B6XeB3PzfUe3769WcsFpkOh5xouK5u3cqW7icakSe6Sw5ycqmvjRBpTrTNeH3/8MTfccAMZGRlVPhISqu6pEKpmzpzJMcccQ4sWLUhLS2Pt2rVBm8vAc94M2vcWEWkKy5cv59JLLyU5ORmbzcbbb79dZcymTZv461//Stu2bUlISOCMM85g69atruMHDx4kIyODDh060Lp1a6644gp27tzpdY2tW7cyaNAgWrVqRceOHbn33nspt1q9/eGTTz7htNNOIz4+nuOPP545c+Y0xksWqTVr/ZG1Nqk2DR88s2WxsSZA8fnRr1EoBB4t2c+8smHEHDzIiqg+jCf3kOdYQReY4MpzDZdnCaL1vOd6L6i6Nk6kMdU649W3b1/atGnDOeecU+VY9+7dG2RSjW3BggVkZWUxe/Zs0tLSmD59OgMGDGDz5s107Ngx2NMTEYk4JSUl9OjRg5tvvpnLL7+8yvHvv/+e3r17M2LECB566CESExPZuHGjV1e/zMxM3nvvPRYtWkTbtm0ZNWoUl19+OZ999hkAFRUVDBo0iKSkJFatWsXPP//M9ddfT2xsLI8+atpRb9myhUGDBnHbbbcxb948lixZwi233ELnzp0ZMGBA09wMkWrk5NQ9wzVpknvdVriaSQanODdysF07bjj4ChUHD/021TNTV1ZmHouLzaPTaQKsrCz3mE6dYPt2kx2D+t1zkdoKOPD67rvvOP7443nzzeqzM3l5eQ0yqcY2depUbr31Vm666SYAZs+ezXvvvceLL77I2LFjgzw7EWkyobZ5cgQbOHAgAwcOrPb4Aw88wMUXX8xjjz3meu64445zfb53715eeOEF5s+fz/nnnw/ASy+9xIknnsjq1as588wz+eijj/jPf/7Dxx9/TKdOnejZsyc5OTmMGTMGu91OXFwcs2fPpmvXrvz666/88ssvjBo1ipUrVzJt2jQFXhLWrOxOVJR3Q402bcy6rspK9zHP46HiJl7kJuZQQRTrsrIonNg54HMTEkzQ5XCYtVqepYRWUNW6tcl0WeWHBQWN8CJEDiHgUsOTTz6ZSy+9lCVLljTmfBpdWVkZ69ato1+/fq7noqKi6NevH/n5+X7PKS0tpaioyOtDRESo8m9jaWlpra9RWVnJe++9x5///GcGDBhAx44dSUtL8ypHXLduHQ6Hw+vf7m7dunHUUUe5/u3Oz8/n1FNPpVOnTq4xAwYMoKioiI0bN7rG9OvXj71799KvXz/+9Kc/4XA4XFkzkXBlde574AHv/bcqK01A0rKlyYj5djkMBd35ipmYDcUfjX+QX30qqDznGRvrfa7T6d44OSHB/bXvPmf+yjjV0VCaWq0yXv/85z8ZPnw4hx9+OP/4xz+47rrrwm5zx19//ZWKigqv/5gBOnXqxDfffOP3nIkTJ/LQQw81xfRERBrcC9xELK0a9JoO9gNLSUlJ8Xr+wQcfxG631+pau3btori4mEmTJpGbm8vkyZNZvHgxl19+OcuWLePcc8+lsLCQuLg42rVr53Vup06dKCwsBKCwsNDvv+3WMc8x06dP55dffuH//u//mDFjBsXFxfTv35+RI0cyePBgYn3f3YmEON+SuT59TNlhaqoJLPx1MwyFdV2J7OV1htCSg7zPQCbZxjCfxV5jnE53SWFamrtRhqdx40yLeGuTZN/1cf5KCq0sWH2amIjURsAZr5SUFHJzc9m2bRv3338/c+fOpUuXLowbN45tnisbI9C4cePYu3ev6yPSX6+ISKC2bdvm9e/juDqsUK/841fwgwcPJjMzk549ezJ27FguueQSZs+e3dBTdjniiCPIyspixh8d1I477jiuu+46kpOTyczM5Ntvv2207y1SF4FkaKwW6tZar9WrTWBhZbpCJctlOHmBEfyJ79hKCtfxfzhtVd+axsa6W8ivXm3KCmNizB5eYLJYTqcppczNNcFmIBsiV7cZtUhjCTjwKisrY9euXfzwww8ce+yx3H///dx0003MmDGD448/vjHn2KAOP/xwoqOjq3TC2rlzJ0lJSX7PiY+PJzEx0etDRKTB2YM9gdrz/bcxPj7+0Cf5OPzww4mJieGkk07yev7EE090dTVMSkqirKyMPXv2eI3x/Lc7KSnJ77/t1rHqxmzevJn4+HiWLVtGdHQ0F198MRs2bOCkk05imm+9kkgQebZKry4I822uYbO5G0nYbGYfrFBxF08xhDcoI5YrWcRuOvgd53SaUsqYGFMu6XBAfLzZwwtMBmzaNHcGL9AGI9VtRi3SWAIOvFq0aMHxxx/PwIEDue2225g0aRLffPMNf/3rXxkxYkRjzrFBxcXF0atXL6+1apWVlSxZsoT09PQgzkxEpHmKi4vjjDPOYPPmzV7P//e//+Xoo48GoFevXsTGxnr9271582a2bt3q+rc7PT2dDRs2sGvXLteYvLw8EhMTXUFdeno6S5YsweFw8MYbb3DJJZdwzz33EB8fz+jRo9mxYwdz587l448/ZuHChTysd2QSQjwzNNbGv7m53sGXtb4rJcWMHTvWZInABCaTJjX9vP1JYzVPcA8A9/AEazHNjqzMltX6HdyPnu3xs7K874cVXILJ+mn9loSigNd4DR06lLy8PP76179y1113ceyxxzbmvBpVVlYWN9xwA6effjp/+ctfmD59OiUlJa4uhyIi0rCKi4v57rvvXF9v2bKF9evX0759e4466ijuvfderrrqKs455xzOO+88Fi9ezL/+9S8++eQTANq2bcuIESPIysqiffv2JCYmcuedd5Kens6ZZ54JQP/+/TnppJO47rrreOyxxygsLGT8+PFkZGS4MnG33XYbM2bMoG3btsTFxdGjRw8AFi5cWKWr4XnnnVdlTZlIU7HWKlnd+cB7ndLEie6xnmuUVqww506a5F4X5RmwOBxNM/+atOc3FjKUWMpZyJUUnH0nsWvNPK0ga8cOk4my1mt5Boy9e7uzVJ5dC8GUJX75JeTnm+tp/ZaEkoAzXq+99hpfffWVa8Phyy67zPUfYri56qqreOKJJ5gwYQI9e/Zk/fr1LF68uMqibBERaRhffPEFqamppP7xa+msrCxSU1OZMGECAH/729+YPXs2jz32GKeeeirPP/88b7zxBr092rNNmzaNSy65hCuuuIJzzjmHpKQkry1OoqOjeffdd4mOjiY9PZ1rr72W66+/3itr1bVrV9577z2OOOII9u/fz/bt23nhhRf8tpJv164dW7ZsaaxbIlIjz7JCf8aONdmh2Niqa5SmTXOX5IVatayNSv6P6ziKbfyXP3ELz7PyMxsOhwkSrcAwN9e7FNCzEYi/VvBW9svqamizaf2WhJ6AAy+ALl26MGnSJH788UcGDBjAbbfdRs+ePZkzZ04jTa/xjBo1ih9//JHS0lLWrFlDWpr28wka7aUkEvH69u2L0+ms8uH5/8fNN9/Mt99+y4EDB1i/fj2DBw/2ukaLFi2YOXMmu3fvpqSkhDfffLPK2tyjjz6a999/n/379/PLL7/wxBNPEBPjXdzRt29ffvzxR8rKyvj++++58cYbG+tli9SZZxmdv7K5nBw480wTqCxb5n6+Tx93xz8rKAulhhrjmMjFfMABWjCE19mHe928Z5PUZ54xj9ZrP/PM6gNNcAdpY8e6Syy1fktCTcClhjNmzGDfvn1eH926dWPp0qWMGDFC/3GFEzthuYhfpEEp4BeREOZv89/cXPcxcDeRWLnSjMnM9G4skZYGU6fCkUfC9u1Vv0ebNrBvX+O9Bl/nsZSHMVnu6X96hg3feu/XtW0bWP3aDhxwl1uWlJgsVyBlkv7axouEioAzXvPmzWP58uVs2bKF8vJyOnfuTHp6Oo8//jjz589vzDmKiIiINEvZ2eC5L/mkSe7sl+dGyVZZoudzK1ea5/0FXdC0QVdndvAqVxNNJS9wMw98dxNRf7wL9cx0/fSTeaysdK9xU8mgRIqAM175+fmNOQ8RERER8TFpkrvpRKtWZg8rK8gqLnZnw8B09isoMMGXv02GgyWacl7lajqxi6/ozihm4HSa9VgJCbB1qzu75bmWKyvLlAoqgyWRolZrvERERESk6Vjrs5xOOOwwU24XFeXOAFkZoexs0za+pMQ8Zmb6v04w5DKec1lOEW24M+l1ymNaYrN5r9ey1mjdYzrMc999Wp8lkUeBl4iIiEiQHGq/qTFj3J9bJYOVlSZQSUkxWaLUVLOWq7LSHLfZqu7X5ZlJakqX8g5jmQzA+1e8yJf7/uSaj9Np5t2nT9V74HRWf2+0R5eEKwVeIiIiIkFSU9t4q/yud2+T1fK1fbs511rLZQVeDof33l2Aaz1VUzqGLczlBgBmxd7F1W8M8Wr1brN5z3/aNHc3w2eeqf7eHKrVvkioUuAlIiIiEiT+mkf06WOCktxcd0e/4mJTmlcbVgni+PH1D7xiY00790BERUEcpSziSg5jD6tJ4x+Ox13HrVbvY8aYOVqBZWqqu5FIaan52l9jDTXckHAVcHMNEREREWlY/tqfe7aEB9NQIzvbBCpWaeGaNYdur15SYkoOA2nDfihpafDZZ4GNdTrhSVsmpzvX8RvtGcpCHMQB5nVYa7d8X3vr1u6sXXm5O+D0pZbxEq6U8QoRA895M9hTEBERkRBgtYS3GlA4HCbgshpQrFgBcXGBXashgi4wwWCgDTqGOedzm3MWANfyCts4CvAOuvyxMlmgjJZEJgVeIiKhwB7sCYhIqFixwpQHtmplMk0JCaZ8z2aDxEQTjO3fb8Y2ZbfCQMoVu7GJZxkJQC4PsJiBgCmftIKuQJpj7NhRv66GasAhoUiBl4SG89KCPQNpTvTzJiIhzmogYZXbWZsd79tnyvCsLoUxMYGvvaqvpKSaj7eihNcZQmtKWGY7jwd5CDCB4/Ll7nGHaprRENSAQ0KRAi8RERGREOPbQKJLF/PYpo07y2WzmUYVDoe7PLGxREW529n75+SzU2/jZP4DnTuz9h/zqSTada4lO9s0zvDcw8viWWpYX2rAIaFIgZeIiIhIHVklbbm5DXMdqzTOWs9lldtt22ayXEVFpgQRIDravQ+Wb0OOhuZvH7DYWHdAOJLn6LnhFcqJ5vl+r5HznDs9ZmXrwGSgysvNGjXfUsKcHFNi2BB8759IKFDgJSIiIlJHVkmbtf/UoVS39sizNK6m9UnZ2abLYUyM9z5Yjc1f4OVwmCxYKl/yJHcB8ACPcPur51BW5h7Xpo37c2WipDlT4CUiIiJSR1YgkZER2Hh/a498y+8mTXK3greOt25tMlu5ue4Nkisq/K/vqm3Djfo06GjHHhZxJS0o5V9cwuPcS2WldzfFAwfcnysTJc2ZAq/myh7sCYiIiIQ/K5B44IHAxluBWmqqO6vlW35n7WVlPVrBmm9mq7LSBE2eGSUw2anaBFOe2ayazvP9PuDkRW7iOH7g97ZHc0eruTiJqpIda8rOiyKhTIGXiIiISBOxArWCAnfmKzPTZK6sjZKtZhRRUd6lhf7auZeXe6+hsthsJsDzLVf0FwRZAVVKignmquuS6Pt9MpnG33ibUuK4uHgROx3t/XZZHDvW//VEmhsFXiIiIiINJDk5sL2jfNc6lZe7N0oeO9YcGzfOfG2VFiYnB971LyrKf0mflY2ygq0uXUwpYGwsHH20ybqVl9d8bZsN+kSv4vGoMQCMi5vKF7YzcDggPh7GjDHXi4k59KbJIs2JAi8RERGResjONkERBL53VE6OCb6mTnWv5QITiHmug8rMdB/bvt3765r28KqsNGvCWrd2dx70ZGWvtm93B30rV3qvzapOB+cvvFo5lOjKchg2jKkH73AFi9b8y8rMtZzOmjcy1kbH0pwo8JLQoU1tpSno50xEGpjnxr/VdezzF2BY53mWBT78sPfYnBz3Hl19+sDkye7zW7asPlCqrDSBVElJzftv9e5duw2Yo6hgnu1ajnT+BCecQP//PYstysajj5qg0MpuWa/BahSSm+s/uNJGx9KcKPASEQk2e7AnICL14bnx744d/kvr/AUY1nljx3qXBfqOXbHCZI6WL/cuA/S3tsuXv3VhlthY6NsXzjyz6rHx4/2vBxtPLv2dH1EW05Ke371O3mpTs1hZ6f3arNdgNQixnvOl9vLSnCjwCiEDz3kz2FMQiWzKdolIIwhk41/fBhqefLsAegYj2dlm3VVsrMl4+dtPqyaegY/FCqgcDpOJ8u2W2KePCZJ8v1c/8niQhwD4e+Vsvqo4xXUsKso7eLJeg2fg5y+4Unt5aU4UeDVn9mBPwA+9MRYRkQiUk2MaTzgcpvzOswzPNxPkGYx4Ntc41EbJUVE1Z7gsNQVvvXvDl1/CYYe5n7PZIJmfmMdwonDCLbcwL/p6r3NatvS+rvUarLVfjdVkQ2vEJJwo8BIRCSZ7sCcgIk3FygLZbN5ru6pbExYbC/v3m2DKs518VJT/MsDKSv8ZrpQUc77NZq7pGZz57gPmb11YtNPBAq6iI7+wnp7w1FOMGeMOqDxb43vOv3Vr83ljZrS0RkzCiQIvEWkelE0VkRCRlmaClqQkEzQsW2ae98zeWJsqO53uJhr33+8u33M6zefjx5tgqibbtrmvNWYMnHWW+5jT6b1WzAoGe/d2Pz4RM47efMZeElmW8Tq0bFml86JvANlUAZHWiEk4UeAlIiIi0gSsYKSgAFJTTUAEJsuUnW3WW5WUmM6F1pqw2Fgz1jN7lJRkPo+KcreiD7Qz4bRpsHq1++s+fbyzZzEx5nuvWGEej/z8bf5RPgWAtm+8ROaM46pc0986raYKiLRGTMKJAq8Q0+QNNuxN++0CosyEiIhEIM9gxHO9ltXMwlJebgIKh8NkqKzyv8mTTQBmlQHu2+feg6u83L1hsecmyzExJmtlBVft27s7I8bGmk6JZ59tvo6Kcm/iDPDWlB+YXXojAJ+lZdL6+ssDXkulgEikKgVeIhL5QjWYtwd7AiLS2Dy7En7yiXnO6fTem2v58qobI3vug2VxOt1rw/yJj/cu/cvONoHUihXQqpUZY2XZADp1Mt/DyoBFRZl5lpbCQ+MO8lHbIbRjL6tsZ3HhuslaSyVSTwq8RERERBqBVT7o2ZXQCl489+YCkyEaP94ETGlp7rJDz82VrRJDq+zQYp1nlfVVV/rna/t27722zjzTBInl5ZDyxD9ILizgVw5nqHMBByvMQrLU1Aa8QSLNjAIvCU2hmqGQ8KOfJREJEs/skFXy52/dk5UVmzTJBEhr1riPlZe7G3B4lhj6fp/MTP9lfZ7dBX21aeO911ZBgbnO9VGvcHP5s1Ri492r57EnoYtrDVlBQeCvX0S8KfASERERaQSeJX9jx7oDG88AyTcr5rtxsfX5ypXuAMm31LCkxLsk0ZPV0CM3t+qxffu899rKyoKcYRt5pvLvAEyOzebG+f0pLsbVPl7dA0XqToGXaJ2JSDDYgz0BEWkMycnuzXx9N0L2XCNl7dPlGRDFxJhSPpvNnSHzVFlpgp8HHnAHdFYrec9gzLMtvb8SQ2tsSorPPO8rhiFDSGA/S6P6cfC+Ca5z1CxDpP4UeIWgJu9sGKpUIib1pZ8hEWli1TWg8OxoaGW5rO6CYIIsh8M0urCyX337utdveZYpegZBViZq7Fj3tTyDPGvtmKfoaPO4e7fHk04njBwJ33wDycmc//M8HsqNbqjbIiIo8BIRERFpMNWV4/lmv3xZa6c8M1eTJrnXb61YYc53Ot3ZLGujZd/yRd89tKzgy9oX7Mwz/cxz9mx49VUTlS1YAB071vteePLMwok0VwFutyciIiIih7Jjh7v8rzqZmWZPLqfTBEEFBe4gKC3NrOey2aCiwmSuHn3UHJs2zbR6t9aCgff6rZwc96P1uSUnx3sD5+Jij4NffAGjR5vPJ02qWuPYAHyzcCLNkTJeYtiDPYFqqFRM6iqUf3bswZ6AiDQ1z4xPTo4pEYyPN+WEnmunrMxXTIy7sUZlpTtwsTZKTk2FsjL39XNz3Vmw6jJLvuWOrVvDI/f8DldeaS42eDDcfXejvH7fLJxIc6TAS0RERKSB5OZ6Bz6eGyF7rv/ybbZhsQIUz86Gffp4N8mw2UxWzOHwPnfatOqvC/6afTjpMf1G+N//oGtXmDOn+t2Z60nNOUQUeIUsNdgQEYlMkyZNwmazMdoq7QIOHjxIRkYGHTp0oHXr1lxxxRXs3LkzeJOUOnvmGf8BlrURcmqqCcRSU/1ngKwAxWrxnp1tNlnOyXFXAHo25bDExJikVXXX9ZWZCffHPsElFe9AXByzzl9E6y7ttAZLpBEp8JLQF8olYxKa9DMjIerzzz/nn//8J927d/d6PjMzk3/9618sWrSITz/9lB07dnD55ZcHaZZSH2VlJgiyAp/MTLPmy+k0nxcUeK+zevhh7/JAzw2PfZtpWGWI0dEmuLLawffpY8oWHQ7v69Ykp/8KHqkcZ7548knufa1XtZkyEWkYCrzEzR7sCYg0A/ZgT0CCpbi4mOHDh/Pcc89x2GGHuZ7fu3cvL7zwAlOnTuX888+nV69evPTSS6xatYrVq1cHccbiKdCufA6HCYKswCcnB+LiTJYqNxesP/rUVPc5nuWBvqWCnl9bZYjjxpngautWE5gtX17LNVQ7d8JVV5nuHddcA3//u9ZgiTQBdTUUkciibJeEqIyMDAYNGkS/fv3I9dg1d926dTgcDvr16+d6rlu3bhx11FHk5+dz5plnVrlWaWkppaWlrq+LiooAcDgcOHwX/gTAOqcu5zYXs2ebJhdPPWU+v+MO7/2xrHvXoYODESO811/dfTc8/rj5/LffoGVLs12WNebuu02JYkaGCaSszx0O72MPPAATJljfz3t+EyZUf8xLRQXRV19N1M8/4+zWjfIZM6C8PPDzG5l+FutP97D+ansPAx2nwEvCw3lpsGxNsGchIlInr732Gl9++SWff/55lWOFhYXExcXRrl07r+c7depEYWGh3+tNnDiRhx56qMrzH330Ea1atarzPPPy8up8bqR7/vmqz73/ftXnZszIq3LstNPMFlnVnX/aad7Xtz5//33vY/6+X211mz+fE5Ytozw+nuUZGexbvrz+F20E+lmsP93D+gv0Hu7fvz+gcQq8REREGtG2bdv4xz/+QV5eHi1atGiQa44bN44sj5qwoqIiUlJS6N+/P4mJibW+nsPhIC8vjwsvvJDYQ21C1czl5npnoCzWPRw16kJ++y2WhASzp1egkpNNSWFtz6tufr4ZOYBXb/yIPy1cZL745z/pc801df9GjUQ/i/Wne1h/tb2HVtXBoSjwqocRvMTLZDTa9Qee8yYfLNfiapGAhXqZoT3YE5BgWLduHbt27eK0005zPVdRUcHy5cuZMWMGH374IWVlZezZs8cr67Vz506SkpL8XjM+Pp74+Pgqz8fGxtbrjVZ9z28OHnrIfIBZ72WtvbLK9G6+OZYpU2K5/fZDb6Ts6bbbzLVuv93d7j0z03uzYc/vV90mxFOmmABuyhT3PAHYto1B828gCifPx9zGj9/dwLTDar5WMOlnsf50D+sv0HsY6H1Wcw3xZg/2BGoQ6m+qRUT8uOCCC9iwYQPr1693fZx++ukMHz7c9XlsbCxLlixxnbN582a2bt1Kenp6EGcuh+LZ+MJj2V6d9ququsdW1Q6DNe3RZfHbJKOsDIYOpQO/URB1GjvunRbQtUSkYSnwEhERaURt2rThlFNO8fpISEigQ4cOnHLKKbRt25YRI0aQlZXFsmXLWLduHTfddBPp6el+G2tI6PAMcp55xjxnPVr8tYrv06fmDonVdRgMpPOg342Kx4yB1auhbVtSv13EhEdbqIuhSBCo1FBEIoMyohLGpk2bRlRUFFdccQWlpaUMGDCAZ3zfwUvIyclxl+nZbOYxw2cFgpVZmjTJvfHxypXuY/7K/DyvG8jzNXrjDZg+3Xw+dy4ce2zdryUi9aKMl4QXvbmWcGUP9gSCa/ny5Vx66aUkJydjs9l4++23XcccDgdjxozh1FNPJSEhgeTkZK6//np2+HQY2L17N8OHDycxMZF27doxYsQIiouLvcb8+9//pk+fPrRo0YKUlBQee+yxKnNZtGgR3bp1o0WLFpx66qm83xCt4mrpk08+Ybr1Zhho0aIFM2fOZPfu3ZSUlPDmm29Wu75LQpPVyMKz4Qa4s1RWYAbQu3fN2aZA9ww7pG+/hZtvNp/fcw8MHlzPC4pIfSjwqqfb+GejXn/gOW826vX9sjf9txSpFwXkIa+kpIQePXowc+bMKsf279/Pl19+SXZ2Nl9++SVvvvkmmzdv5q9//avXuOHDh7Nx40by8vJ49913Wb58OSNHjnQdLyoqon///hx99NGsW7eOxx9/HLvdzrPPPusas2rVKq6++mpGjBhBQUEBl112GZdddhlff/114714adas0r8xY0ywlZ0NK1bUvA6sQdZfHTgAV14JRUUm0nv0Ua/DDRbciUjAVGoo4Ud7eomEnYEDBzJw4EC/x9q2bVtlr5QZM2bwl7/8ha1bt3LUUUexadMmFi9ezOeff87pp58OwNNPP83FF1/ME088QXJyMvPmzaOsrIwXX3yRuLg4Tj75ZNavX8/UqVNdAdqTTz7JRRddxL333gtATk4OeXl5zJgxg9mzZzfiHZDmrjalfZmZJuiq1/qrO++Er76CI46A116r0mLRM7hTyaFI01DGS0SksdmDPYHGU1RU5PVRWlraINfdu3cvNpvN1V49Pz+fdu3auYIugH79+hEVFcWaNWtcY8455xzi4uJcYwYMGMDmzZv5/fffXWP69evn9b0GDBhAfn5+g8xbpCH4bZBRG3PnwgsvmPrG+fPhyCOrDFFzDZGmp4yXhCdlvcSiMsND+vizv0JC7TfVrVGJ2SwyJSXF6+kHH3wQu91er0sfPHiQMWPGcPXVV7s2Ay4sLKRjx45e42JiYmjfvj2FhYWuMV27dvUa06lTJ9exww47jMLCQtdznmOsa4jUV26u2UMraPtjbdhgNgMDsNvB5xcNFjXXEGl6yniJf/ZgT0BEwsG2bdvYu3ev62PcuHH1up7D4WDo0KE4nU5mzZrVQLMUaTrPPON/fVaTrKkqKoIhQ8z6rgED3B0/RCQkKPCS8KVMh0jQJSYmen3Ex8fX+VpW0PXjjz+Sl5fnynYBJCUlsWvXLq/x5eXl7N6929X9LykpiZ07d3qNsb4+1Bh1EBR/6hIs3XGH/xK+Rt+w2OmEW2+F//4XunSBV16BKL3NEwkl+hvZACKys6FIOAiH4Nse7AmEByvo+vbbb/n444/p0KGD1/H09HT27NnDunXrXM8tXbqUyspK0tLSXGOWL1+Ow+FwjcnLy+OEE07gsMMOc41ZsmSJ17Xz8vJIT09vrJcmYaw+wZLT6f11o6+pmjkTFi6EmBjzePjhjfSNRKSuFHhJeAuHN94iQnFxMevXr2f9+vUAbNmyhfXr17N161YcDgdDhgzhiy++YN68eVRUVFBYWEhhYSFlZWUAnHjiiVx00UXceuutrF27ls8++4xRo0YxbNgwkpOTAbjmmmuIi4tjxIgRbNy4kQULFvDkk0+S5fFO9x//+AeLFy9mypQpfPPNN9jtdr744gtGjRrV5PdEQl9dgqXqSg3r0zDjkJm3tWvdk3zsMdAvEkRCkgIvqZ492BMQqYGC7rDyxRdfkJqaSmpqKgBZWVmkpqYyYcIEfvrpJ9555x22b99Oz5496dy5s+tj1apVrmvMmzePbt26ccEFF3DxxRfTu3dvrz262rZty0cffcSWLVvo1asXd999NxMmTPDa6+uss85i/vz5PPvss/To0YPXX3+dt99+m1NOOaXpboaEjboES9WVGtZHjZm3334z+3U5HHD55TB6dMN9YxFpUOpqKOFPHQ5FQl7fvn1x+tZeeajpmKV9+/bMnz+/xjHdu3dnxYoVNY658sorufLKKw/5/UTqYvx4eOihhr1mtft6VVbC9dfD1q1w3HHw4oumhbyIhCQFXiIijcUe7AmISCSotvX75Mnw/vsQHw+vvw5t2zb53EQkcCo1bCBqsBFkKjtrXvTnLSLN3SefuNvFP/009OwZzNmISAAUeImIiIiEk8JCGDbMlBpedx3cckuwZyQiAVDgJTWzB3sCtaAsiIiIhJla7xVWXg5XXw07d8LJJ8OsWVrXJRImFHiJSHgJlwDbHuwJiEg4qPVeYQ8+aMoMW7c267oSEhpzeiLSgBR4SWQJlzflIiISkmqdgaqnWu0V9t578Oij5vPnnoNu3Rp1biLSsBR4iUj4UGAtIo2s1hmoegp4r7AffzSt4wEyMswaLxEJKwq8GlDEdja0B+fb1pnenIuISB3VKgPVVMrKYOhQ2L0bTj8dpkwJ9oxEpA4iKvA65phjsNlsXh+TJk3yGvPvf/+bPn360KJFC1JSUnjssceCNFsRiVj2YE9AROoq4AxUU7rnHli7Fg47DBYtMvt2iUjYiajAC+Dhhx/m559/dn3ceeedrmNFRUX079+fo48+mnXr1vH4449jt9t59tlngzhjaRTKekUe/ZmKSIhpkvVgixaZfboAXn4ZjjmmEb+ZiDSmmGBPoKG1adOGpKQkv8fmzZtHWVkZL774InFxcZx88smsX7+eqVOnMnLkyCaeqYiIiIQzz/VgEyY0wjfYvBluvtl8PnYsXHJJI3wTEWkqEZfxmjRpEh06dCA1NZXHH3+c8vJy17H8/HzOOecc4uLiXM8NGDCAzZs38/vvv1d7zdLSUoqKirw+mh17sCdQB8qQiIhII2rU9WD798OVV5q6x3PPNTWQIhLWIirwuuuuu3jttddYtmwZf//733n00Ue57777XMcLCwvp1KmT1znW14WFhdVed+LEibRt29b1kZKSUu3YiG2wEa4UfEWGcPpztAd7AiLSVBp1PVhGBmzYAJ06wauvQkzEFSmJNDshH3iNHTu2SsMM349vvvkGgKysLPr27Uv37t257bbbmDJlCk8//TSlpaX1msO4cePYu3ev62Pbtm0N8dJEJBDhFHSJiDSEF1+EOXMgKsoEXZ07B3tGItIAQv7XJ3fffTc33nhjjWOOPfZYv8+npaVRXl7O//73P0444QSSkpLYuXOn1xjr6+rWhQHEx8cTrw5C5jf59iDPoS7OS4Nla4I9C2kO7MGegIiEva++MtkuMCm1884L7nxEpMGEfMbriCOOoFu3bjV+eK7Z8rR+/XqioqLo2LEjAOnp6SxfvhyHw+Eak5eXxwknnMBhhx3WYHNWuWEIUtYkPOnPTUSaQJN0JwzE3r0wZAgcPAgXX2waaohIxAj5wCtQ+fn5TJ8+na+++ooffviBefPmkZmZybXXXusKqq655hri4uIYMWIEGzduZMGCBTz55JNkhdQuidJo9CY+vITbn5c92BMQkbry7E4YNE4njBgB330HRx1lWsdHRczbNBEhggKv+Ph4XnvtNc4991xOPvlkHnnkETIzM7326Grbti0fffQRW7ZsoVevXtx9991MmDBBreRrwx7sCdRTuL2Zb6705yQiTahRuxMG6qmn4I03IDYWFi6EDh2COBkRaQwhv8YrUKeddhqrV68+5Lju3buzYsWKJphR4xp4zpt8sPzyYE8jPGnNV2hT0CUiTSwnJ8jd2levhnvuMZ8/8QSk6d9BkUgUMRmvUNPY67yCyh7sCTQAvbkPPeelhe+fiz3YExCRsPXrrzB0KJSXm3277rwz2DMSkUaiwCuMqcmGRIxwDbhEROqjshKuvRa2bYM//Qmefx5stmDPSkQaiQIvqRt7sCfQAPRmPzSE+5+DPdgTEJGw9eij8OGH0KIFvP46JCYGe0Yi0ogUeDWiiC43hMh4wxnub/rDne6/iDRXS5fCgw+az595Brp3D+58RKTRKfAKcyo3bAB68x8ckXDf7cGegIiEpR074OqrTanhTTeZDxGJeAq8pH7swZ5AA4mEICCc6H6LSHNVXm6Crl274NRTYcaMYM9IRJqIAq9G1hTlhsp6NRAFA00jUu6zPdgTEJGwNH48LF8ObdqYdV2tWgV7RiLSRBR4Sf3Zgz2BBhQpQUGo0v0Vkebs3Xdh8mTz+QsvwJ//HNz5iEiTipgNlEUkhCngEpHm7n//g+uvN5/fdZfZs0tEmhVlvJpAsyg3tAf32zcoBQkNKxLvpz3YExCRsFJaagKt33+HtDR4/PFgz0hEgkCBl4g/kRgsBIPuo4gI3H03fPEFtG8PCxdCXFywZyQiQaDAK4Io69XAFDTUT6TeP3uwJyAiYeW112DmTPP5K6/AUUcFdz4iEjQKvJpIxG+mHKkiNXhobLpvIiLwzTdwyy3m8wcegIEDgzsfEQkqBV7SsOzBnkAjUBBRO5F8v+zBnoCIhI2SEhgyxDyedx489FCwZyQiQabAK8IEvdwQIvPNaSQHEw1J90lEBJxOuOMO2LgRkpJg/nyIjg72rEQkyBR4iQRKQUXNIv3+2IM9AREJG88/Dy+/DFFRZo1XUlKwZyQiIUCBVxNqVuu87MGeQCM5Ly3yA4za0j0REXErKIA77zSfP/oonHtucOcjIiFDgVcEColyw0inYMPQPRARcYkpKSHmmmvMvl2XXAL33hvsKYlICFHgJY3HHuwJNIHmHIA1p9dtD/YERCTkOZ2kPv00tu+/h6OPhrlzTamhiMgf9C9CE2uqckNlvZpYcwvAmtNrFREJQNSTT5K8ejXOuDhYtMhsliwi4kGBlzQue7An0MSaQwAW6a/Plz3YExCRkLdqFVH33w9A5eOPwxlnBHlCIhKKFHiJNIZIDcAi8TWJNLKJEydyxhln0KZNGzp27Mhll13G5s2bvcYcPHiQjIwMOnToQOvWrbniiivYuXNnkGYstfLLLzB0KLbycrb36UPlbbcFe0YiEqIUeAVBsys3tAd7AkEUSQFYpLyO2rAHewISCT799FMyMjJYvXo1eXl5OBwO+vfvT0lJiWtMZmYm//rXv1i0aBGffvopO3bs4PLLLw/irCUgFRVw7bXw0084//xnvrrjDrDZgj0rEQlRMcGegEizYAUty9YEdx511RyDLpEGsnjxYq+v58yZQ8eOHVm3bh3nnHMOe/fu5YUXXmD+/Pmcf/75ALz00kuceOKJrF69mjPPPDMY05ZA5ObCRx9By5aUv/Ya5Vu3BntGIhLCFHhFuIHnvMkHy0Pgt6Z2lD0A7wAmlIMwBVr6eZVGs3fvXgDa/9F8Yd26dTgcDvr16+ca061bN4466ijy8/P9Bl6lpaWUlpa6vi4qKgLA4XDgcDhqPSfrnLqc21zZliwh+qGHsAHlM2fiOOEE2LpV97Ce9LNYf7qH9VfbexjoOAVeQXIb/2Q2fw/2NJqWHb2Z9RQqWTAFWSJNprKyktGjR3P22WdzyimnAFBYWEhcXBzt2rXzGtupUycKCwv9XmfixIk89NBDVZ7/6KOPaNWqVZ3nl5eXV+dzm5MWv/1G36wsYpxO/nfhhXzVvj38ce90DxuG7mP96R7WX6D3cP/+/QGNU+AlEmxNGYApyAqMPdgTkEiVkZHB119/zcqVK+t1nXHjxpGVleX6uqioiJSUFPr3709iYmKtr+dwOMjLy+PCCy8kNja2XnOLeA4H0RdeSNTevTh79ODIN97gyBYtdA8biO5j/eke1l9t76FVdXAoCryagZApNwRlvWrS0AGYgqy6sQd7ApGpoqICu93OK6+8QmFhIcnJydx4442MHz8e2x/NCJxOJw8++CDPPfcce/bs4eyzz2bWrFn86U9/cl1n9+7d3HnnnfzrX/8iKiqKK664gieffJLWrVu7xvz73/8mIyODzz//nCOOOII777yT++67r8lfs69Ro0bx7rvvsnz5crp06eJ6PikpibKyMvbs2eOV9dq5cydJSUl+rxUfH098fHyV52NjY+v1Rqu+5zcL998Pq1ZBYiK2N94gtk0br8O6hw1D97H+dA/rL9B7GOh9VlfDIGqq7oYSZurSCdE6x/NDJIRMnjyZWbNmMWPGDDZt2sTkyZN57LHHePrpp11jHnvsMZ566ilmz57NmjVrSEhIYMCAARw8eNA1Zvjw4WzcuJG8vDxXEDNy5EjX8aKiIvr378/RRx/NunXrePzxx7Hb7Tz77LNN+no9OZ1ORo0axVtvvcXSpUvp2rWr1/FevXoRGxvLkiVLXM9t3ryZrVu3kp6e3tTTlZr8v/8HTzxhPn/pJTjuuODOR0TCijJezYSyXmGopgyYAqvGYQ/2BCLXqlWrGDx4MIMGDQLgmGOO4dVXX2Xt2rWACU6mT5/O+PHjGTx4MAAvv/wynTp14u2332bYsGFs2rSJxYsX8/nnn3P66acD8PTTT3PxxRfzxBNPkJyczLx58ygrK+PFF18kLi6Ok08+mfXr1zN16lSvAK0pZWRkMH/+fP7f//t/tGnTxrVuq23btrRs2ZK2bdsyYsQIsrKyaN++PYmJidx5552kp6ero2Eo+eEHuOEG83lmJqjdv4jUkjJeIqFO2aymYQ/2BCLbWWedxZIlS/jvf/8LwFdffcXKlSsZOHAgAFu2bKGwsNCrs1/btm1JS0sjPz8fgPz8/9/evcdFVeZ/AP9wHUQd8MLVK66GWuZ1Raw0EyGjNjNNy0RNKgy2FNN0t+Boeam8dcF0M8Utr/irrU1LCEVLSYvANW9bamIpUCYCXrg+vz9mmRy5OMDMPOfMfN6v17wYZp45fObhnJnnO885ZzLh7e1tLLoAICwsDM7Ozjhw4ICxzZAhQ+Du7m5sExERgRMnTuDixYtWf561eeedd3Dp0iXcfffdCAgIMF62bNlibLN8+XLcf//9ePjhhzFkyBD4+/vjww9V8l2MBFy7BowdC1y6BISGAq++KjsREWkQZ7xIDgUc6BLVIeyOT/CF7BBmuvGA4rqOPZozZw6KiorQvXt3uLi4oLKyEgsWLMCECRMAwDgL5OfnZ/K468/sl5eXB19fX5P7XV1d0bp1a5M2N+7KV73MvLw8tGrVqrFPtdGEEDdt4+HhgaSkJCQlJdkgETXY9OnAd98BbdoAW7YAPG6GiBqBhZdktjytvKp2NyRSE0V2ACtbBMu/2lcYfnTo0MHk5sTERCiKUqP51q1bsWHDBmzcuNG4+9/06dMRGBiISdW7bxGp0YYNwOrVgJOT4foN6zwRkblYeJE8Cux/wEtk586ePWty+vLaZrsAYNasWZgzZw7Gjx8PAOjVqxfOnDmDRYsWYdKkScaz9+Xn5yMgIMD4uPz8fPTp0weA4ex/BQUFJsutqKjA77//bny8v78/8vPzTdpU/17XGQKJ6nT0KFB9bOBLLwEREXLzEJGm8RgvBzNyCI8ZIDKhyA5gSmvbqF6vN7nUVXhduXIFzs6mbzkuLi6oqqoCAAQFBcHf39/kzH5FRUU4cOCA8cx+oaGhKCwsRFZWlrHNrl27UFVVhZCQEGObvXv3ory83NgmLS0NwcHBUnYzJA0rKQHGjAGuXAHCwoCEBNmJiEjjWHipgEOfVl6RHYAcmiI7gON44IEHsGDBAmzfvh0//fQTPvroIyxbtgwPPfQQAMDJyQnTp0/HK6+8gk8++QSHDx9GVFQUAgMDMWrUKABAjx49cO+99+LJJ5/EwYMHsW/fPsTFxWH8+PEIDAwEADz22GNwd3fH1KlTceTIEWzZsgVvvPGGyZcNE92UEEBMDHDsGBAYaNjF0MVFdioi0jjuakhEpBJam+1qiLfeegsvvfQSnnnmGRQUFCAwMBBPP/00Eq6bRZg9ezYuX76Mp556CoWFhbjzzjvx+eefw8PDw9hmw4YNiIuLw/Dhw41foPzmm28a7/fy8kJqaipiY2PRv39/tG3bFgkJCdJOJU8atXr1H8XWli3ADSd1ISJqDBZeDkh1J9lQwJkHsj1FdgDH0rJlS6xYsQIrVqyos42TkxPmz5+P+fPn19mmdevW2LhxY71/6/bbb8eXX37Z2Kjk6LKygOeeM1xfvBi48065eYjIbnBXQ5Vw6N0NAQ6CybYU2QFqsufZLiLNuHjR8H1dZWXAgw8CM2fKTkREdoSFl4PiII+IiOg6QgCTJwOnTwNBQUBysuEU8kREFsLCi9RDkR2AHIIiO0BN/CCESAWWLAE++QTQ6YBt2wBvb9mJiMjOsPBSEYff3ZDI2hTZAYhIlb78Epg713D9jTeAfv3k5iEiu8TCy4Gp8lN2RXYAIiJyKAUFwPjxQGUlMGHCH1+YTERkYSy8iMgxKLID1E6VH4AQOYrKSuCxx4Bz54AePYBVq3hcFxFZDQsvlbH17oaqHPQpsgOQ3VFkByAiVZo3D0hPB5o3B/7v/4AWLWQnIiI7xsKL1EmRHYDI+lT5wQeRo9i5E3jlFcP1f/zDMONFRGRFLLyIgz+yb4rsALXjdkck0dmzhuO5hABiYgy7GxIRWRkLLxXi2Q3/R5EdgDRPkR2AiFSnvBwYNw64cMFw9sLly2UnIiIHwcKLAKj403dFdgDSLEV2gLqpdnsjcgQvvABkZgJeXkBKCuDhITsRETkIFl4qxVmv6yiyA5DmKLID1I1FF5FEH374xwzX+vVAly5y8xCRQ2HhRUaqHhAqsgOQZiiyAxCRKv34IzBliuH6888DDz4oNw8RORwWXk1w3+FdVl2+jFkvVRdfRDejyA5QP25fRJJcvQqMHQsUFQF33gksXCg7ERE5IBZepB2K7ACkaorsAESkWs8+C+TkAD4+wObNgJub7ERE5IBYeDXRXw6lWnX5nPW6gSI7AKmSIjvAzal6uyKyZ//8J7BmDeDkBGzcCLRrJzsRETkoFl6kPQo0MdAmG1FkB7g5Fl1Eknz/veF7ugBAUYCwMKlxiMixsfCiWmlioKjIDkBERKpVXAyMGWM4vis8HHjxRdmJiMjBsfCyAHvc3VAzFNkBSCpFdoCb08SHGET2RgjgqaeAEyeA9u2BDz4AnDnkISK5+CpEddLMgFGRHYCkUGQHuDnNbENE9mblSsNJNFxdgS1bDCfVICKSjIWXhXDWSzJFdgCyKUV2ACJSrW++AWbMMFx/7TVg8GC5eYiI/oeFF9VLU5/YK+CA3BEosgOYR1PbDpG9+P13w/d1lZcDo0cD06fLTkREZMTCy4LsddZLcwNIRXYAshpFdgAiUq2qKiAqCjhzBvjTn4C1aw2nkCciUgkWXmSfFNkByOIU2QHMp7kPK4jswWuvAdu3AzodsG0b4OUlOxERkQkWXhbGWS8VUWQHIItRZAcwnya3FSKt27MH+PvfDdfffhvo00dqHCKi2rDwIvumQFODdqqFIjsAEalaXh4wfvwfuxpOnSo7ERFRrVh4kdk0/Um+IjsANYoiO0DDaHobIdKiigrg0UcNxdettxpOI8/juohIpVh4WYG97m6oeYrsANQgiuwADcOii0iCxEQgIwNo0QL4v/8DmjeXnYiIqE6aKbwWLFiAwYMHw9PTE97e3rW2yc3NRWRkJDw9PeHr64tZs2ahoqLCpE1GRgb69esHnU6Hrl27Ijk52frhrYDHejWSIjsA3ZQC/p+I6OZ27AAWLjRcX7MGCA6Wm4eI6CY0U3iVlZVh7NixmDZtWq33V1ZWIjIyEmVlZdi/fz/Wr1+P5ORkJCQkGNucPn0akZGRGDZsGHJycjB9+nRER0dj586dFs9r7VkvmVh8kdUosgM0jua3CSKtOXMGmDjRcD02Fhg3Tm4eIiIzaKbwmjdvHmbMmIFevXrVen9qaiqOHj2KDz74AH369MHIkSPx8ssvIykpCWVlZQCAVatWISgoCEuXLkWPHj0QFxeHMWPGYPny5bZ8KhYjc5dDzQ80FWh2kG+XFPD/QUTmKSsDHnnE8GXJAwYAS5fKTkREZBbNFF43k5mZiV69esHPz894W0REBIqKinDkyBFjm7CwMJPHRUREIDMzs95ll5aWoqioyORiDnue9bIbiuwADk6B5v8Hmv8Qgkhrnn8eOHgQaNUKSEkxfG8XEZEG2E3hlZeXZ1J0ATD+npeXV2+boqIiXL16tc5lL1q0CF5eXsZLhw4dLJy+8TjrZQGK7AAOSpEdoOnsZhsg0oqUFOCttwzX//lPoHNnqXGIiBpCauE1Z84cODk51Xs5fvy4zIgAgLlz5+LSpUvGy9mzZ81+rL3PetnNwFORHcCBKGB/E1HDnTgBPPGE4fqcOcD998vNQ0TUQK4y//jMmTMxefLkett06dLFrGX5+/vj4MGDJrfl5+cb76v+WX3b9W30ej2aNWtW57J1Oh10Kt6VIQarsQpPS/v7I4d8iM/2jpb29y1GueEnWZ4iO4Dl2M2HDkRacOUKMHYsUFICDB0KvPyy7ERERA0mtfDy8fGBj4+PRZYVGhqKBQsWoKCgAL6+vgCAtLQ06PV69OzZ09hmx44dJo9LS0tDaGioRTKQnVBgVwWCKiiyA1gWiy4iG4uNBQ4fBnx9gU2bAFepwxciokbRzDFeubm5yMnJQW5uLiorK5GTk4OcnByUlJQAAMLDw9GzZ09MnDgRhw4dws6dO/Hiiy8iNjbWOFsVExODU6dOYfbs2Th+/DhWrlyJrVu3YsaMGVbNbovdDWV/qbLdDUQV2QHshAL2JRE1zdq1QHIy4OwMbN4MBATITkRE1CiaKbwSEhLQt29fJCYmoqSkBH379kXfvn3x7bffAgBcXFzw6aefwsXFBaGhoXj88ccRFRWF+fPnG5cRFBSE7du3Iy0tDb1798bSpUuxZs0aREREyHpaFsXiy8IU2QE0TpEdwDrsbj0nUrNDhwyzXQAwfz4wbJjcPERETaCZufrk5GQkJyfX26ZTp041diW80d13343s7GwLJjPPXw6l4pPe4Tb/u7ZmN8d7VVNgtwWE1SiyA1gPiy4iGyoqMhzXde0aMHIkMHeu7ERERE2imRkvMo/sWS/ADgenCuy6mLAoRXYAIrILQgDR0cAPPwAdOgDvv2/Y1ZCISMP4KmZD9n5qebungIVFXRTYfd/Y3QcKRGr21luG7+xyczP8bNNGdiIioiZj4WWHOOtlZQocotAwiwL2AxFZ1oEDwPPPG64vWQKEhMjNQ0RkISy8yGrsuviqpsBxCw9FdgDbcYh1mUgNLlwwHNdVXm74+de/yk5ERGQxLLxszFa7G6ph1gtwoAGrAscpwhQ4xvP8H4dZh4lkq6oCJk4Ezp4FunUD1qwBnJxkpyIishgWXkSWpsB+ixNFdgAi+5aUlITOnTvDw8MDISEhOHjwoOxItrNoEfDZZ4CHB7BtG6DXy05ERGRRLLwk4KyXA1FgH0WYAu0/h0Zw6HWXbG7Lli2Ij49HYmIivvvuO/Tu3RsREREoKCiQHc36du8GEhIM11euBG6/XW4eIiIrYOFl51h8qYgC7RQwCrSV1wq4zpKtLVu2DE8++SSmTJmCnj17YtWqVfD09MTatWtlR7Ouc+eA8eMNuxpOmWK4EBHZIRZekjjiqeU5kL2OAvlFjVLPxcFxXbW+xYsXw8nJCdOnTzfedu3aNcTGxqJNmzZo0aIFHn74YeTn55s8Ljc3F5GRkfD09ISvry9mzZqFiooKkzYZGRno168fdDodunbtiuTkZBs8o6YpKytDVlYWwsLCjLc5OzsjLCwMmZmZEpNZWUUF8OijQEGBYZbr7bdlJyIishpX2QHI+mKwGqvwtOwYVBeljuvWWD6RCnzzzTdYvXo1br9hd7IZM2Zg+/btSElJgZeXF+Li4jB69Gjs27cPAFBZWYnIyEj4+/tj//79OH/+PKKiouDm5oaFCxcCAE6fPo3IyEjExMRgw4YNSE9PR3R0NAICAhAREWHz52qu3377DZWVlfDz8zO53c/PD8ePH6/RvrS0FKWlpcbfi4qKAADl5eUoLy9v8N+vfkxjHtsUzn/7G1z27oVo2RIVmzYZvrfLxhksRVYf2hv2Y9OxD5uuoX1objsWXhL95VAqPukdLjuGTY0c8iE+2ztadgz1Uuq43pDHUZNwtsu6SkpKMGHCBLz77rt45ZVXjLdfunQJ7733HjZu3Ih77rkHALBu3Tr06NEDX3/9NQYNGoTU1FQcPXoUX3zxBfz8/NCnTx+8/PLLeOGFF6AoCtzd3bFq1SoEBQVh6dKlAIAePXrgq6++wvLly1VdeDXUokWLMG/evBq3p6amwtPTs9HLTUtLa0qsBvH75hsMWrIEAPDNtGk4/8MPwA8/2OzvW4st+9CesR+bjn3YdOb24ZUrV8xqx8LLQahp1ovFl5mUG64rtbYiC2LRZX2xsbGIjIxEWFiYSeGVlZWF8vJyk13tunfvjo4dOyIzMxODBg1CZmYmevXqZTIrFBERgWnTpuHIkSPo27cvMjMzTZZR3eb6XRrVqG3btnBxcamxa2V+fj78/f1rtJ87dy7i4+ONvxcVFaFDhw4IDw+HvhFnAywvL0daWhpGjBgBNze3hj+BhvrpJ7j+71iuyrg49H3lFfS1/l+1Kpv3oZ1iPzYd+7DpGtqH1Xsd3AwLL5KCxVcDKbID2D81FV1TsQ5fyA5hphvfbHQ6HXQ6Xa1tN2/ejO+++w7ffPNNjfvy8vLg7u4Ob29vk9v9/PyQl5dnbFPbrnjV99XXpqioCFevXkWzZs3Mf3I25O7ujv79+yM9PR2jRo0CAFRVVSE9PR1xcXE12tfVz25ubk0aaDX18WYpLQUeewy4eBEICYHL0qVwsaPBoU360AGwH5uOfdh05vahuf3MwksyW+5uqKZZL4DFF6mHmoouq/jyWwDNLbzQywCADh06mNyamJgIRVFqtD579iyee+45pKWlwcPDw8JZ7EN8fDwmTZqEAQMGYODAgVixYgUuX76MKfZ2lr/4eODbb4HWrYGtWwF3d9mJiIhsgoWXg1Fb8UUkm9qKrhishnl7iqvD2bNnTXZtq2u2KysrCwUFBejXr5/xtsrKSuzduxdvv/02du7cibKyMhQWFprMel2/q52/v3+NLxSu3jXv+ja17a6n1+tVO9tVbdy4cfj111+RkJCAvLw89OnTB59//nmNGTxN27zZ8D1dAPDBB0DHjnLzEBHZEE8nrwKOeGr5amob9JJj4frXdHq93uRSV+E1fPhwHD58GDk5OcbLgAEDMGHCBON1Nzc3pKenGx9z4sQJ5ObmIjQ0FAAQGhqKw4cPm3yhcFpaGvR6PXr27Glsc/0yqttUL0Pt4uLicObMGZSWluLAgQMICQmRHclyjh0DoqMN1//+d2DkSLl5iIhsjIWXA1LLlypX4+CXZFDjeqe2bdOSWrZsidtuu83k0rx5c7Rp0wa33XYbvLy8MHXqVMTHx2P37t3IysrClClTEBoaikGDBgEAwsPD0bNnT0ycOBGHDh3Czp078eKLLyI2NtZY8MXExODUqVOYPXs2jh8/jpUrV2Lr1q2YMWOGzKdPly8DY8cafg4bBtRyRkYiInvHwkslHHnWC1DnIJjslxrXN3suusy1fPly3H///Xj44YcxZMgQ+Pv748MP//hfubi44NNPP4WLiwtCQ0Px+OOPIyoqCvPnzze2CQoKwvbt25GWlobevXtj6dKlWLNmjV2dSl5zhACmTQOOHAH8/YGNGwEXF9mpiIhsjsd4OSge60WOSo1Fl6PKyMgw+d3DwwNJSUlISkqq8zGdOnXCjh076l3u3XffjezsbEtEJEtYswZ4/33A2dlwjFctp8cnInIEnPFSEVvPeqntE3YOiMna1LqOqW1bJLKY7Gzgr381XF+wABg6VG4eIiKJWHiRqqh1YEzap9Z1i0UX2a1LlwzHdZWWAvffD8yeLTsREZFULLxUxtFnvQD1DpBJu9S6Tqlx+yOyCCGAKVOAkyeBTp2A9esNuxoSETkwvgqSKgd/ah0ok/aodV1S43ZHZDErVgAffWT4cuSUFMOXJRMROTgWXirk6Gc4rKbWATNpB9chIgn27/9jt8Jly4A//1luHiIilWDhpVLc5dCAA2dqjJFDPlT1uqPW7Y2oyX79FXjkEaCiAhg/HnjmGdmJiIhUg4UXGal1MKjmATSpj9rXF7VuZ0RNVlkJPP448MsvQHAw8I9/AE5OslMREakGC6+mWGHdxcvY5VCtg0K1D6ZJHdS+nqh1+yKyiAULgNRUoFkzYNs2oGVL2YmIiFSFhRdphtoH1SSX2tcPFl1k1774AlAUw/VVq4DbbpMah4hIjVh4NdWr1l08Z71Mqf3YHZKD6wSRRL/8Ajz2mOEU8tHRQFSU7ERERKrEwotqpebiC2ABRn/Qwnqg9u2JqNHKy4Fx4wwn1ejTB3jrLdmJiIhUi4WXJdjhrBegjcGiFgbdZB1aKb61sB0RNdrf/gbs2wfo9Ybjujw8ZCciIlItFl6WYqfFlxZoZQBOlqOV/zeLLrJr//oXsGSJ4fq6dcCf/iQ1DhGR2rHwonppaeColcE4NY1W/s9a2naIGuzUKWDyZMP1GTOA0aOlxiEi0gIWXpZkp7NeWhpAcvbLvmnlf6ulbYaowa5dA8aMAS5dAgYPBl618psfEZGdYOFFdokFmP3h/5NIJaZPB7KzgbZtgc2bATc32YmIiDSBhZelcdZLVThYtw9a+j9qdVshMssHHwCrVwNOTsCGDUCHDrITERFphqvsAKQdMViNVXhadowGqx60f7aXxyBojZYKLoBFF9m5I0eAp//3HvDSS0B4uNw8REQawxkva7DTWS9A2wNLrQ3iHZkWdxXV8rZBdFMlJcDYscCVK0BYGJCQIDsREZHmsPCyFhZfqqTFAb0j0er/R8vbBNFNCWGY6Tp2DAgMNOxi6OIiOxURkeaw8NIwFl+Np8XBvT3TasFF5BBWrwY2bjQUW1u2AL6+shMREWkSCy9rsvMz7NpD8cXBvlz28D/Q+nZAVK+sLOC55wzXFy8G7rxTbh4iIg1j4aVxMme9APsYdNrD4F9r7KXP7WH9J6rTxYuG47rKyoAHHwRmzpSdiIhI01h4WZsNZr1kF1/2wh4KAbWzl4ILYNFFdk4IYMoU4PRpICgISE42nEKeiIgajaeTpybT6mnma8NTz1uHvRRb1Vh0kd1buhT4+GPA3R1ISQG8vWUnIiLSPM542YIDzHrZ20DU3goFWexphovIYXz5JTBnjuH6G28A/fvLzUNEZCdYeNkKiy/NYdHQePbcd/a2nhOZKCgAxo8HKiuBCRP++MJkIiJqMhZedobFl+XZcxFhafbeV/a4fhMZVVbCJSoKOHcO6NEDWLWKx3UREVkQCy9bsvPTy1ez18GpvRcVTeEIfWOv6zVRteCtW+G8axfg6Qls2wa0aCE7EhGRXeHJNezQXw6l4pPe4VIz2NMJN250Y4HhyCfisPdiqxqLLrJ3TqmpCN661fDLu+8CPXvKDUREZIdYeNnaqwBesP6fYfFlO45YiDlKwQWw6CIHIAScFQVOQqDyqafg8thjshMREdklFl52jMWXHPZaiDlSsUXkUJycULl9O04+/TQ6L1kCF9l5iIjsFAsvGWw066UWjlh8XU/rhZgjF1yc7SKH0aoVjk6ejM4eHrKTEBHZLRZesjjQLocAi6/rXV/IqKEIc+TCqj4suoiIiMiSWHg5ABZf6mWL2TAWVg3HoouIiIgsjYWXTA62yyHA4utmGlqIsaiyLBZcREREZC0svByEWma9ABZfDcHCynZYdBEREZE18QuUZbPhlyr/5VCq7f7YTXCQS2qitvXxvsO7ZEcgIiIiC2Ph5WBYfBGZUtt6qKZtlIiIiCyHhZca2HDWS23UNuglx6K29Y9FFxERkf1i4aUWDrrLIaC+wS85BrWtd2rbLomIiMiyWHg5KLUN8tQ2CCb7FYPVXN+IiIjI5lh4qYmNdzlk8UWORq3rmNq2RSIiIrI8Fl6kKmodGJP2qXXdYtFFRETkGFh4qY2Dz3oB6h0gk3apdZ1S4/ZHRERE1sHCi1Q5+FPrQJm0R63rkhq3OyIiIrIeFl5q5MCnl7+eWgfMpB1ch4iIiEgtNFN4LViwAIMHD4anpye8vb1rbePk5FTjsnnzZpM2GRkZ6NevH3Q6Hbp27Yrk5GTrh28M7nIIgANnahy1n7lQrdubLSQlJaFz587w8PBASEgIDh48KDsSERGRTWim8CorK8PYsWMxbdq0etutW7cO58+fN15GjRplvO/06dOIjIzEsGHDkJOTg+nTpyM6Oho7d+60cnptUOtgUM0DaFIfta8vat3ObGHLli2Ij49HYmIivvvuO/Tu3RsREREoKCiQHY2IiMjqNFN4zZs3DzNmzECvXr3qbeft7Q1/f3/jxcPDw3jfqlWrEBQUhKVLl6JHjx6Ii4vDmDFjsHz58kZl+npbox5mPgm7HKp1UKj2wTSpg9rXE7VuX7aybNkyPPnkk5gyZQp69uyJVatWwdPTE2vXrpUdjYiIyOpcZQewtNjYWERHR6NLly6IiYnBlClT4OTkBADIzMxEWFiYSfuIiAhMnz693mWWlpaitLTU+PulS5cAAJcBFJVbNH5NJVZefi2uFFXY/o+aIQpJAID3MEVyElKjqViHK7JD1OO+w7tQZGbbosuGn0IIC/31yxZaTs1lFhWZPiudTgedTlejdVlZGbKysjB37lzjbc7OzggLC0NmZqYV8jmW6nXlxv+HucrLy3HlyhUUFRXBzc3NktEcBvvQMtiPTcc+bLqG9mH1a+/N3rftqvCaP38+7rnnHnh6eiI1NRXPPPMMSkpK8OyzzwIA8vLy4OfnZ/IYPz8/FBUV4erVq2jWrFmty120aBHmzZtX4/bRAGDtWS9rL79Wu2T80QZQez6S4QvZAazgwoUL8PLyavTj3d3d4e/vj7y8v1gw1R9atGiBDh06mNyWmJgIRVFqtP3tt99QWVlZ62vw8ePHrZLPkRQXFwNAjf8HERHZTnFxcb3v21ILrzlz5uDVV+vfn+7YsWPo3r27Wct76aWXjNf79u2Ly5cv4/XXXzcWXo01d+5cxMfHG38vLCxEp06dkJub26RBkQxFRUXo0KEDzp49C71eLztOgzC7HMxue5cuXULHjh3RunXrJi3Hw8MDp0+fRllZmYWSmRJCGPcoqFbbbBdZX2BgIM6ePYuWLVvW+J+YQ6vbipqwDy2D/dh07MOma2gfCiFQXFyMwMDAettJLbxmzpyJyZMn19umS5cujV5+SEgIXn75ZZSWlkKn08Hf3x/5+fkmbfLz86HX6+uc7QLq3nXGy8tLsyu0Xq9ndgmYXQ6tZnd2bvphuB4eHibHusrStm1buLi41Poa7O/vLymV/XB2dkb79u2bvBytbitqwj60DPZj07EPm64hfWjOZIzUwsvHxwc+Pj5WW35OTg5atWplLJpCQ0OxY8cOkzZpaWkIDQ21WgYiIjLs9ti/f3+kp6cbzzZbVVWF9PR0xMXFyQ1HRERkA5o5xis3Nxe///47cnNzUVlZiZycHABA165d0aJFC/z73/9Gfn4+Bg0aBA8PD6SlpWHhwoV4/vnnjcuIiYnB22+/jdmzZ+OJJ57Arl27sHXrVmzfvl3SsyIichzx8fGYNGkSBgwYgIEDB2LFihW4fPkypkzhCXOIiMj+aabwSkhIwPr1642/9+3bFwCwe/du3H333XBzc0NSUhJmzJgBIQS6du1qPHVxtaCgIGzfvh0zZszAG2+8gfbt22PNmjWIiIhoUBadTofExERNHsvA7HIwuxxaza7V3Dczbtw4/Prrr0hISEBeXh769OmDzz//vMYJN8j27HWdsyX2oWWwH5uOfdh01upDJ2G58xUTERERERFRLTTzBcpERERERERaxcKLiIiIiIjIylh4ERERERERWRkLLyIiIiIiIitj4VWPBQsWYPDgwfD09IS3t3etbXJzcxEZGQlPT0/4+vpi1qxZqKioMGmTkZGBfv36QafToWvXrkhOTrZ++Fp07twZTk5OJpfFixebtPnPf/6Du+66Cx4eHujQoQNee+01KVlvlJSUhM6dO8PDwwMhISE4ePCg7Eg1KIpSo3+7d+9uvP/atWuIjY1FmzZt0KJFCzz88MM1vkzWVvbu3YsHHngAgYGBcHJywr/+9S+T+4UQSEhIQEBAAJo1a4awsDD88MMPJm1+//13TJgwAXq9Ht7e3pg6dSpKSkqkZ588eXKN/8O9994rPfuiRYvw5z//GS1btoSvry9GjRqFEydOmLQxZx0x5zWH6EY3225u9OGHH2LEiBHw8fGBXq9HaGgodu7caZuwKtXQPrzevn374Orqij59+lgtnxY0pg9LS0vx97//HZ06dYJOp0Pnzp2xdu1a64dVqcb04YYNG9C7d294enoiICAATzzxBC5cuGD9sCplzvtxbVJSUtC9e3d4eHigV69eNb4b2BwsvOpRVlaGsWPHYtq0abXeX1lZicjISJSVlWH//v1Yv349kpOTkZCQYGxz+vRpREZGYtiwYcjJycH06dMRHR0t7Q1s/vz5OH/+vPHy17/+1XhfUVERwsPD0alTJ2RlZeH111+Hoij4xz/+ISVrtS1btiA+Ph6JiYn47rvv0Lt3b0RERKCgoEBqrtrceuutJv371VdfGe+bMWMG/v3vfyMlJQV79uzBuXPnMHr0aCk5L1++jN69eyMpKanW+1977TW8+eabWLVqFQ4cOIDmzZsjIiIC165dM7aZMGECjhw5grS0NHz66afYu3cvnnrqKenZAeDee+81+T9s2rTJ5H4Z2ffs2YPY2Fh8/fXXSEtLQ3l5OcLDw3H58mVjm5utI+a85hDVxpzt5np79+7FiBEjsGPHDmRlZWHYsGF44IEHkJ2dbeWk6tXQPqxWWFiIqKgoDB8+3ErJtKMxffjII48gPT0d7733Hk6cOIFNmzYhODjYiinVraF9uG/fPkRFRWHq1Kk4cuQIUlJScPDgQZOvW3I05rwf32j//v149NFHMXXqVGRnZ2PUqFEYNWoUvv/++4b9cUE3tW7dOuHl5VXj9h07dghnZ2eRl5dnvO2dd94Rer1elJaWCiGEmD17trj11ltNHjdu3DgRERFh1cy16dSpk1i+fHmd969cuVK0atXKmF0IIV544QURHBxsg3R1GzhwoIiNjTX+XllZKQIDA8WiRYskpqopMTFR9O7du9b7CgsLhZubm0hJSTHeduzYMQFAZGZm2ihh7QCIjz76yPh7VVWV8Pf3F6+//rrxtsLCQqHT6cSmTZuEEEIcPXpUABDffPONsc1nn30mnJycxC+//CItuxBCTJo0STz44IN1PkYt2QsKCgQAsWfPHiGEeeuIOa85RDdT23Zjjp49e4p58+ZZPpAGNaQPx40bJ1588cV63yMckTl9+NlnnwkvLy9x4cIF24TSGHP68PXXXxddunQxue3NN98U7dq1s2Iybbnx/bg2jzzyiIiMjDS5LSQkRDz99NMN+luc8WqCzMxM9OrVy+TLPyMiIlBUVIQjR44Y24SFhZk8LiIiApmZmTbNWm3x4sVo06YN+vbti9dff91kF6XMzEwMGTIE7u7uxtsiIiJw4sQJXLx4UUZclJWVISsry6QPnZ2dERYWJq0P6/PDDz8gMDAQXbp0wYQJE5CbmwsAyMrKQnl5ucnz6N69Ozp27Ki653H69Gnk5eWZZPXy8kJISIgxa2ZmJry9vTFgwABjm7CwMDg7O+PAgQM2z3yjjIwM+Pr6Ijg4GNOmTTPZpUIt2S9dugQAaN26NQDz1hFzXnOIrKGqqgrFxcXG9ZXMs27dOpw6dQqJiYmyo2jSJ598ggEDBuC1115Du3btcMstt+D555/H1atXZUfTjNDQUJw9exY7duyAEAL5+fnYtm0b7rvvPtnRVOPG9+PaWGo879rweFQtLy/PZAAEwPh7Xl5evW2Kiopw9epVNGvWzDZhATz77LPo168fWrdujf3792Pu3Lk4f/48li1bZswaFBRUI2v1fa1atbJZ1mq//fYbKisra+3D48eP2zxPfUJCQpCcnIzg4GCcP38e8+bNw1133YXvv/8eeXl5cHd3r3GsoJ+fn3FdUYvqPLX1+fXrta+vr8n9rq6uaN26tfTnc++992L06NEICgrCyZMn8be//Q0jR45EZmYmXFxcVJG9qqoK06dPxx133IHbbrsNAMxaR8x5zSGyhiVLlqCkpASPPPKI7Cia8cMPP2DOnDn48ssv4erK4VZjnDp1Cl999RU8PDzw0Ucf4bfffsMzzzyDCxcuYN26dbLjacIdd9yBDRs2YNy4cbh27RoqKirwwAMPNHiXWXtV2/txbep6/23oe6/DvRLMmTMHr776ar1tjh07ZnJSBDVryPOJj4833nb77bfD3d0dTz/9NBYtWgSdTmftqHZv5MiRxuu33347QkJC0KlTJ2zdutWmBbajGz9+vPF6r169cPvtt+NPf/oTMjIyVHOMRWxsLL7//nuTYwCJ1Grjxo2YN28ePv744xofWlDtKisr8dhjj2HevHm45ZZbZMfRrKqqKjg5OWHDhg3w8vICACxbtgxjxozBypUr+d5qhqNHj+K5555DQkICIiIicP78ecyaNQsxMTF47733ZMeTztbvxw5XeM2cOROTJ0+ut02XLl3MWpa/v3+Ns+tVn4HM39/f+PPGs5Ll5+dDr9db5AWjKc8nJCQEFRUV+OmnnxAcHFxnVuCP52Nrbdu2hYuLS625ZGUyl7e3N2655Rb8+OOPGDFiBMrKylBYWGgyo6HG51GdJz8/HwEBAcbb8/PzjWfk8vf3r3Fyk4qKCvz++++qez5dunRB27Zt8eOPP2L48OHSs8fFxRlP6NG+fXvj7f7+/jddR8x5zSGypM2bNyM6OhopKSk1drOhuhUXF+Pbb79FdnY24uLiABiKCCEEXF1dkZqainvuuUdySvULCAhAu3btjEUXAPTo0QNCCPz888/o1q2bxHTasGjRItxxxx2YNWsWAMMHw82bN8ddd92FV155xeR93tHU9X5cm7rGyA1973W4Y7x8fHzQvXv3ei/XH+NUn9DQUBw+fNhkEJeWlga9Xo+ePXsa26Snp5s8Li0tDaGhodKfT05ODpydnY2fYIaGhmLv3r0oLy83yRocHCxlN0MAcHd3R//+/U36sKqqCunp6RbrQ2spKSnByZMnERAQgP79+8PNzc3keZw4cQK5ubmqex5BQUHw9/c3yVpUVIQDBw4Ys4aGhqKwsBBZWVnGNrt27UJVVRVCQkJsnrk+P//8My5cuGB8c5GVXQiBuLg4fPTRR9i1a1eN3XrNWUfMec0hspRNmzZhypQp2LRpEyIjI2XH0RS9Xo/Dhw8jJyfHeImJiUFwcDBycnJU9zqpVnfccQfOnTtn8nUf//3vf+Hs7HzTgTIZXLlyBc7OpsN9FxcXAIb3JUd0s/fj2lhsPN/AE384lDNnzojs7Gwxb9480aJFC5GdnS2ys7NFcXGxEEKIiooKcdttt4nw8HCRk5MjPv/8c+Hj4yPmzp1rXMapU6eEp6enmDVrljh27JhISkoSLi4u4vPPP7fpc9m/f79Yvny5yMnJESdPnhQffPCB8PHxEVFRUcY2hYWFws/PT0ycOFF8//33YvPmzcLT01OsXr3apllvtHnzZqHT6URycrI4evSoeOqpp4S3t7fJmd3UYObMmSIjI0OcPn1a7Nu3T4SFhYm2bduKgoICIYQQMTExomPHjmLXrl3i22+/FaGhoSI0NFRK1uLiYuP6DEAsW7ZMZGdnizNnzgghhFi8eLHw9vYWH3/8sfjPf/4jHnzwQREUFCSuXr1qXMa9994r+vbtKw4cOCC++uor0a1bN/Hoo49KzV5cXCyef/55kZmZKU6fPi2++OIL0a9fP9GtWzdx7do1qdmnTZsmvLy8REZGhjh//rzxcuXKFWObm60j5rzmENXmZtv8nDlzxMSJE43tN2zYIFxdXUVSUpLJ+lpYWCjrKUjX0D68Ec9q2PA+LC4uFu3btxdjxowRR44cEXv27BHdunUT0dHRsp6CdA3tw3Xr1glXV1excuVKcfLkSfHVV1+JAQMGiIEDB8p6CtKZ8348ceJEMWfOHOPv+/btE66urmLJkiXi2LFjIjExUbi5uYnDhw836G+z8KrHpEmTBIAal927dxvb/PTTT2LkyJGiWbNmom3btmLmzJmivLzcZDm7d+8Wffr0Ee7u7qJLly5i3bp1tn0iQoisrCwREhIivLy8hIeHh+jRo4dYuHChyWBUCCEOHTok7rzzTqHT6US7du3E4sWLbZ61Nm+99Zbo2LGjcHd3FwMHDhRff/217Eg1jBs3TgQEBAh3d3fRrl07MW7cOPHjjz8a77969ap45plnRKtWrYSnp6d46KGHxPnz56Vk3b17d63r9qRJk4QQhlPKv/TSS8LPz0/odDoxfPhwceLECZNlXLhwQTz66KOiRYsWQq/XiylTphg/lJCV/cqVKyI8PFz4+PgINzc30alTJ/Hkk0/WKNJlZK8tMwCT1wNz1hFzXnOIbnSzbX7SpEli6NChxvZDhw6tt70jamgf3oiFV+P68NixYyIsLEw0a9ZMtG/fXsTHx5sMkB1NY/rwzTffFD179hTNmjUTAQEBYsKECeLnn3+2fXiVMOf9eOjQoTVe77Zu3SpuueUW4e7uLm699Vaxffv2Bv9tp/8FICIiIiIiIitxuGO8iIiIiIiIbI2FFxERERERkZWx8CIiIiIiIrIyFl5ERERERERWxsKLiIiIiIjIylh4ERERERERWRkLLyIiIiIiIitj4UVERERERGRlLLyIiIiIiIisjIUXkYUMGjQIb775pvH38ePHw8nJCdeuXQMAnD17Fu7u7vjvf/8rKyIRERERScLCi8hCvL29UVxcDMBQZKWmpqJ58+YoLCwEAKxevRojRozALbfcIjElEREREcnAwovIQq4vvN5++208/vjjaNu2LS5evIiysjK8++67eO655wAAn376KYKDg9GtWzesWbNGZmwiIiIpfv31V/j7+2PhwoXG2/bv3w93d3ekp6dLTEZkHa6yAxDZi+rC6/Lly3jvvffw9ddfY8+ePbh48SK2bduGNm3aYMSIEaioqEB8fDx2794NLy8v9O/fHw899BDatGkj+ykQERHZjI+PD9auXYtRo0YhPDwcwcHBmDhxIuLi4jB8+HDZ8YgsjjNeRBZSXXitX78egwcPRteuXaHX63Hx4kUkJSXh2WefhZOTEw4ePIhbb70V7dq1Q4sWLTBy5EikpqbKjk9ERGRz9913H5588klMmDABMTExaN68ORYtWiQ7FpFVsPAishBvb29cunQJb7zxhnGXQi8vL+zevRvHjh1DVFQUAODcuXNo166d8XHt2rXDL7/8IiUzERGRbEuWLEFFRQVSUlKwYcMG6HQ62ZGIrIKFF5GFeHt7Y9euXdDpdMZdJPR6PVatWoXo6Gh4enpKTkhERKQ+J0+exLlz51BVVYWffvpJdhwiq+ExXkQW4u3tjZKSEuNsF2CY8bp27RpiY2ONtwUGBprMcP3yyy8YOHCgTbMSERGpQVlZGR5//HGMGzcOwcHBiI6OxuHDh+Hr6ys7GpHFOQkhhOwQRI6koqICPXr0QEZGhvHkGvv37+fJNYiIyOHMmjUL27Ztw6FDh9CiRQsMHToUXl5e+PTTT2VHI7I47mpIZGOurq5YunQphg0bhj59+mDmzJksuoiIyOFkZGRgxYoVeP/996HX6+Hs7Iz3338fX375Jd555x3Z8YgsjjNeREREREREVsYZLyIiIiIiIitj4UVERERERGRlLLyIiIiIiIisjIUXERERERGRlbHwIiIiIiIisjIWXkRERERERFbGwouIiIiIiMjKWHgRERERERFZGQsvIiIiIiIiK2PhRUREREREZGUsvIiIiIiIiKyMhRcREREREZGV/T+CkNEqCjO5OgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    \"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "        l=loss_star, w0=w0_star, w1=w1_star, t=execution_time\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient vector\n",
    "    # ***************************************************\n",
    "    e = y - np.matmul(tx,w)\n",
    "    grad = -(1/y.shape[0])*np.dot(tx.T,e)\n",
    "    return grad\n",
    "    #return -(1/y.shape[0])*np.matmul(tx.T,(y - np.matmul(tx,w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        grad = compute_gradient(y,tx,w)\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma*grad\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2792.236712759165, w0=51.305745401473324, w1=9.435798704492441\n",
      "GD iter. 1/49: loss=265.3024621089666, w0=66.69746902191562, w1=12.266538315840048\n",
      "GD iter. 2/49: loss=37.878379550441835, w0=71.31498610804832, w1=13.115760199244335\n",
      "GD iter. 3/49: loss=17.41021212017456, w0=72.70024123388814, w1=13.370526764265632\n",
      "GD iter. 4/49: loss=15.568077051450487, w0=73.11581777164008, w1=13.446956733772023\n",
      "GD iter. 5/49: loss=15.402284895265316, w0=73.24049073296567, w1=13.469885724623941\n",
      "GD iter. 6/49: loss=15.387363601208621, w0=73.27789262136332, w1=13.476764421879516\n",
      "GD iter. 7/49: loss=15.386020684743551, w0=73.28911318788263, w1=13.478828031056189\n",
      "GD iter. 8/49: loss=15.385899822261655, w0=73.29247935783842, w1=13.47944711380919\n",
      "GD iter. 9/49: loss=15.385888944638314, w0=73.29348920882516, w1=13.47963283863509\n",
      "GD iter. 10/49: loss=15.385887965652197, w0=73.29379216412119, w1=13.479688556082861\n",
      "GD iter. 11/49: loss=15.385887877543448, w0=73.29388305071, w1=13.479705271317192\n",
      "GD iter. 12/49: loss=15.3858878696137, w0=73.29391031668663, w1=13.479710285887492\n",
      "GD iter. 13/49: loss=15.38588786890002, w0=73.29391849647962, w1=13.479711790258582\n",
      "GD iter. 14/49: loss=15.385887868835784, w0=73.29392095041752, w1=13.479712241569908\n",
      "GD iter. 15/49: loss=15.385887868829968, w0=73.29392168659889, w1=13.479712376963306\n",
      "GD iter. 16/49: loss=15.385887868829489, w0=73.2939219074533, w1=13.479712417581325\n",
      "GD iter. 17/49: loss=15.385887868829426, w0=73.29392197370963, w1=13.479712429766732\n",
      "GD iter. 18/49: loss=15.385887868829375, w0=73.29392199358652, w1=13.479712433422353\n",
      "GD iter. 19/49: loss=15.385887868829363, w0=73.29392199954958, w1=13.47971243451904\n",
      "GD iter. 20/49: loss=15.385887868829448, w0=73.2939220013385, w1=13.479712434848047\n",
      "GD iter. 21/49: loss=15.385887868829402, w0=73.29392200187519, w1=13.479712434946748\n",
      "GD iter. 22/49: loss=15.385887868829375, w0=73.29392200203618, w1=13.479712434976358\n",
      "GD iter. 23/49: loss=15.385887868829393, w0=73.29392200208449, w1=13.479712434985242\n",
      "GD iter. 24/49: loss=15.385887868829375, w0=73.29392200209898, w1=13.479712434987906\n",
      "GD iter. 25/49: loss=15.385887868829402, w0=73.29392200210333, w1=13.479712434988706\n",
      "GD iter. 26/49: loss=15.385887868829398, w0=73.29392200210462, w1=13.479712434988945\n",
      "GD iter. 27/49: loss=15.385887868829402, w0=73.29392200210502, w1=13.479712434989018\n",
      "GD iter. 28/49: loss=15.38588786882937, w0=73.29392200210513, w1=13.47971243498904\n",
      "GD iter. 29/49: loss=15.385887868829403, w0=73.29392200210518, w1=13.479712434989047\n",
      "GD iter. 30/49: loss=15.38588786882941, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 31/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 32/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 33/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 34/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 35/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 36/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 37/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 38/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 39/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 40/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 41/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 42/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 43/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 44/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 45/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 46/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 47/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 48/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 49/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD: execution time=0.059 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7539803fafb461babfb692920a1f615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    # ***************************************************\n",
    "    return -(1/y.shape[0])*np.matmul(tx.T,(y - np.matmul(tx,w)))\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        # ***************************************************\n",
    "        for minibatch_y,minibatch_tx in batch_iter(y,tx,batch_size):\n",
    "            grad = compute_stoch_gradient(minibatch_y,minibatch_tx,w)\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        w = w - gamma*grad\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2792.236712759165, w0=8.699023511775723, w1=5.336111004894752\n",
      "SGD iter. 1/49: loss=2134.7954654829355, w0=15.749902522167769, w1=11.120993602407683\n",
      "SGD iter. 2/49: loss=1673.8247540881223, w0=20.707572864893713, w1=8.58853921173764\n",
      "SGD iter. 3/49: loss=1410.0097334091065, w0=26.970345940802922, w1=9.169867241393051\n",
      "SGD iter. 4/49: loss=1097.6101202188406, w0=31.780160409398057, w1=15.689480947454284\n",
      "SGD iter. 5/49: loss=879.5236270962347, w0=36.56711211965362, w1=10.58966210095257\n",
      "SGD iter. 6/49: loss=693.991365406332, w0=40.278641027802834, w1=11.580141379614524\n",
      "SGD iter. 7/49: loss=562.1944618721042, w0=42.89399783720994, w1=12.486156153978651\n",
      "SGD iter. 8/49: loss=477.95715952628836, w0=46.12258508898759, w1=9.804132454555699\n",
      "SGD iter. 9/49: loss=391.2816067881834, w0=49.465027345637004, w1=18.772774827154727\n",
      "SGD iter. 10/49: loss=313.3022528870373, w0=51.00507210265236, w1=21.77427136168872\n",
      "SGD iter. 11/49: loss=298.18215668324467, w0=52.81368165348978, w1=21.115921021258156\n",
      "SGD iter. 12/49: loss=254.26185102386177, w0=55.83119697600449, w1=17.37806561568256\n",
      "SGD iter. 13/49: loss=175.45784929814292, w0=58.1805184246801, w1=13.297600846475385\n",
      "SGD iter. 14/49: loss=129.60995403122718, w0=59.9498325039201, w1=11.488531103946245\n",
      "SGD iter. 15/49: loss=106.40065168316266, w0=62.80864018940632, w1=10.922307555391377\n",
      "SGD iter. 16/49: loss=73.62661507378117, w0=63.87735484159505, w1=12.060965606998597\n",
      "SGD iter. 17/49: loss=60.72817769399473, w0=65.155302372178, w1=11.39881036127381\n",
      "SGD iter. 18/49: loss=50.66952932934358, w0=66.02996533766995, w1=12.239571642645727\n",
      "SGD iter. 19/49: loss=42.537395672642916, w0=66.75602273062624, w1=12.0917113629802\n",
      "SGD iter. 20/49: loss=37.72122479878072, w0=67.49544040389156, w1=13.368869165358975\n",
      "SGD iter. 21/49: loss=32.20322540645164, w0=68.77914612704545, w1=11.881489516455623\n",
      "SGD iter. 22/49: loss=26.85464671850292, w0=68.66781755611039, w1=11.91440619385645\n",
      "SGD iter. 23/49: loss=27.31140085572023, w0=68.7938181298821, w1=12.105328397124964\n",
      "SGD iter. 24/49: loss=26.455821040995875, w0=69.57114929894931, w1=12.439961760028678\n",
      "SGD iter. 25/49: loss=22.85594690155088, w0=69.73105133208837, w1=12.36163177475941\n",
      "SGD iter. 26/49: loss=22.357963755852246, w0=71.11421767708615, w1=13.7665983158446\n",
      "SGD iter. 27/49: loss=17.802595095399894, w0=71.39928006934183, w1=13.343679967588402\n",
      "SGD iter. 28/49: loss=17.189974311615593, w0=71.4034489208806, w1=13.346301135069206\n",
      "SGD iter. 29/49: loss=17.181731391719897, w0=70.89021515010113, w1=14.227460528331942\n",
      "SGD iter. 30/49: loss=18.554354789564016, w0=71.19354902097182, w1=13.831950997039392\n",
      "SGD iter. 31/49: loss=17.653707201064616, w0=71.32128839439825, w1=13.92498058783298\n",
      "SGD iter. 32/49: loss=17.43066140792535, w0=71.52645447173342, w1=13.614856315543413\n",
      "SGD iter. 33/49: loss=16.956990538514333, w0=72.36478227553765, w1=13.308973477342082\n",
      "SGD iter. 34/49: loss=15.832114080401592, w0=72.94901534061005, w1=12.82538406183568\n",
      "SGD iter. 35/49: loss=15.659440981358046, w0=72.96867539592085, w1=12.80535733849757\n",
      "SGD iter. 36/49: loss=15.666157944328617, w0=72.42386072714467, w1=13.173621701825196\n",
      "SGD iter. 37/49: loss=15.811236948386746, w0=72.12449343872366, w1=13.27415317455247\n",
      "SGD iter. 38/49: loss=16.090796756031313, w0=72.78747630546992, w1=13.767361001330832\n",
      "SGD iter. 39/49: loss=15.555502339508818, w0=73.25948784612775, w1=13.085505414562675\n",
      "SGD iter. 40/49: loss=15.464180311855012, w0=72.7675663213683, w1=13.95372435724507\n",
      "SGD iter. 41/49: loss=15.636756671371836, w0=72.50605100727134, w1=13.755611469192296\n",
      "SGD iter. 42/49: loss=15.734318359616784, w0=72.31826711020561, w1=13.59215104276764\n",
      "SGD iter. 43/49: loss=15.868160323132761, w0=72.45806642575971, w1=13.715769970337991\n",
      "SGD iter. 44/49: loss=15.763076721080827, w0=72.69578267354754, w1=13.966711530562378\n",
      "SGD iter. 45/49: loss=15.6833572565577, w0=73.22445089383815, w1=13.490739966397888\n",
      "SGD iter. 46/49: loss=15.388361789495834, w0=72.50736676420692, w1=11.925130378161983\n",
      "SGD iter. 47/49: loss=16.903585125666357, w0=73.14163741360366, w1=13.13081515592401\n",
      "SGD iter. 48/49: loss=15.458347822446465, w0=73.72506359948873, w1=13.69639921028562\n",
      "SGD iter. 49/49: loss=15.502305986620815, w0=72.81427492534252, w1=14.3064657383906\n",
      "SGD: execution time=0.059 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7607e70b6264500b97730329d792a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "# ***************************************************\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=False)\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200,), (200, 2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2829.272224438416, w0=51.54259072181176, w1=10.132993413506084\n",
      "GD iter. 1/49: loss=267.0500258779429, w0=67.0053679383553, w1=13.172891437557826\n",
      "GD iter. 2/49: loss=36.45002800750046, w0=71.64420110331838, w1=14.084860844773324\n",
      "GD iter. 3/49: loss=15.69602819916064, w0=73.03585105280729, w1=14.358451666937965\n",
      "GD iter. 4/49: loss=13.82816821641008, w0=73.45334603765397, w1=14.440528913587356\n",
      "GD iter. 5/49: loss=13.660060817962526, w0=73.57859453310797, w1=14.46515208758217\n",
      "GD iter. 6/49: loss=13.64493115210224, w0=73.61616908174418, w1=14.472539039780616\n",
      "GD iter. 7/49: loss=13.643569482174815, w0=73.62744144633503, w1=14.474755125440149\n",
      "GD iter. 8/49: loss=13.64344693188135, w0=73.63082315571229, w1=14.47541995113801\n",
      "GD iter. 9/49: loss=13.643435902354938, w0=73.63183766852546, w1=14.475619398847368\n",
      "GD iter. 10/49: loss=13.643434909697556, w0=73.63214202236942, w1=14.475679233160175\n",
      "GD iter. 11/49: loss=13.643434820358403, w0=73.6322333285226, w1=14.475697183454017\n",
      "GD iter. 12/49: loss=13.643434812317876, w0=73.63226072036856, w1=14.47570256854217\n",
      "GD iter. 13/49: loss=13.643434811594227, w0=73.63226893792235, w1=14.475704184068615\n",
      "GD iter. 14/49: loss=13.643434811529096, w0=73.63227140318848, w1=14.475704668726548\n",
      "GD iter. 15/49: loss=13.643434811523235, w0=73.63227214276833, w1=14.47570481412393\n",
      "GD iter. 16/49: loss=13.64343481152271, w0=73.63227236464228, w1=14.475704857743143\n",
      "GD iter. 17/49: loss=13.643434811522654, w0=73.63227243120447, w1=14.475704870828908\n",
      "GD iter. 18/49: loss=13.643434811522654, w0=73.63227245117312, w1=14.475704874754637\n",
      "GD iter. 19/49: loss=13.643434811522654, w0=73.63227245716372, w1=14.475704875932355\n",
      "GD iter. 20/49: loss=13.643434811522656, w0=73.6322724589609, w1=14.47570487628567\n",
      "GD iter. 21/49: loss=13.643434811522653, w0=73.63227245950004, w1=14.475704876391665\n",
      "GD iter. 22/49: loss=13.643434811522656, w0=73.63227245966179, w1=14.475704876423464\n",
      "GD iter. 23/49: loss=13.643434811522662, w0=73.63227245971032, w1=14.475704876433003\n",
      "GD iter. 24/49: loss=13.643434811522662, w0=73.63227245972487, w1=14.475704876435866\n",
      "GD iter. 25/49: loss=13.643434811522656, w0=73.63227245972924, w1=14.475704876436724\n",
      "GD iter. 26/49: loss=13.643434811522653, w0=73.63227245973054, w1=14.475704876436982\n",
      "GD iter. 27/49: loss=13.643434811522656, w0=73.63227245973094, w1=14.475704876437058\n",
      "GD iter. 28/49: loss=13.643434811522653, w0=73.63227245973106, w1=14.475704876437081\n",
      "GD iter. 29/49: loss=13.643434811522651, w0=73.6322724597311, w1=14.475704876437089\n",
      "GD iter. 30/49: loss=13.643434811522653, w0=73.63227245973111, w1=14.47570487643709\n",
      "GD iter. 31/49: loss=13.64343481152266, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 32/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 33/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 34/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 35/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 36/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 37/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 38/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 39/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 40/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 41/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 42/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 43/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 44/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 45/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 46/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 47/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 48/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 49/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD: execution time=0.002 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points\n",
    "#       and the model fit\n",
    "# ***************************************************\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad0f9b54e734b8181bafb94296d0a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "# ***************************************************\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.8351145358524, w0=51.84746409844842, w1=7.7244264061924195\n",
      "GD iter. 1/49: loss=318.28212470159644, w0=67.40170332798297, w1=10.041754328050114\n",
      "GD iter. 2/49: loss=88.64235561651283, w0=72.06797509684336, w1=10.736952704607411\n",
      "GD iter. 3/49: loss=67.97477639885523, w0=73.46785662750146, w1=10.945512217574597\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631798\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.05160722578589, w1=11.032481534481914\n",
      "GD iter. 7/49: loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536943\n",
      "GD iter. 8/49: loss=65.93074217249234, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073118889339, w0=74.06736849193958, w1=11.034829706038408\n",
      "GD iter. 10/49: loss=65.93073020036948, w0=74.06767464603033, w1=11.034875318003895\n",
      "GD iter. 11/49: loss=65.93073011140233, w0=74.06776649225755, w1=11.034889001593541\n",
      "GD iter. 12/49: loss=65.93073010339529, w0=74.06779404612573, w1=11.034893106670433\n",
      "GD iter. 13/49: loss=65.93073010267464, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260395, w0=74.06780553608874, w1=11.034894818487496\n",
      "GD iter. 16/49: loss=65.93073010260343, w0=74.06780575927507, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260339, w0=74.06780582623098, w1=11.034894861713955\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706557\n",
      "GD iter. 19/49: loss=65.93073010260338, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=65.93073010260339, w0=74.06780585415159, w1=11.034894865873675\n",
      "GD iter. 21/49: loss=65.93073010260338, w0=74.06780585469393, w1=11.034894865954474\n",
      "GD iter. 22/49: loss=65.93073010260338, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260338, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.06780585492008, w1=11.034894865988164\n",
      "GD iter. 25/49: loss=65.93073010260338, w0=74.06780585492449, w1=11.03489486598882\n",
      "GD iter. 26/49: loss=65.93073010260339, w0=74.06780585492581, w1=11.034894865989015\n",
      "GD iter. 27/49: loss=65.93073010260338, w0=74.06780585492619, w1=11.034894865989076\n",
      "GD iter. 28/49: loss=65.93073010260338, w0=74.0678058549263, w1=11.034894865989099\n",
      "GD iter. 29/49: loss=65.93073010260339, w0=74.06780585492635, w1=11.0348948659891\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 31/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.001 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points\n",
    "#       and the model fit\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "# ***************************************************\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3198a4c0673e4f3a8f7ff246a6ef84b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute subgradient gradient vector for MAE\n",
    "    # ***************************************************\n",
    "    r = np.random.randint(-1,2) #Return a random no. from [-1,1]\n",
    "    del_h = y - np.matmul(tx,w)\n",
    "    del_h[del_h < 0] = -1\n",
    "    del_h[del_h == 0] = r\n",
    "    del_h[del_h > 0] = 1\n",
    "    return -(1/y.shape[0])*np.matmul(tx.T,del_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute subgradient and loss\n",
    "        # ***************************************************\n",
    "        subgrad = compute_subgradient_mae(y, tx, w)\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma*subgrad\n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=6.109524327590712e-16\n",
      "SubGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=1.2219048655181425e-15\n",
      "SubGD iter. 2/499: loss=72.66780585492637, w0=2.0999999999999996, w1=1.832857298277214e-15\n",
      "SubGD iter. 3/499: loss=71.96780585492638, w0=2.8, w1=2.443809731036285e-15\n",
      "SubGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=3.054762163795356e-15\n",
      "SubGD iter. 5/499: loss=70.56780585492639, w0=4.2, w1=3.665714596554428e-15\n",
      "SubGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=4.276667029313499e-15\n",
      "SubGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=4.887619462072571e-15\n",
      "SubGD iter. 8/499: loss=68.46780585492637, w0=6.300000000000001, w1=5.498571894831642e-15\n",
      "SubGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=6.109524327590714e-15\n",
      "SubGD iter. 10/499: loss=67.06780585492639, w0=7.700000000000001, w1=6.720476760349785e-15\n",
      "SubGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=7.331429193108857e-15\n",
      "SubGD iter. 12/499: loss=65.66780585492639, w0=9.1, w1=7.942381625867928e-15\n",
      "SubGD iter. 13/499: loss=64.96780585492638, w0=9.799999999999999, w1=8.553334058627e-15\n",
      "SubGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=9.164286491386072e-15\n",
      "SubGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=9.775238924145143e-15\n",
      "SubGD iter. 16/499: loss=62.867805854926374, w0=11.899999999999997, w1=1.0386191356904215e-14\n",
      "SubGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=1.0997143789663286e-14\n",
      "SubGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=1.1608096222422358e-14\n",
      "SubGD iter. 19/499: loss=60.76780585492638, w0=13.999999999999995, w1=1.2219048655181429e-14\n",
      "SubGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=1.28300010879405e-14\n",
      "SubGD iter. 21/499: loss=59.36780585492639, w0=15.399999999999993, w1=1.3440953520699572e-14\n",
      "SubGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=1.4051905953458644e-14\n",
      "SubGD iter. 23/499: loss=57.96780585492638, w0=16.799999999999994, w1=1.4662858386217714e-14\n",
      "SubGD iter. 24/499: loss=57.26780585492638, w0=17.499999999999993, w1=1.5273810818976784e-14\n",
      "SubGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=1.5884763251735854e-14\n",
      "SubGD iter. 26/499: loss=55.867805854926395, w0=18.89999999999999, w1=1.6495715684494924e-14\n",
      "SubGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=1.7106668117253994e-14\n",
      "SubGD iter. 28/499: loss=54.46780585492638, w0=20.29999999999999, w1=1.7717620550013064e-14\n",
      "SubGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=1.8328572982772134e-14\n",
      "SubGD iter. 30/499: loss=53.06780585492639, w0=21.69999999999999, w1=1.8939525415531204e-14\n",
      "SubGD iter. 31/499: loss=52.367805854926395, w0=22.399999999999988, w1=1.9550477848290273e-14\n",
      "SubGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=2.0161430281049343e-14\n",
      "SubGD iter. 33/499: loss=50.96780585492639, w0=23.799999999999986, w1=2.0772382713808413e-14\n",
      "SubGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=2.1383335146567483e-14\n",
      "SubGD iter. 35/499: loss=49.56780585492639, w0=25.199999999999985, w1=2.1994287579326553e-14\n",
      "SubGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=2.2605240012085623e-14\n",
      "SubGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=2.3216192444844693e-14\n",
      "SubGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=2.3827144877603763e-14\n",
      "SubGD iter. 39/499: loss=46.7678058549264, w0=27.999999999999982, w1=2.4438097310362833e-14\n",
      "SubGD iter. 40/499: loss=46.067805854926405, w0=28.69999999999998, w1=2.5049049743121903e-14\n",
      "SubGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=2.5660002175880973e-14\n",
      "SubGD iter. 42/499: loss=44.6678058549264, w0=30.09999999999998, w1=2.6270954608640043e-14\n",
      "SubGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=2.6881907041399113e-14\n",
      "SubGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=2.7492859474158183e-14\n",
      "SubGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=2.8103811906917253e-14\n",
      "SubGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=2.871476433967632e-14\n",
      "SubGD iter. 47/499: loss=41.167805854926385, w0=33.59999999999999, w1=2.9325716772435396e-14\n",
      "SubGD iter. 48/499: loss=40.46780585492639, w0=34.29999999999999, w1=2.993666920519447e-14\n",
      "SubGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=3.054762163795354e-14\n",
      "SubGD iter. 50/499: loss=39.067805854926384, w0=35.699999999999996, w1=3.1158574070712615e-14\n",
      "SubGD iter. 51/499: loss=38.36780585492638, w0=36.4, w1=3.176952650347169e-14\n",
      "SubGD iter. 52/499: loss=37.66780585492638, w0=37.1, w1=3.238047893623076e-14\n",
      "SubGD iter. 53/499: loss=36.96780585492638, w0=37.800000000000004, w1=3.2991431368989835e-14\n",
      "SubGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=3.360238380174891e-14\n",
      "SubGD iter. 55/499: loss=35.56780585492637, w0=39.20000000000001, w1=3.421333623450798e-14\n",
      "SubGD iter. 56/499: loss=34.86780585492637, w0=39.90000000000001, w1=3.4824288667267054e-14\n",
      "SubGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=3.543524110002613e-14\n",
      "SubGD iter. 58/499: loss=33.46780585492636, w0=41.30000000000002, w1=3.60461935327852e-14\n",
      "SubGD iter. 59/499: loss=32.767805854926365, w0=42.00000000000002, w1=3.6657145965544273e-14\n",
      "SubGD iter. 60/499: loss=32.067805854926355, w0=42.700000000000024, w1=3.7268098398303347e-14\n",
      "SubGD iter. 61/499: loss=31.36780585492636, w0=43.40000000000003, w1=3.787905083106242e-14\n",
      "SubGD iter. 62/499: loss=30.667805854926353, w0=44.10000000000003, w1=3.849000326382149e-14\n",
      "SubGD iter. 63/499: loss=29.96780585492635, w0=44.80000000000003, w1=3.9100955696580566e-14\n",
      "SubGD iter. 64/499: loss=29.267805854926348, w0=45.500000000000036, w1=3.971190812933964e-14\n",
      "SubGD iter. 65/499: loss=28.567805854926345, w0=46.20000000000004, w1=4.032286056209871e-14\n",
      "SubGD iter. 66/499: loss=27.867805854926342, w0=46.90000000000004, w1=4.0933812994857785e-14\n",
      "SubGD iter. 67/499: loss=27.17327020966892, w0=47.59306930693074, w1=0.011147845678271063\n",
      "SubGD iter. 68/499: loss=26.490451563751197, w0=48.279207920792125, w1=0.03308574108989941\n",
      "SubGD iter. 69/499: loss=25.817212322770175, w0=48.96534653465351, w1=0.05502363650152776\n",
      "SubGD iter. 70/499: loss=25.15503943465645, w0=49.63069306930698, w1=0.10538326388307814\n",
      "SubGD iter. 71/499: loss=24.524103413894778, w0=50.28910891089114, w1=0.16746568532793435\n",
      "SubGD iter. 72/499: loss=23.899295346035593, w0=50.947524752475296, w1=0.22954810677279056\n",
      "SubGD iter. 73/499: loss=23.284392925657148, w0=51.59207920792084, w1=0.31242512932747524\n",
      "SubGD iter. 74/499: loss=22.686876444181845, w0=52.22277227722777, w1=0.4119501328839991\n",
      "SubGD iter. 75/499: loss=22.10626756964055, w0=52.84653465346539, w1=0.5208167847923756\n",
      "SubGD iter. 76/499: loss=21.537818828008433, w0=53.4564356435644, w1=0.6457900912635992\n",
      "SubGD iter. 77/499: loss=20.986339874628463, w0=54.0594059405941, w1=0.7796904498577214\n",
      "SubGD iter. 78/499: loss=20.445560936620446, w0=54.655445544554496, w1=0.9197570104995693\n",
      "SubGD iter. 79/499: loss=19.91191015895785, w0=55.24455445544559, w1=1.0670920297849913\n",
      "SubGD iter. 80/499: loss=19.389644090563234, w0=55.819801980198065, w1=1.2261255948210765\n",
      "SubGD iter. 81/499: loss=18.887989064395885, w0=56.36732673267331, w1=1.410709342622213\n",
      "SubGD iter. 82/499: loss=18.415960501854236, w0=56.900990099009945, w1=1.605853732220269\n",
      "SubGD iter. 83/499: loss=17.954898543040386, w0=57.42772277227727, w1=1.808762802293962\n",
      "SubGD iter. 84/499: loss=17.505757656579824, w0=57.933663366336674, w1=2.0285064197514697\n",
      "SubGD iter. 85/499: loss=17.07495742693161, w0=58.43267326732677, w1=2.2494370848672776\n",
      "SubGD iter. 86/499: loss=16.652967297509903, w0=58.91089108910895, w1=2.4837982986028337\n",
      "SubGD iter. 87/499: loss=16.24854073149673, w0=59.382178217821824, w1=2.7260245553531504\n",
      "SubGD iter. 88/499: loss=15.849105212654159, w0=59.83960396039608, w1=2.978742333469136\n",
      "SubGD iter. 89/499: loss=15.46691979123133, w0=60.262376237623805, w1=3.251528669355438\n",
      "SubGD iter. 90/499: loss=15.108294621512217, w0=60.67821782178222, w1=3.5270865794242794\n",
      "SubGD iter. 91/499: loss=14.754896345922832, w0=61.087128712871326, w1=3.806459183951815\n",
      "SubGD iter. 92/499: loss=14.40452896162028, w0=61.49603960396043, w1=4.085831788479351\n",
      "SubGD iter. 93/499: loss=14.055787028127279, w0=61.891089108910926, w1=4.373839384328607\n",
      "SubGD iter. 94/499: loss=13.714620911605637, w0=62.27920792079211, w1=4.666037469532047\n",
      "SubGD iter. 95/499: loss=13.381236307284155, w0=62.65346534653469, w1=4.959829093241769\n",
      "SubGD iter. 96/499: loss=13.058821615166238, w0=63.02079207920796, w1=5.25705719205664\n",
      "SubGD iter. 97/499: loss=12.74025172433924, w0=63.38118811881192, w1=5.560434316352406\n",
      "SubGD iter. 98/499: loss=12.423218888756113, w0=63.74158415841588, w1=5.863811440648173\n",
      "SubGD iter. 99/499: loss=12.107561731901173, w0=64.08811881188123, w1=6.172402175278548\n",
      "SubGD iter. 100/499: loss=11.800622097398135, w0=64.42772277227726, w1=6.486369310516498\n",
      "SubGD iter. 101/499: loss=11.49504179464643, w0=64.7673267326733, w1=6.800336445754448\n",
      "SubGD iter. 102/499: loss=11.189461491894715, w0=65.10693069306933, w1=7.114303580992399\n",
      "SubGD iter. 103/499: loss=10.883881189143004, w0=65.44653465346536, w1=7.428270716230349\n",
      "SubGD iter. 104/499: loss=10.584593408313204, w0=65.76534653465349, w1=7.747893210218626\n",
      "SubGD iter. 105/499: loss=10.295816534318941, w0=66.070297029703, w1=8.073669686866905\n",
      "SubGD iter. 106/499: loss=10.01135208122136, w0=66.37524752475251, w1=8.399446163515185\n",
      "SubGD iter. 107/499: loss=9.728084326668132, w0=66.6663366336634, w1=8.73297028041739\n",
      "SubGD iter. 108/499: loss=9.44812546112251, w0=66.9574257425743, w1=9.066494397319596\n",
      "SubGD iter. 109/499: loss=9.17104110409667, w0=67.23465346534658, w1=9.39863031947029\n",
      "SubGD iter. 110/499: loss=8.903656131158963, w0=67.51188118811886, w1=9.730766241620982\n",
      "SubGD iter. 111/499: loss=8.636271158221255, w0=67.78910891089114, w1=10.062902163771675\n",
      "SubGD iter. 112/499: loss=8.376151920302375, w0=68.06633663366343, w1=10.363999289979422\n",
      "SubGD iter. 113/499: loss=8.140540838751498, w0=68.32970297029709, w1=10.660466909273612\n",
      "SubGD iter. 114/499: loss=7.918544501597273, w0=68.59306930693076, w1=10.943174379960814\n",
      "SubGD iter. 115/499: loss=7.7052797283770005, w0=68.85643564356442, w1=11.225881850648015\n",
      "SubGD iter. 116/499: loss=7.493695831178641, w0=69.11287128712878, w1=11.504395843582206\n",
      "SubGD iter. 117/499: loss=7.289992405743416, w0=69.35544554455453, w1=11.78820189306775\n",
      "SubGD iter. 118/499: loss=7.097234035781543, w0=69.58415841584166, w1=12.060911465190971\n",
      "SubGD iter. 119/499: loss=6.919905294668923, w0=69.80594059405948, w1=12.324245668386048\n",
      "SubGD iter. 120/499: loss=6.750573527315454, w0=70.0277227722773, w1=12.587579871581125\n",
      "SubGD iter. 121/499: loss=6.584744810805664, w0=70.25643564356443, w1=12.824765405096484\n",
      "SubGD iter. 122/499: loss=6.430343276347806, w0=70.47821782178225, w1=13.065616959310148\n",
      "SubGD iter. 123/499: loss=6.278071481890353, w0=70.69306930693077, w1=13.302953389983912\n",
      "SubGD iter. 124/499: loss=6.133663329263324, w0=70.89405940594067, w1=13.525403099312918\n",
      "SubGD iter. 125/499: loss=6.00584079834303, w0=71.08811881188126, w1=13.742945617944212\n",
      "SubGD iter. 126/499: loss=5.885021825223219, w0=71.27524752475254, w1=13.953548196006844\n",
      "SubGD iter. 127/499: loss=5.771635252269659, w0=71.46237623762383, w1=14.164150774069476\n",
      "SubGD iter. 128/499: loss=5.667162061790258, w0=71.62178217821788, w1=14.349779559473173\n",
      "SubGD iter. 129/499: loss=5.586726765993147, w0=71.75346534653471, w1=14.51689010761231\n",
      "SubGD iter. 130/499: loss=5.523847812160388, w0=71.87128712871292, w1=14.670791185324186\n",
      "SubGD iter. 131/499: loss=5.480093708591872, w0=71.95445544554461, w1=14.780276456654521\n",
      "SubGD iter. 132/499: loss=5.4530880035020255, w0=72.0376237623763, w1=14.889761727984856\n",
      "SubGD iter. 133/499: loss=5.427392630862905, w0=72.10693069306937, w1=14.985916181776727\n",
      "SubGD iter. 134/499: loss=5.407322445682752, w0=72.17623762376245, w1=15.082070635568597\n",
      "SubGD iter. 135/499: loss=5.387252260502599, w0=72.24554455445552, w1=15.178225089360467\n",
      "SubGD iter. 136/499: loss=5.3704607803386955, w0=72.30099009900998, w1=15.25972348971591\n",
      "SubGD iter. 137/499: loss=5.357406523334741, w0=72.34950495049513, w1=15.335091856448138\n",
      "SubGD iter. 138/499: loss=5.345929264022583, w0=72.39801980198028, w1=15.410460223180365\n",
      "SubGD iter. 139/499: loss=5.335714659517474, w0=72.43267326732682, w1=15.469961786755725\n",
      "SubGD iter. 140/499: loss=5.330043910465361, w0=72.46039603960405, w1=15.51864528583281\n",
      "SubGD iter. 141/499: loss=5.325676428273225, w0=72.48811881188128, w1=15.561592159086487\n",
      "SubGD iter. 142/499: loss=5.322176726526591, w0=72.5019801980199, w1=15.597828332032526\n",
      "SubGD iter. 143/499: loss=5.320111309643112, w0=72.52277227722782, w1=15.624722856626713\n",
      "SubGD iter. 144/499: loss=5.318478284898438, w0=72.55049504950505, w1=15.642690329098\n",
      "SubGD iter. 145/499: loss=5.3172400485651465, w0=72.56435643564366, w1=15.664356578291091\n",
      "SubGD iter. 146/499: loss=5.316406547951546, w0=72.58514851485158, w1=15.677095775361284\n",
      "SubGD iter. 147/499: loss=5.315557122666144, w0=72.6059405940595, w1=15.689834972431477\n",
      "SubGD iter. 148/499: loss=5.314707697380741, w0=72.62673267326743, w1=15.70257416950167\n",
      "SubGD iter. 149/499: loss=5.313876880922167, w0=72.64059405940604, w1=15.72424041869476\n",
      "SubGD iter. 150/499: loss=5.313052246871384, w0=72.66138613861396, w1=15.736979615764954\n",
      "SubGD iter. 151/499: loss=5.312377839024388, w0=72.66831683168327, w1=15.74811029423128\n",
      "SubGD iter. 152/499: loss=5.312132229725043, w0=72.67524752475258, w1=15.759240972697606\n",
      "SubGD iter. 153/499: loss=5.311886620425697, w0=72.68217821782189, w1=15.770371651163932\n",
      "SubGD iter. 154/499: loss=5.311683566098434, w0=72.68217821782189, w1=15.774323911906686\n",
      "SubGD iter. 155/499: loss=5.311661251291322, w0=72.68217821782189, w1=15.77827617264944\n",
      "SubGD iter. 156/499: loss=5.31163893648421, w0=72.68217821782189, w1=15.782228433392193\n",
      "SubGD iter. 157/499: loss=5.311616621677096, w0=72.68217821782189, w1=15.786180694134947\n",
      "SubGD iter. 158/499: loss=5.311594306869984, w0=72.68217821782189, w1=15.7901329548777\n",
      "SubGD iter. 159/499: loss=5.311571992062871, w0=72.68217821782189, w1=15.794085215620454\n",
      "SubGD iter. 160/499: loss=5.31154967725576, w0=72.68217821782189, w1=15.798037476363207\n",
      "SubGD iter. 161/499: loss=5.311527362448647, w0=72.68217821782189, w1=15.801989737105961\n",
      "SubGD iter. 162/499: loss=5.3115050476415355, w0=72.68217821782189, w1=15.805941997848715\n",
      "SubGD iter. 163/499: loss=5.311482732834422, w0=72.68217821782189, w1=15.809894258591468\n",
      "SubGD iter. 164/499: loss=5.311460418027309, w0=72.68217821782189, w1=15.813846519334222\n",
      "SubGD iter. 165/499: loss=5.311438103220198, w0=72.68217821782189, w1=15.817798780076975\n",
      "SubGD iter. 166/499: loss=5.311415788413085, w0=72.68217821782189, w1=15.821751040819729\n",
      "SubGD iter. 167/499: loss=5.311393473605974, w0=72.68217821782189, w1=15.825703301562482\n",
      "SubGD iter. 168/499: loss=5.31137115879886, w0=72.68217821782189, w1=15.829655562305236\n",
      "SubGD iter. 169/499: loss=5.3113488439917464, w0=72.68217821782189, w1=15.83360782304799\n",
      "SubGD iter. 170/499: loss=5.311326529184636, w0=72.68217821782189, w1=15.837560083790743\n",
      "SubGD iter. 171/499: loss=5.311304214377523, w0=72.68217821782189, w1=15.841512344533497\n",
      "SubGD iter. 172/499: loss=5.311281899570409, w0=72.68217821782189, w1=15.84546460527625\n",
      "SubGD iter. 173/499: loss=5.311259584763298, w0=72.68217821782189, w1=15.849416866019004\n",
      "SubGD iter. 174/499: loss=5.311237269956186, w0=72.68217821782189, w1=15.853369126761757\n",
      "SubGD iter. 175/499: loss=5.311214955149073, w0=72.68217821782189, w1=15.857321387504511\n",
      "SubGD iter. 176/499: loss=5.31119264034196, w0=72.68217821782189, w1=15.861273648247264\n",
      "SubGD iter. 177/499: loss=5.311170325534848, w0=72.68217821782189, w1=15.865225908990018\n",
      "SubGD iter. 178/499: loss=5.311148010727736, w0=72.68217821782189, w1=15.869178169732772\n",
      "SubGD iter. 179/499: loss=5.311125695920623, w0=72.68217821782189, w1=15.873130430475525\n",
      "SubGD iter. 180/499: loss=5.31110338111351, w0=72.68217821782189, w1=15.877082691218279\n",
      "SubGD iter. 181/499: loss=5.311081066306398, w0=72.68217821782189, w1=15.881034951961032\n",
      "SubGD iter. 182/499: loss=5.311058751499285, w0=72.68217821782189, w1=15.884987212703786\n",
      "SubGD iter. 183/499: loss=5.3110364366921745, w0=72.68217821782189, w1=15.88893947344654\n",
      "SubGD iter. 184/499: loss=5.311014121885061, w0=72.68217821782189, w1=15.892891734189293\n",
      "SubGD iter. 185/499: loss=5.310991807077948, w0=72.68217821782189, w1=15.896843994932047\n",
      "SubGD iter. 186/499: loss=5.310969492270837, w0=72.68217821782189, w1=15.9007962556748\n",
      "SubGD iter. 187/499: loss=5.310947177463723, w0=72.68217821782189, w1=15.904748516417554\n",
      "SubGD iter. 188/499: loss=5.31092486265661, w0=72.68217821782189, w1=15.908700777160307\n",
      "SubGD iter. 189/499: loss=5.310902547849499, w0=72.68217821782189, w1=15.91265303790306\n",
      "SubGD iter. 190/499: loss=5.310913706061381, w0=72.67524752475258, w1=15.910526938117323\n",
      "SubGD iter. 191/499: loss=5.31089223718627, w0=72.67524752475258, w1=15.914479198860077\n",
      "SubGD iter. 192/499: loss=5.3108699223791564, w0=72.67524752475258, w1=15.91843145960283\n",
      "SubGD iter. 193/499: loss=5.310862636053835, w0=72.66831683168327, w1=15.916305359817093\n",
      "SubGD iter. 194/499: loss=5.310859611715927, w0=72.66831683168327, w1=15.920257620559847\n",
      "SubGD iter. 195/499: loss=5.310837296908816, w0=72.66831683168327, w1=15.9242098813026\n",
      "SubGD iter. 196/499: loss=5.310814982101703, w0=72.66831683168327, w1=15.928162142045354\n",
      "SubGD iter. 197/499: loss=5.310823570190169, w0=72.66138613861396, w1=15.926036042259616\n",
      "SubGD iter. 198/499: loss=5.310804671438475, w0=72.66138613861396, w1=15.92998830300237\n",
      "SubGD iter. 199/499: loss=5.3107823566313614, w0=72.66138613861396, w1=15.933940563745123\n",
      "SubGD iter. 200/499: loss=5.310772500182623, w0=72.65445544554466, w1=15.931814463959386\n",
      "SubGD iter. 201/499: loss=5.3107720459681325, w0=72.65445544554466, w1=15.93576672470214\n",
      "SubGD iter. 202/499: loss=5.310749731161019, w0=72.65445544554466, w1=15.939718985444893\n",
      "SubGD iter. 203/499: loss=5.310727416353908, w0=72.65445544554466, w1=15.943671246187646\n",
      "SubGD iter. 204/499: loss=5.310733434318959, w0=72.64752475247535, w1=15.941545146401909\n",
      "SubGD iter. 205/499: loss=5.310717105690678, w0=72.64752475247535, w1=15.945497407144662\n",
      "SubGD iter. 206/499: loss=5.3106947908835656, w0=72.64752475247535, w1=15.949449667887416\n",
      "SubGD iter. 207/499: loss=5.310682364311412, w0=72.64059405940604, w1=15.947323568101679\n",
      "SubGD iter. 208/499: loss=5.3106844802203375, w0=72.64059405940604, w1=15.951275828844432\n",
      "SubGD iter. 209/499: loss=5.310662165413224, w0=72.64059405940604, w1=15.955228089587186\n",
      "SubGD iter. 210/499: loss=5.310639850606112, w0=72.64059405940604, w1=15.95918035032994\n",
      "SubGD iter. 211/499: loss=5.310643298447746, w0=72.63366336633673, w1=15.957054250544202\n",
      "SubGD iter. 212/499: loss=5.310629539942882, w0=72.63366336633673, w1=15.961006511286955\n",
      "SubGD iter. 213/499: loss=5.3106072251357705, w0=72.63366336633673, w1=15.964958772029709\n",
      "SubGD iter. 214/499: loss=5.310592228440201, w0=72.62673267326743, w1=15.962832672243971\n",
      "SubGD iter. 215/499: loss=5.310633183099028, w0=72.63366336633673, w1=15.967301372051375\n",
      "SubGD iter. 216/499: loss=5.310599343585063, w0=72.62673267326743, w1=15.965175272265638\n",
      "SubGD iter. 217/499: loss=5.31061822827579, w0=72.63366336633673, w1=15.969643972073042\n",
      "SubGD iter. 218/499: loss=5.310606458729926, w0=72.62673267326743, w1=15.967517872287305\n",
      "SubGD iter. 219/499: loss=5.3106032734525535, w0=72.63366336633673, w1=15.971986572094709\n",
      "SubGD iter. 220/499: loss=5.31061357387479, w0=72.62673267326743, w1=15.969860472308971\n",
      "SubGD iter. 221/499: loss=5.310588318629316, w0=72.63366336633673, w1=15.974329172116375\n",
      "SubGD iter. 222/499: loss=5.310620689019651, w0=72.62673267326743, w1=15.972203072330638\n",
      "SubGD iter. 223/499: loss=5.310574966149887, w0=72.62673267326743, w1=15.970593411609551\n",
      "SubGD iter. 224/499: loss=5.310583639649728, w0=72.63366336633673, w1=15.975062111416955\n",
      "SubGD iter. 225/499: loss=5.310622915165495, w0=72.62673267326743, w1=15.972936011631218\n",
      "SubGD iter. 226/499: loss=5.310576651555033, w0=72.62673267326743, w1=15.97132635091013\n",
      "SubGD iter. 227/499: loss=5.31057896067014, w0=72.63366336633673, w1=15.975795050717535\n",
      "SubGD iter. 228/499: loss=5.310625141311338, w0=72.62673267326743, w1=15.973668950931797\n",
      "SubGD iter. 229/499: loss=5.310578336960181, w0=72.62673267326743, w1=15.97205929021071\n",
      "SubGD iter. 230/499: loss=5.310574635520699, w0=72.62673267326743, w1=15.970449629489623\n",
      "SubGD iter. 231/499: loss=5.310584557534204, w0=72.63366336633673, w1=15.974918329297028\n",
      "SubGD iter. 232/499: loss=5.31062247845816, w0=72.62673267326743, w1=15.97279222951129\n",
      "SubGD iter. 233/499: loss=5.310576320925847, w0=72.62673267326743, w1=15.971182568790203\n",
      "SubGD iter. 234/499: loss=5.3105798785546146, w0=72.63366336633673, w1=15.975651268597607\n",
      "SubGD iter. 235/499: loss=5.310624704604003, w0=72.62673267326743, w1=15.97352516881187\n",
      "SubGD iter. 236/499: loss=5.310578006330994, w0=72.62673267326743, w1=15.971915508090783\n",
      "SubGD iter. 237/499: loss=5.310575199575027, w0=72.63366336633673, w1=15.976384207898187\n",
      "SubGD iter. 238/499: loss=5.310626930749845, w0=72.62673267326743, w1=15.97425810811245\n",
      "SubGD iter. 239/499: loss=5.310579691736141, w0=72.62673267326743, w1=15.972648447391363\n",
      "SubGD iter. 240/499: loss=5.310575990296659, w0=72.62673267326743, w1=15.971038786670276\n",
      "SubGD iter. 241/499: loss=5.310580796439089, w0=72.63366336633673, w1=15.97550748647768\n",
      "SubGD iter. 242/499: loss=5.310624267896667, w0=72.62673267326743, w1=15.973381386691942\n",
      "SubGD iter. 243/499: loss=5.310577675701806, w0=72.62673267326743, w1=15.971771725970855\n",
      "SubGD iter. 244/499: loss=5.310576117459501, w0=72.63366336633673, w1=15.97624042577826\n",
      "SubGD iter. 245/499: loss=5.31062649404251, w0=72.62673267326743, w1=15.974114325992522\n",
      "SubGD iter. 246/499: loss=5.310579361106954, w0=72.62673267326743, w1=15.972504665271435\n",
      "SubGD iter. 247/499: loss=5.310575659667472, w0=72.62673267326743, w1=15.970895004550348\n",
      "SubGD iter. 248/499: loss=5.310581714323563, w0=72.63366336633673, w1=15.975363704357752\n",
      "SubGD iter. 249/499: loss=5.310623831189332, w0=72.62673267326743, w1=15.973237604572015\n",
      "SubGD iter. 250/499: loss=5.31057734507262, w0=72.62673267326743, w1=15.971627943850928\n",
      "SubGD iter. 251/499: loss=5.310577035343975, w0=72.63366336633673, w1=15.976096643658332\n",
      "SubGD iter. 252/499: loss=5.310626057335176, w0=72.62673267326743, w1=15.973970543872595\n",
      "SubGD iter. 253/499: loss=5.310579030477766, w0=72.62673267326743, w1=15.972360883151508\n",
      "SubGD iter. 254/499: loss=5.3105753290382856, w0=72.62673267326743, w1=15.97075122243042\n",
      "SubGD iter. 255/499: loss=5.310582632208036, w0=72.63366336633673, w1=15.975219922237825\n",
      "SubGD iter. 256/499: loss=5.310623394481998, w0=72.62673267326743, w1=15.973093822452087\n",
      "SubGD iter. 257/499: loss=5.3105770144434326, w0=72.62673267326743, w1=15.971484161731\n",
      "SubGD iter. 258/499: loss=5.3105779532284485, w0=72.63366336633673, w1=15.975952861538405\n",
      "SubGD iter. 259/499: loss=5.3106256206278415, w0=72.62673267326743, w1=15.973826761752667\n",
      "SubGD iter. 260/499: loss=5.310578699848581, w0=72.62673267326743, w1=15.97221710103158\n",
      "SubGD iter. 261/499: loss=5.310574998409099, w0=72.62673267326743, w1=15.970607440310493\n",
      "SubGD iter. 262/499: loss=5.3105835500925105, w0=72.63366336633673, w1=15.975076140117897\n",
      "SubGD iter. 263/499: loss=5.3106229577746635, w0=72.62673267326743, w1=15.97295004033216\n",
      "SubGD iter. 264/499: loss=5.310576683814246, w0=72.62673267326743, w1=15.971340379611073\n",
      "SubGD iter. 265/499: loss=5.310578871112924, w0=72.63366336633673, w1=15.975809079418477\n",
      "SubGD iter. 266/499: loss=5.310625183920507, w0=72.62673267326743, w1=15.97368297963274\n",
      "SubGD iter. 267/499: loss=5.310578369219393, w0=72.62673267326743, w1=15.972073318911653\n",
      "SubGD iter. 268/499: loss=5.310574667779911, w0=72.62673267326743, w1=15.970463658190566\n",
      "SubGD iter. 269/499: loss=5.310584467976985, w0=72.63366336633673, w1=15.97493235799797\n",
      "SubGD iter. 270/499: loss=5.310622521067329, w0=72.62673267326743, w1=15.972806258212232\n",
      "SubGD iter. 271/499: loss=5.310576353185058, w0=72.62673267326743, w1=15.971196597491145\n",
      "SubGD iter. 272/499: loss=5.310579788997397, w0=72.63366336633673, w1=15.97566529729855\n",
      "SubGD iter. 273/499: loss=5.310624747213171, w0=72.62673267326743, w1=15.973539197512812\n",
      "SubGD iter. 274/499: loss=5.310578038590206, w0=72.62673267326743, w1=15.971929536791725\n",
      "SubGD iter. 275/499: loss=5.310575110017809, w0=72.63366336633673, w1=15.97639823659913\n",
      "SubGD iter. 276/499: loss=5.310626973359014, w0=72.62673267326743, w1=15.974272136813392\n",
      "SubGD iter. 277/499: loss=5.310579723995353, w0=72.62673267326743, w1=15.972662476092305\n",
      "SubGD iter. 278/499: loss=5.310576022555872, w0=72.62673267326743, w1=15.971052815371218\n",
      "SubGD iter. 279/499: loss=5.31058070688187, w0=72.63366336633673, w1=15.975521515178622\n",
      "SubGD iter. 280/499: loss=5.310624310505837, w0=72.62673267326743, w1=15.973395415392885\n",
      "SubGD iter. 281/499: loss=5.31057770796102, w0=72.62673267326743, w1=15.971785754671798\n",
      "SubGD iter. 282/499: loss=5.310576027902282, w0=72.63366336633673, w1=15.976254454479202\n",
      "SubGD iter. 283/499: loss=5.31062653665168, w0=72.62673267326743, w1=15.974128354693464\n",
      "SubGD iter. 284/499: loss=5.310579393366166, w0=72.62673267326743, w1=15.972518693972377\n",
      "SubGD iter. 285/499: loss=5.310575691926684, w0=72.62673267326743, w1=15.97090903325129\n",
      "SubGD iter. 286/499: loss=5.3105816247663435, w0=72.63366336633673, w1=15.975377733058695\n",
      "SubGD iter. 287/499: loss=5.310623873798502, w0=72.62673267326743, w1=15.973251633272957\n",
      "SubGD iter. 288/499: loss=5.310577377331832, w0=72.62673267326743, w1=15.97164197255187\n",
      "SubGD iter. 289/499: loss=5.310576945786757, w0=72.63366336633673, w1=15.976110672359274\n",
      "SubGD iter. 290/499: loss=5.310626099944344, w0=72.62673267326743, w1=15.973984572573537\n",
      "SubGD iter. 291/499: loss=5.310579062736979, w0=72.62673267326743, w1=15.97237491185245\n",
      "SubGD iter. 292/499: loss=5.310575361297499, w0=72.62673267326743, w1=15.970765251131363\n",
      "SubGD iter. 293/499: loss=5.310582542650818, w0=72.63366336633673, w1=15.975233950938767\n",
      "SubGD iter. 294/499: loss=5.310623437091167, w0=72.62673267326743, w1=15.97310785115303\n",
      "SubGD iter. 295/499: loss=5.310577046702645, w0=72.62673267326743, w1=15.971498190431943\n",
      "SubGD iter. 296/499: loss=5.31057786367123, w0=72.63366336633673, w1=15.975966890239347\n",
      "SubGD iter. 297/499: loss=5.310625663237009, w0=72.62673267326743, w1=15.97384079045361\n",
      "SubGD iter. 298/499: loss=5.310578732107792, w0=72.62673267326743, w1=15.972231129732522\n",
      "SubGD iter. 299/499: loss=5.310575030668311, w0=72.62673267326743, w1=15.970621469011435\n",
      "SubGD iter. 300/499: loss=5.310583460535291, w0=72.63366336633673, w1=15.97509016881884\n",
      "SubGD iter. 301/499: loss=5.310623000383832, w0=72.62673267326743, w1=15.972964069033102\n",
      "SubGD iter. 302/499: loss=5.3105767160734585, w0=72.62673267326743, w1=15.971354408312015\n",
      "SubGD iter. 303/499: loss=5.310578781555703, w0=72.63366336633673, w1=15.97582310811942\n",
      "SubGD iter. 304/499: loss=5.310625226529674, w0=72.62673267326743, w1=15.973697008333682\n",
      "SubGD iter. 305/499: loss=5.3105784014786055, w0=72.62673267326743, w1=15.972087347612595\n",
      "SubGD iter. 306/499: loss=5.3105747000391235, w0=72.62673267326743, w1=15.970477686891508\n",
      "SubGD iter. 307/499: loss=5.310584378419766, w0=72.63366336633673, w1=15.974946386698912\n",
      "SubGD iter. 308/499: loss=5.310622563676496, w0=72.62673267326743, w1=15.972820286913175\n",
      "SubGD iter. 309/499: loss=5.3105763854442705, w0=72.62673267326743, w1=15.971210626192088\n",
      "SubGD iter. 310/499: loss=5.310579699440178, w0=72.63366336633673, w1=15.975679325999492\n",
      "SubGD iter. 311/499: loss=5.31062478982234, w0=72.62673267326743, w1=15.973553226213754\n",
      "SubGD iter. 312/499: loss=5.310578070849419, w0=72.62673267326743, w1=15.971943565492667\n",
      "SubGD iter. 313/499: loss=5.310575020460591, w0=72.63366336633673, w1=15.976412265300072\n",
      "SubGD iter. 314/499: loss=5.3106270159681825, w0=72.62673267326743, w1=15.974286165514334\n",
      "SubGD iter. 315/499: loss=5.310579756254565, w0=72.62673267326743, w1=15.972676504793247\n",
      "SubGD iter. 316/499: loss=5.310576054815085, w0=72.62673267326743, w1=15.97106684407216\n",
      "SubGD iter. 317/499: loss=5.310580617324651, w0=72.63366336633673, w1=15.975535543879564\n",
      "SubGD iter. 318/499: loss=5.3106243531150055, w0=72.62673267326743, w1=15.973409444093827\n",
      "SubGD iter. 319/499: loss=5.310577740220231, w0=72.62673267326743, w1=15.97179978337274\n",
      "SubGD iter. 320/499: loss=5.310575938345063, w0=72.63366336633673, w1=15.976268483180144\n",
      "SubGD iter. 321/499: loss=5.310626579260848, w0=72.62673267326743, w1=15.974142383394407\n",
      "SubGD iter. 322/499: loss=5.310579425625379, w0=72.62673267326743, w1=15.97253272267332\n",
      "SubGD iter. 323/499: loss=5.310575724185897, w0=72.62673267326743, w1=15.970923061952233\n",
      "SubGD iter. 324/499: loss=5.310581535209124, w0=72.63366336633673, w1=15.975391761759637\n",
      "SubGD iter. 325/499: loss=5.310623916407671, w0=72.62673267326743, w1=15.9732656619739\n",
      "SubGD iter. 326/499: loss=5.310577409591045, w0=72.62673267326743, w1=15.971656001252812\n",
      "SubGD iter. 327/499: loss=5.310576856229538, w0=72.63366336633673, w1=15.976124701060217\n",
      "SubGD iter. 328/499: loss=5.310626142553512, w0=72.62673267326743, w1=15.973998601274479\n",
      "SubGD iter. 329/499: loss=5.310579094996193, w0=72.62673267326743, w1=15.972388940553392\n",
      "SubGD iter. 330/499: loss=5.310575393556711, w0=72.62673267326743, w1=15.970779279832305\n",
      "SubGD iter. 331/499: loss=5.310582453093599, w0=72.63366336633673, w1=15.97524797963971\n",
      "SubGD iter. 332/499: loss=5.310623479700335, w0=72.62673267326743, w1=15.973121879853972\n",
      "SubGD iter. 333/499: loss=5.310577078961859, w0=72.62673267326743, w1=15.971512219132885\n",
      "SubGD iter. 334/499: loss=5.3105777741140106, w0=72.63366336633673, w1=15.975980918940289\n",
      "SubGD iter. 335/499: loss=5.310625705846178, w0=72.62673267326743, w1=15.973854819154552\n",
      "SubGD iter. 336/499: loss=5.310578764367005, w0=72.62673267326743, w1=15.972245158433465\n",
      "SubGD iter. 337/499: loss=5.3105750629275255, w0=72.62673267326743, w1=15.970635497712378\n",
      "SubGD iter. 338/499: loss=5.3105833709780725, w0=72.63366336633673, w1=15.975104197519782\n",
      "SubGD iter. 339/499: loss=5.310623042993001, w0=72.62673267326743, w1=15.972978097734044\n",
      "SubGD iter. 340/499: loss=5.310576748332672, w0=72.62673267326743, w1=15.971368437012957\n",
      "SubGD iter. 341/499: loss=5.310578691998486, w0=72.63366336633673, w1=15.975837136820362\n",
      "SubGD iter. 342/499: loss=5.310625269138844, w0=72.62673267326743, w1=15.973711037034624\n",
      "SubGD iter. 343/499: loss=5.310578433737818, w0=72.62673267326743, w1=15.972101376313537\n",
      "SubGD iter. 344/499: loss=5.3105747322983365, w0=72.62673267326743, w1=15.97049171559245\n",
      "SubGD iter. 345/499: loss=5.310584288862547, w0=72.63366336633673, w1=15.974960415399854\n",
      "SubGD iter. 346/499: loss=5.3106226062856665, w0=72.62673267326743, w1=15.972834315614117\n",
      "SubGD iter. 347/499: loss=5.310576417703483, w0=72.62673267326743, w1=15.97122465489303\n",
      "SubGD iter. 348/499: loss=5.310579609882957, w0=72.63366336633673, w1=15.975693354700434\n",
      "SubGD iter. 349/499: loss=5.310624832431509, w0=72.62673267326743, w1=15.973567254914697\n",
      "SubGD iter. 350/499: loss=5.310578103108631, w0=72.62673267326743, w1=15.97195759419361\n",
      "SubGD iter. 351/499: loss=5.310574930903371, w0=72.63366336633673, w1=15.976426294001014\n",
      "SubGD iter. 352/499: loss=5.310627058577351, w0=72.62673267326743, w1=15.974300194215276\n",
      "SubGD iter. 353/499: loss=5.3105797885137775, w0=72.62673267326743, w1=15.97269053349419\n",
      "SubGD iter. 354/499: loss=5.310576087074297, w0=72.62673267326743, w1=15.971080872773102\n",
      "SubGD iter. 355/499: loss=5.310580527767432, w0=72.63366336633673, w1=15.975549572580507\n",
      "SubGD iter. 356/499: loss=5.310624395724175, w0=72.62673267326743, w1=15.973423472794769\n",
      "SubGD iter. 357/499: loss=5.310577772479444, w0=72.62673267326743, w1=15.971813812073682\n",
      "SubGD iter. 358/499: loss=5.3105758487878445, w0=72.63366336633673, w1=15.976282511881086\n",
      "SubGD iter. 359/499: loss=5.310626621870016, w0=72.62673267326743, w1=15.974156412095349\n",
      "SubGD iter. 360/499: loss=5.310579457884592, w0=72.62673267326743, w1=15.972546751374262\n",
      "SubGD iter. 361/499: loss=5.31057575644511, w0=72.62673267326743, w1=15.970937090653175\n",
      "SubGD iter. 362/499: loss=5.310581445651906, w0=72.63366336633673, w1=15.975405790460579\n",
      "SubGD iter. 363/499: loss=5.310623959016839, w0=72.62673267326743, w1=15.973279690674842\n",
      "SubGD iter. 364/499: loss=5.310577441850257, w0=72.62673267326743, w1=15.971670029953755\n",
      "SubGD iter. 365/499: loss=5.310576766672319, w0=72.63366336633673, w1=15.976138729761159\n",
      "SubGD iter. 366/499: loss=5.310626185162682, w0=72.62673267326743, w1=15.974012629975421\n",
      "SubGD iter. 367/499: loss=5.310579127255404, w0=72.62673267326743, w1=15.972402969254334\n",
      "SubGD iter. 368/499: loss=5.310575425815924, w0=72.62673267326743, w1=15.970793308533247\n",
      "SubGD iter. 369/499: loss=5.31058236353638, w0=72.63366336633673, w1=15.975262008340652\n",
      "SubGD iter. 370/499: loss=5.310623522309504, w0=72.62673267326743, w1=15.973135908554914\n",
      "SubGD iter. 371/499: loss=5.31057711122107, w0=72.62673267326743, w1=15.971526247833827\n",
      "SubGD iter. 372/499: loss=5.310577684556793, w0=72.63366336633673, w1=15.975994947641231\n",
      "SubGD iter. 373/499: loss=5.3106257484553465, w0=72.62673267326743, w1=15.973868847855494\n",
      "SubGD iter. 374/499: loss=5.310578796626218, w0=72.62673267326743, w1=15.972259187134407\n",
      "SubGD iter. 375/499: loss=5.310575095186736, w0=72.62673267326743, w1=15.97064952641332\n",
      "SubGD iter. 376/499: loss=5.310583281420854, w0=72.63366336633673, w1=15.975118226220724\n",
      "SubGD iter. 377/499: loss=5.3106230856021694, w0=72.62673267326743, w1=15.972992126434987\n",
      "SubGD iter. 378/499: loss=5.310576780591885, w0=72.62673267326743, w1=15.9713824657139\n",
      "SubGD iter. 379/499: loss=5.310578602441266, w0=72.63366336633673, w1=15.975851165521304\n",
      "SubGD iter. 380/499: loss=5.310625311748012, w0=72.62673267326743, w1=15.973725065735566\n",
      "SubGD iter. 381/499: loss=5.310578465997031, w0=72.62673267326743, w1=15.97211540501448\n",
      "SubGD iter. 382/499: loss=5.31057476455755, w0=72.62673267326743, w1=15.970505744293392\n",
      "SubGD iter. 383/499: loss=5.310584199305326, w0=72.63366336633673, w1=15.974974444100797\n",
      "SubGD iter. 384/499: loss=5.310622648894835, w0=72.62673267326743, w1=15.972848344315059\n",
      "SubGD iter. 385/499: loss=5.310576449962697, w0=72.62673267326743, w1=15.971238683593972\n",
      "SubGD iter. 386/499: loss=5.310579520325739, w0=72.63366336633673, w1=15.975707383401376\n",
      "SubGD iter. 387/499: loss=5.310624875040678, w0=72.62673267326743, w1=15.973581283615639\n",
      "SubGD iter. 388/499: loss=5.3105781353678445, w0=72.62673267326743, w1=15.971971622894552\n",
      "SubGD iter. 389/499: loss=5.310574841346153, w0=72.63366336633673, w1=15.976440322701956\n",
      "SubGD iter. 390/499: loss=5.31062710118652, w0=72.62673267326743, w1=15.974314222916218\n",
      "SubGD iter. 391/499: loss=5.3105798207729915, w0=72.62673267326743, w1=15.972704562195132\n",
      "SubGD iter. 392/499: loss=5.3105761193335095, w0=72.62673267326743, w1=15.971094901474045\n",
      "SubGD iter. 393/499: loss=5.310580438210215, w0=72.63366336633673, w1=15.975563601281449\n",
      "SubGD iter. 394/499: loss=5.310624438333342, w0=72.62673267326743, w1=15.973437501495711\n",
      "SubGD iter. 395/499: loss=5.3105778047386565, w0=72.62673267326743, w1=15.971827840774624\n",
      "SubGD iter. 396/499: loss=5.310575759230627, w0=72.63366336633673, w1=15.976296540582029\n",
      "SubGD iter. 397/499: loss=5.3106266644791855, w0=72.62673267326743, w1=15.974170440796291\n",
      "SubGD iter. 398/499: loss=5.310579490143804, w0=72.62673267326743, w1=15.972560780075204\n",
      "SubGD iter. 399/499: loss=5.310575788704323, w0=72.62673267326743, w1=15.970951119354117\n",
      "SubGD iter. 400/499: loss=5.310581356094687, w0=72.63366336633673, w1=15.975419819161521\n",
      "SubGD iter. 401/499: loss=5.310624001626008, w0=72.62673267326743, w1=15.973293719375784\n",
      "SubGD iter. 402/499: loss=5.31057747410947, w0=72.62673267326743, w1=15.971684058654697\n",
      "SubGD iter. 403/499: loss=5.310576677115099, w0=72.63366336633673, w1=15.976152758462101\n",
      "SubGD iter. 404/499: loss=5.31062622777185, w0=72.62673267326743, w1=15.974026658676364\n",
      "SubGD iter. 405/499: loss=5.310579159514617, w0=72.62673267326743, w1=15.972416997955277\n",
      "SubGD iter. 406/499: loss=5.310575458075137, w0=72.62673267326743, w1=15.97080733723419\n",
      "SubGD iter. 407/499: loss=5.310582273979161, w0=72.63366336633673, w1=15.975276037041594\n",
      "SubGD iter. 408/499: loss=5.310623564918673, w0=72.62673267326743, w1=15.973149937255856\n",
      "SubGD iter. 409/499: loss=5.310577143480284, w0=72.62673267326743, w1=15.97154027653477\n",
      "SubGD iter. 410/499: loss=5.310577594999573, w0=72.63366336633673, w1=15.976008976342174\n",
      "SubGD iter. 411/499: loss=5.310625791064516, w0=72.62673267326743, w1=15.973882876556436\n",
      "SubGD iter. 412/499: loss=5.31057882888543, w0=72.62673267326743, w1=15.972273215835349\n",
      "SubGD iter. 413/499: loss=5.310575127445949, w0=72.62673267326743, w1=15.970663555114262\n",
      "SubGD iter. 414/499: loss=5.3105831918636355, w0=72.63366336633673, w1=15.975132254921666\n",
      "SubGD iter. 415/499: loss=5.310623128211338, w0=72.62673267326743, w1=15.973006155135929\n",
      "SubGD iter. 416/499: loss=5.310576812851097, w0=72.62673267326743, w1=15.971396494414842\n",
      "SubGD iter. 417/499: loss=5.310578512884048, w0=72.63366336633673, w1=15.975865194222246\n",
      "SubGD iter. 418/499: loss=5.31062535435718, w0=72.62673267326743, w1=15.973739094436509\n",
      "SubGD iter. 419/499: loss=5.310578498256244, w0=72.62673267326743, w1=15.972129433715422\n",
      "SubGD iter. 420/499: loss=5.310574796816762, w0=72.62673267326743, w1=15.970519772994335\n",
      "SubGD iter. 421/499: loss=5.31058410974811, w0=72.63366336633673, w1=15.974988472801739\n",
      "SubGD iter. 422/499: loss=5.310622691504004, w0=72.62673267326743, w1=15.972862373016001\n",
      "SubGD iter. 423/499: loss=5.310576482221909, w0=72.62673267326743, w1=15.971252712294914\n",
      "SubGD iter. 424/499: loss=5.310579430768521, w0=72.63366336633673, w1=15.975721412102319\n",
      "SubGD iter. 425/499: loss=5.310624917649847, w0=72.62673267326743, w1=15.973595312316581\n",
      "SubGD iter. 426/499: loss=5.310578167627056, w0=72.62673267326743, w1=15.971985651595494\n",
      "SubGD iter. 427/499: loss=5.310574751788934, w0=72.63366336633673, w1=15.976454351402898\n",
      "SubGD iter. 428/499: loss=5.310627143795689, w0=72.62673267326743, w1=15.97432825161716\n",
      "SubGD iter. 429/499: loss=5.310579853032204, w0=72.62673267326743, w1=15.972718590896074\n",
      "SubGD iter. 430/499: loss=5.310576151592722, w0=72.62673267326743, w1=15.971108930174987\n",
      "SubGD iter. 431/499: loss=5.310580348652995, w0=72.63366336633673, w1=15.975577629982391\n",
      "SubGD iter. 432/499: loss=5.310624480942511, w0=72.62673267326743, w1=15.973451530196654\n",
      "SubGD iter. 433/499: loss=5.31057783699787, w0=72.62673267326743, w1=15.971841869475567\n",
      "SubGD iter. 434/499: loss=5.310575669673408, w0=72.63366336633673, w1=15.97631056928297\n",
      "SubGD iter. 435/499: loss=5.310626707088354, w0=72.62673267326743, w1=15.974184469497233\n",
      "SubGD iter. 436/499: loss=5.3105795224030174, w0=72.62673267326743, w1=15.972574808776146\n",
      "SubGD iter. 437/499: loss=5.310575820963536, w0=72.62673267326743, w1=15.97096514805506\n",
      "SubGD iter. 438/499: loss=5.3105812665374685, w0=72.63366336633673, w1=15.975433847862464\n",
      "SubGD iter. 439/499: loss=5.310624044235177, w0=72.62673267326743, w1=15.973307748076726\n",
      "SubGD iter. 440/499: loss=5.310577506368683, w0=72.62673267326743, w1=15.97169808735564\n",
      "SubGD iter. 441/499: loss=5.310576587557881, w0=72.63366336633673, w1=15.976166787163043\n",
      "SubGD iter. 442/499: loss=5.31062627038102, w0=72.62673267326743, w1=15.974040687377306\n",
      "SubGD iter. 443/499: loss=5.31057919177383, w0=72.62673267326743, w1=15.972431026656219\n",
      "SubGD iter. 444/499: loss=5.310575490334348, w0=72.62673267326743, w1=15.970821365935132\n",
      "SubGD iter. 445/499: loss=5.310582184421943, w0=72.63366336633673, w1=15.975290065742536\n",
      "SubGD iter. 446/499: loss=5.310623607527843, w0=72.62673267326743, w1=15.973163965956799\n",
      "SubGD iter. 447/499: loss=5.310577175739496, w0=72.62673267326743, w1=15.971554305235712\n",
      "SubGD iter. 448/499: loss=5.310577505442355, w0=72.63366336633673, w1=15.976023005043116\n",
      "SubGD iter. 449/499: loss=5.310625833673684, w0=72.62673267326743, w1=15.973896905257378\n",
      "SubGD iter. 450/499: loss=5.310578861144643, w0=72.62673267326743, w1=15.972287244536291\n",
      "SubGD iter. 451/499: loss=5.310575159705161, w0=72.62673267326743, w1=15.970677583815204\n",
      "SubGD iter. 452/499: loss=5.310583102306416, w0=72.63366336633673, w1=15.975146283622609\n",
      "SubGD iter. 453/499: loss=5.310623170820506, w0=72.62673267326743, w1=15.973020183836871\n",
      "SubGD iter. 454/499: loss=5.310576845110308, w0=72.62673267326743, w1=15.971410523115784\n",
      "SubGD iter. 455/499: loss=5.310578423326828, w0=72.63366336633673, w1=15.975879222923188\n",
      "SubGD iter. 456/499: loss=5.3106253969663495, w0=72.62673267326743, w1=15.97375312313745\n",
      "SubGD iter. 457/499: loss=5.310578530515456, w0=72.62673267326743, w1=15.972143462416364\n",
      "SubGD iter. 458/499: loss=5.310574829075976, w0=72.62673267326743, w1=15.970533801695277\n",
      "SubGD iter. 459/499: loss=5.31058402019089, w0=72.63366336633673, w1=15.975002501502681\n",
      "SubGD iter. 460/499: loss=5.3106227341131715, w0=72.62673267326743, w1=15.972876401716944\n",
      "SubGD iter. 461/499: loss=5.310576514481123, w0=72.62673267326743, w1=15.971266740995857\n",
      "SubGD iter. 462/499: loss=5.3105793412113025, w0=72.63366336633673, w1=15.97573544080326\n",
      "SubGD iter. 463/499: loss=5.310624960259015, w0=72.62673267326743, w1=15.973609341017523\n",
      "SubGD iter. 464/499: loss=5.31057819988627, w0=72.62673267326743, w1=15.971999680296436\n",
      "SubGD iter. 465/499: loss=5.310574662231714, w0=72.63366336633673, w1=15.97646838010384\n",
      "SubGD iter. 466/499: loss=5.310627186404858, w0=72.62673267326743, w1=15.974342280318103\n",
      "SubGD iter. 467/499: loss=5.310579885291418, w0=72.62673267326743, w1=15.972732619597016\n",
      "SubGD iter. 468/499: loss=5.310576183851936, w0=72.62673267326743, w1=15.97112295887593\n",
      "SubGD iter. 469/499: loss=5.310580259095775, w0=72.63366336633673, w1=15.975591658683333\n",
      "SubGD iter. 470/499: loss=5.31062452355168, w0=72.62673267326743, w1=15.973465558897596\n",
      "SubGD iter. 471/499: loss=5.310577869257083, w0=72.62673267326743, w1=15.971855898176509\n",
      "SubGD iter. 472/499: loss=5.310575580116187, w0=72.63366336633673, w1=15.976324597983913\n",
      "SubGD iter. 473/499: loss=5.310626749697522, w0=72.62673267326743, w1=15.974198498198175\n",
      "SubGD iter. 474/499: loss=5.3105795546622305, w0=72.62673267326743, w1=15.972588837477089\n",
      "SubGD iter. 475/499: loss=5.3105758532227485, w0=72.62673267326743, w1=15.970979176756002\n",
      "SubGD iter. 476/499: loss=5.310581176980251, w0=72.63366336633673, w1=15.975447876563406\n",
      "SubGD iter. 477/499: loss=5.310624086844346, w0=72.62673267326743, w1=15.973321776777668\n",
      "SubGD iter. 478/499: loss=5.3105775386278955, w0=72.62673267326743, w1=15.971712116056581\n",
      "SubGD iter. 479/499: loss=5.3105764980006605, w0=72.63366336633673, w1=15.976180815863986\n",
      "SubGD iter. 480/499: loss=5.310626312990188, w0=72.62673267326743, w1=15.974054716078248\n",
      "SubGD iter. 481/499: loss=5.3105792240330425, w0=72.62673267326743, w1=15.972445055357161\n",
      "SubGD iter. 482/499: loss=5.3105755225935605, w0=72.62673267326743, w1=15.970835394636074\n",
      "SubGD iter. 483/499: loss=5.310582094864723, w0=72.63366336633673, w1=15.975304094443478\n",
      "SubGD iter. 484/499: loss=5.31062365013701, w0=72.62673267326743, w1=15.97317799465774\n",
      "SubGD iter. 485/499: loss=5.310577207998708, w0=72.62673267326743, w1=15.971568333936654\n",
      "SubGD iter. 486/499: loss=5.3105774158851355, w0=72.63366336633673, w1=15.976037033744058\n",
      "SubGD iter. 487/499: loss=5.310625876282852, w0=72.62673267326743, w1=15.97391093395832\n",
      "SubGD iter. 488/499: loss=5.310578893403856, w0=72.62673267326743, w1=15.972301273237234\n",
      "SubGD iter. 489/499: loss=5.310575191964374, w0=72.62673267326743, w1=15.970691612516147\n",
      "SubGD iter. 490/499: loss=5.3105830127491975, w0=72.63366336633673, w1=15.97516031232355\n",
      "SubGD iter. 491/499: loss=5.310623213429675, w0=72.62673267326743, w1=15.973034212537813\n",
      "SubGD iter. 492/499: loss=5.310576877369521, w0=72.62673267326743, w1=15.971424551816726\n",
      "SubGD iter. 493/499: loss=5.31057833376961, w0=72.63366336633673, w1=15.97589325162413\n",
      "SubGD iter. 494/499: loss=5.310625439575518, w0=72.62673267326743, w1=15.973767151838393\n",
      "SubGD iter. 495/499: loss=5.310578562774669, w0=72.62673267326743, w1=15.972157491117306\n",
      "SubGD iter. 496/499: loss=5.310574861335188, w0=72.62673267326743, w1=15.97054783039622\n",
      "SubGD iter. 497/499: loss=5.310583930633671, w0=72.63366336633673, w1=15.975016530203623\n",
      "SubGD iter. 498/499: loss=5.310622776722341, w0=72.62673267326743, w1=15.972890430417886\n",
      "SubGD iter. 499/499: loss=5.310576546740335, w0=72.62673267326743, w1=15.971280769696799\n",
      "SubGD: execution time=0.013 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f546fdeba8d74ffbad828fa08926995e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses,\n",
    "        subgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Compute a stochastic subgradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        # ***************************************************\n",
    "        for minibatch_y,minibatch_tx in batch_iter(y,tx,batch_size):\n",
    "            subgrad = compute_subgradient_mae(minibatch_y,minibatch_tx,w)\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        w = w - gamma*subgrad\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "\n",
    "        print(\n",
    "            \"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=-0.678346995974543\n",
      "SubSGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=-0.07294384453055835\n",
      "SubSGD iter. 2/499: loss=72.66780585492639, w0=2.0999999999999996, w1=1.2898075508611884\n",
      "SubSGD iter. 3/499: loss=71.96780585492637, w0=2.8, w1=1.1056467725428685\n",
      "SubSGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=1.8670037231619379\n",
      "SubSGD iter. 5/499: loss=70.56780585492639, w0=4.2, w1=1.1803090876558011\n",
      "SubSGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=1.5353365651856272\n",
      "SubSGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=1.330503045310404\n",
      "SubSGD iter. 8/499: loss=68.46780585492638, w0=6.300000000000001, w1=1.9616683853052566\n",
      "SubSGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=0.9347361877127229\n",
      "SubSGD iter. 10/499: loss=67.06780585492638, w0=7.700000000000001, w1=1.5477286848962826\n",
      "SubSGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=1.8178671351859526\n",
      "SubSGD iter. 12/499: loss=65.66780585492637, w0=9.1, w1=1.9501906615397913\n",
      "SubSGD iter. 13/499: loss=64.96780585492638, w0=9.799999999999999, w1=2.7453603866571386\n",
      "SubSGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=2.3632344021068037\n",
      "SubSGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=2.006508883750967\n",
      "SubSGD iter. 16/499: loss=62.867805854926374, w0=11.899999999999997, w1=0.8225066833570753\n",
      "SubSGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=-0.017520888102735555\n",
      "SubSGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=-0.9191531525154932\n",
      "SubSGD iter. 19/499: loss=60.76780585492637, w0=13.999999999999995, w1=-1.7729994229407624\n",
      "SubSGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=-2.3160158843058607\n",
      "SubSGD iter. 21/499: loss=59.367805854926395, w0=15.399999999999993, w1=-2.747808462589281\n",
      "SubSGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=-2.1243203403031035\n",
      "SubSGD iter. 23/499: loss=57.96780585492639, w0=16.799999999999994, w1=-1.9278002575246007\n",
      "SubSGD iter. 24/499: loss=57.26780585492638, w0=17.499999999999993, w1=-1.3980992834624846\n",
      "SubSGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=-0.8683983094003684\n",
      "SubSGD iter. 26/499: loss=55.86780585492639, w0=18.89999999999999, w1=-0.7029455174677426\n",
      "SubSGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=-0.5245933960879282\n",
      "SubSGD iter. 28/499: loss=54.46780585492639, w0=20.29999999999999, w1=-0.9001792343744979\n",
      "SubSGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=-1.0820430779496668\n",
      "SubSGD iter. 30/499: loss=53.067805854926384, w0=21.69999999999999, w1=-1.7855479963405294\n",
      "SubSGD iter. 31/499: loss=52.36780585492639, w0=22.399999999999988, w1=-1.9674118399156981\n",
      "SubSGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=-1.426454787944556\n",
      "SubSGD iter. 33/499: loss=50.9678058549264, w0=23.799999999999986, w1=-1.631288307819779\n",
      "SubSGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=-1.872268222486427\n",
      "SubSGD iter. 35/499: loss=49.567805854926405, w0=25.199999999999985, w1=-1.3238737187286163\n",
      "SubSGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=-0.9886456634253613\n",
      "SubSGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=-1.1156601086533864\n",
      "SubSGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=-2.241592522154557\n",
      "SubSGD iter. 39/499: loss=46.767805854926394, w0=27.999999999999982, w1=-1.8978799009711667\n",
      "SubSGD iter. 40/499: loss=46.06780585492639, w0=28.69999999999998, w1=-2.1419067256803177\n",
      "SubSGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=-2.4889907113003087\n",
      "SubSGD iter. 42/499: loss=44.66780585492641, w0=30.09999999999998, w1=-2.2598690190135504\n",
      "SubSGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=-1.6468765218299906\n",
      "SubSGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=-2.590383005367111\n",
      "SubSGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=-1.577704664014344\n",
      "SubSGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=-1.9771624364009228\n",
      "SubSGD iter. 47/499: loss=41.1678058549264, w0=33.59999999999999, w1=-1.7123773272101546\n",
      "SubSGD iter. 48/499: loss=40.4678058549264, w0=34.29999999999999, w1=-2.749891521023695\n",
      "SubSGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=-2.7845823803651664\n",
      "SubSGD iter. 50/499: loss=39.067805854926384, w0=35.699999999999996, w1=-2.9680481594155856\n",
      "SubSGD iter. 51/499: loss=38.36780585492639, w0=36.4, w1=-3.1185048358667133\n",
      "SubSGD iter. 52/499: loss=37.667805854926385, w0=37.1, w1=-3.7412912426870055\n",
      "SubSGD iter. 53/499: loss=36.96780585492638, w0=37.800000000000004, w1=-4.0264657372362995\n",
      "SubSGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=-3.602924484182284\n",
      "SubSGD iter. 55/499: loss=35.56780585492638, w0=39.20000000000001, w1=-3.3526433677202276\n",
      "SubSGD iter. 56/499: loss=34.867805854926374, w0=39.90000000000001, w1=-3.977459467531174\n",
      "SubSGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=-3.8116532079636767\n",
      "SubSGD iter. 58/499: loss=33.47159643167563, w0=41.30000000000002, w1=-3.388111954909661\n",
      "SubSGD iter. 59/499: loss=32.77178202113434, w0=42.00000000000002, w1=-3.7351959405296515\n",
      "SubSGD iter. 60/499: loss=32.098433834614994, w0=42.700000000000024, w1=-2.722757602663231\n",
      "SubSGD iter. 61/499: loss=31.375047310644113, w0=43.40000000000003, w1=-3.5171269471540114\n",
      "SubSGD iter. 62/499: loss=30.73307871726833, w0=44.10000000000003, w1=-3.228629682596596\n",
      "SubSGD iter. 63/499: loss=30.041403673976145, w0=44.80000000000003, w1=-4.172136166133717\n",
      "SubSGD iter. 64/499: loss=29.46710715792251, w0=45.500000000000036, w1=-3.159697828267296\n",
      "SubSGD iter. 65/499: loss=28.70848186857995, w0=46.20000000000004, w1=-3.403724652976447\n",
      "SubSGD iter. 66/499: loss=28.086040154232844, w0=46.90000000000004, w1=-4.174547719044634\n",
      "SubSGD iter. 67/499: loss=27.562641496697335, w0=46.20000000000004, w1=-3.0486153055434633\n",
      "SubSGD iter. 68/499: loss=28.04524153786485, w0=46.90000000000004, w1=-3.8886428770032744\n",
      "SubSGD iter. 69/499: loss=27.51817649252957, w0=47.600000000000044, w1=-4.461652960017898\n",
      "SubSGD iter. 70/499: loss=27.00838180878963, w0=48.30000000000005, w1=-3.5147406982754674\n",
      "SubSGD iter. 71/499: loss=26.223874297879487, w0=49.00000000000005, w1=-2.9017482010919076\n",
      "SubSGD iter. 72/499: loss=25.50364697915208, w0=49.70000000000005, w1=-3.3249876258644218\n",
      "SubSGD iter. 73/499: loss=24.987130305068092, w0=50.400000000000055, w1=-3.525737162913673\n",
      "SubSGD iter. 74/499: loss=24.445476234944735, w0=51.10000000000006, w1=-3.170709685383847\n",
      "SubSGD iter. 75/499: loss=23.78706520910693, w0=51.80000000000006, w1=-2.6426538698731985\n",
      "SubSGD iter. 76/499: loss=23.088123116565285, w0=51.10000000000006, w1=-1.9620546922171158\n",
      "SubSGD iter. 77/499: loss=23.515510396756625, w0=51.80000000000006, w1=-2.3615124646036945\n",
      "SubSGD iter. 78/499: loss=23.022267077125935, w0=52.500000000000064, w1=-2.708596450223685\n",
      "SubSGD iter. 79/499: loss=22.535237522786936, w0=53.20000000000007, w1=-2.99114426930991\n",
      "SubSGD iter. 80/499: loss=22.095161307880335, w0=52.500000000000064, w1=-2.2876393509190476\n",
      "SubSGD iter. 81/499: loss=22.429715126212315, w0=53.20000000000007, w1=-0.9222698982554365\n",
      "SubSGD iter. 82/499: loss=21.53463628939213, w0=53.90000000000007, w1=0.1341313331234535\n",
      "SubSGD iter. 83/499: loss=20.71303855369687, w0=54.60000000000007, w1=-0.04773251045171534\n",
      "SubSGD iter. 84/499: loss=20.181335096318133, w0=55.300000000000075, w1=0.14878757232678738\n",
      "SubSGD iter. 85/499: loss=19.582026459700344, w0=56.00000000000008, w1=-0.20115035915352983\n",
      "SubSGD iter. 86/499: loss=19.175992842019102, w0=55.300000000000075, w1=0.9828518412403616\n",
      "SubSGD iter. 87/499: loss=19.36464032896185, w0=56.00000000000008, w1=0.18848249674958129\n",
      "SubSGD iter. 88/499: loss=19.047366817177753, w0=56.70000000000008, w1=0.8314121955133769\n",
      "SubSGD iter. 89/499: loss=18.33907204788707, w0=57.400000000000084, w1=1.4743418942771724\n",
      "SubSGD iter. 90/499: loss=17.630960528673317, w0=58.10000000000009, w1=0.9846583608805841\n",
      "SubSGD iter. 91/499: loss=17.31770339913522, w0=58.80000000000009, w1=0.6482954032045074\n",
      "SubSGD iter. 92/499: loss=17.0178886645967, w0=58.10000000000009, w1=1.4426647476952876\n",
      "SubSGD iter. 93/499: loss=17.15475846849454, w0=57.400000000000084, w1=2.1461696660861502\n",
      "SubSGD iter. 94/499: loss=17.423645004620788, w0=58.10000000000009, w1=3.2387941804119444\n",
      "SubSGD iter. 95/499: loss=16.590861929620004, w0=58.80000000000009, w1=3.2041033210704732\n",
      "SubSGD iter. 96/499: loss=16.08792045334357, w0=59.50000000000009, w1=3.1903481250949826\n",
      "SubSGD iter. 97/499: loss=15.609013026771079, w0=60.200000000000095, w1=2.845171037369984\n",
      "SubSGD iter. 98/499: loss=15.306406341195883, w0=60.9000000000001, w1=2.6279140331902338\n",
      "SubSGD iter. 99/499: loss=14.995419524354556, w0=61.6000000000001, w1=3.094496159156809\n",
      "SubSGD iter. 100/499: loss=14.416951928790569, w0=62.300000000000104, w1=4.262826959674277\n",
      "SubSGD iter. 101/499: loss=13.542247895071364, w0=63.00000000000011, w1=4.908974193061498\n",
      "SubSGD iter. 102/499: loss=12.90181429898643, w0=63.70000000000011, w1=4.324661655515607\n",
      "SubSGD iter. 103/499: loss=12.831197655985207, w0=64.4000000000001, w1=5.16585182374025\n",
      "SubSGD iter. 104/499: loss=12.114140704245822, w0=65.10000000000011, w1=5.789339946026428\n",
      "SubSGD iter. 105/499: loss=11.508610445796755, w0=64.4000000000001, w1=6.523454269027416\n",
      "SubSGD iter. 106/499: loss=11.491857944752699, w0=63.7000000000001, w1=7.028472000851739\n",
      "SubSGD iter. 107/499: loss=11.634458401098406, w0=64.4000000000001, w1=7.293257110042507\n",
      "SubSGD iter. 108/499: loss=11.155289783311767, w0=63.7000000000001, w1=7.782940643439096\n",
      "SubSGD iter. 109/499: loss=11.356297787120326, w0=64.4000000000001, w1=8.662112955669903\n",
      "SubSGD iter. 110/499: loss=10.639664827747843, w0=65.10000000000011, w1=8.775933282484216\n",
      "SubSGD iter. 111/499: loss=10.190026811885742, w0=65.80000000000011, w1=5.402789604894538\n",
      "SubSGD iter. 112/499: loss=11.433495043265392, w0=66.50000000000011, w1=4.663456805450594\n",
      "SubSGD iter. 113/499: loss=11.618386163061627, w0=67.20000000000012, w1=5.364390802885347\n",
      "SubSGD iter. 114/499: loss=11.000761648399207, w0=67.90000000000012, w1=5.593512495172105\n",
      "SubSGD iter. 115/499: loss=10.66773339949761, w0=67.20000000000012, w1=6.004587679714331\n",
      "SubSGD iter. 116/499: loss=10.639653385763655, w0=67.90000000000012, w1=7.367339075106077\n",
      "SubSGD iter. 117/499: loss=9.665896734880434, w0=68.60000000000012, w1=8.425254381933636\n",
      "SubSGD iter. 118/499: loss=8.897646254530887, w0=69.30000000000013, w1=8.949434933385353\n",
      "SubSGD iter. 119/499: loss=8.434950822746611, w0=70.00000000000013, w1=8.424802008673614\n",
      "SubSGD iter. 120/499: loss=8.532550449591849, w0=69.30000000000013, w1=7.81939885722963\n",
      "SubSGD iter. 121/499: loss=9.031920468164643, w0=70.00000000000013, w1=8.26805789601434\n",
      "SubSGD iter. 122/499: loss=8.617572027046977, w0=69.30000000000013, w1=7.71966339225653\n",
      "SubSGD iter. 123/499: loss=9.087539335256023, w0=68.60000000000012, w1=8.503896120295854\n",
      "SubSGD iter. 124/499: loss=8.856450573550516, w0=67.90000000000012, w1=8.30513525447733\n",
      "SubSGD iter. 125/499: loss=9.181772666140512, w0=68.60000000000012, w1=8.834836228539446\n",
      "SubSGD iter. 126/499: loss=8.687331499077331, w0=69.30000000000013, w1=9.258377481593461\n",
      "SubSGD iter. 127/499: loss=8.278935571282677, w0=68.60000000000012, w1=10.066491222635394\n",
      "SubSGD iter. 128/499: loss=8.088812077918506, w0=69.30000000000013, w1=9.462725152668881\n",
      "SubSGD iter. 129/499: loss=8.176663345602423, w0=70.00000000000013, w1=10.519126384047771\n",
      "SubSGD iter. 130/499: loss=7.46691466842625, w0=70.70000000000013, w1=11.699931084747668\n",
      "SubSGD iter. 131/499: loss=6.737274498906757, w0=70.00000000000013, w1=12.58876290891416\n",
      "SubSGD iter. 132/499: loss=6.593401890100731, w0=69.30000000000013, w1=11.931580205719175\n",
      "SubSGD iter. 133/499: loss=7.061202934974981, w0=68.60000000000012, w1=12.371775460426147\n",
      "SubSGD iter. 134/499: loss=7.1899929531758735, w0=69.30000000000013, w1=11.693428464451603\n",
      "SubSGD iter. 135/499: loss=7.152398587133978, w0=68.60000000000012, w1=12.877430664845495\n",
      "SubSGD iter. 136/499: loss=7.051574757427522, w0=67.90000000000012, w1=13.28850584938772\n",
      "SubSGD iter. 137/499: loss=7.288940385774248, w0=68.60000000000012, w1=13.945688552582705\n",
      "SubSGD iter. 138/499: loss=6.7941378881332355, w0=69.30000000000013, w1=13.595351619214314\n",
      "SubSGD iter. 139/499: loss=6.551559711067015, w0=68.60000000000012, w1=14.431390476487131\n",
      "SubSGD iter. 140/499: loss=6.703744982764539, w0=69.30000000000013, w1=15.487791707866021\n",
      "SubSGD iter. 141/499: loss=6.205496931108905, w0=68.60000000000012, w1=14.293052870761596\n",
      "SubSGD iter. 142/499: loss=6.728041480696037, w0=69.30000000000013, w1=14.425376397115434\n",
      "SubSGD iter. 143/499: loss=6.359757187051851, w0=70.00000000000013, w1=14.0294001538936\n",
      "SubSGD iter. 144/499: loss=6.149659768448155, w0=70.70000000000013, w1=15.08731546072116\n",
      "SubSGD iter. 145/499: loss=5.671535448895827, w0=71.40000000000013, w1=15.615371276231809\n",
      "SubSGD iter. 146/499: loss=5.446803113749359, w0=72.10000000000014, w1=15.245103185703012\n",
      "SubSGD iter. 147/499: loss=5.374817252294419, w0=71.40000000000013, w1=15.527651004789238\n",
      "SubSGD iter. 148/499: loss=5.45169055401184, w0=70.70000000000013, w1=16.15043741160953\n",
      "SubSGD iter. 149/499: loss=5.610978846039656, w0=70.00000000000013, w1=16.65545514343385\n",
      "SubSGD iter. 150/499: loss=5.905214913939561, w0=70.70000000000013, w1=17.1835109589445\n",
      "SubSGD iter. 151/499: loss=5.696795414234371, w0=71.40000000000013, w1=16.743822553859314\n",
      "SubSGD iter. 152/499: loss=5.458759552177837, w0=72.10000000000014, w1=16.06547555788477\n",
      "SubSGD iter. 153/499: loss=5.338445913229364, w0=72.80000000000014, w1=16.60643260985591\n",
      "SubSGD iter. 154/499: loss=5.341674184921235, w0=73.50000000000014, w1=16.081799685144173\n",
      "SubSGD iter. 155/499: loss=5.360028041340268, w0=74.20000000000014, w1=16.24760594471167\n",
      "SubSGD iter. 156/499: loss=5.491141621647735, w0=74.90000000000015, w1=16.68753038759585\n",
      "SubSGD iter. 157/499: loss=5.700850278816659, w0=74.20000000000014, w1=17.24565409134489\n",
      "SubSGD iter. 158/499: loss=5.606195502853136, w0=73.50000000000014, w1=16.750027830876057\n",
      "SubSGD iter. 159/499: loss=5.418057891371896, w0=74.20000000000014, w1=17.62927579677223\n",
      "SubSGD iter. 160/499: loss=5.683993691327623, w0=74.90000000000015, w1=16.83490645228145\n",
      "SubSGD iter. 161/499: loss=5.720356771499135, w0=74.20000000000014, w1=16.70258292592761\n",
      "SubSGD iter. 162/499: loss=5.532340982075695, w0=73.50000000000014, w1=17.125822350700126\n",
      "SubSGD iter. 163/499: loss=5.472860829186377, w0=74.20000000000014, w1=17.843985674731314\n",
      "SubSGD iter. 164/499: loss=5.7324510463338765, w0=74.90000000000015, w1=18.283910117615495\n",
      "SubSGD iter. 165/499: loss=6.035493568787316, w0=74.20000000000014, w1=17.378735014428432\n",
      "SubSGD iter. 166/499: loss=5.630926947893896, w0=73.50000000000014, w1=17.948695138964418\n",
      "SubSGD iter. 167/499: loss=5.630909231185022, w0=74.20000000000014, w1=16.764692938570526\n",
      "SubSGD iter. 168/499: loss=5.538485995105768, w0=74.90000000000015, w1=17.6439409044667\n",
      "SubSGD iter. 169/499: loss=5.858676295621309, w0=75.60000000000015, w1=18.769070864597005\n",
      "SubSGD iter. 170/499: loss=6.4429061593160695, w0=76.30000000000015, w1=18.38694488004667\n",
      "SubSGD iter. 171/499: loss=6.625329170242416, w0=75.60000000000015, w1=17.180928590276224\n",
      "SubSGD iter. 172/499: loss=6.031312746936071, w0=76.30000000000015, w1=16.410105524208035\n",
      "SubSGD iter. 173/499: loss=6.226937310876193, w0=77.00000000000016, w1=13.036961846618357\n",
      "SubSGD iter. 174/499: loss=6.679403733378976, w0=76.30000000000015, w1=12.90463832026452\n",
      "SubSGD iter. 175/499: loss=6.407603249445106, w0=75.60000000000015, w1=13.031652765492545\n",
      "SubSGD iter. 176/499: loss=6.131452839684689, w0=74.90000000000015, w1=13.850311135298174\n",
      "SubSGD iter. 177/499: loss=5.780336704913036, w0=74.20000000000014, w1=14.355328867122497\n",
      "SubSGD iter. 178/499: loss=5.545928875515241, w0=73.50000000000014, w1=14.020100811819242\n",
      "SubSGD iter. 179/499: loss=5.504731077396614, w0=72.80000000000014, w1=15.109895834892463\n",
      "SubSGD iter. 180/499: loss=5.351542662550371, w0=73.50000000000014, w1=14.582707196145236\n",
      "SubSGD iter. 181/499: loss=5.425265512022052, w0=72.80000000000014, w1=15.418746053418053\n",
      "SubSGD iter. 182/499: loss=5.332623211914714, w0=73.50000000000014, w1=15.69687302221974\n",
      "SubSGD iter. 183/499: loss=5.363605394492854, w0=74.20000000000014, w1=12.323729344630063\n",
      "SubSGD iter. 184/499: loss=6.08154204399861, w0=74.90000000000015, w1=11.987366386953987\n",
      "SubSGD iter. 185/499: loss=6.282424585545489, w0=74.20000000000014, w1=11.652138331650733\n",
      "SubSGD iter. 186/499: loss=6.358661673076767, w0=73.50000000000014, w1=11.423016639363974\n",
      "SubSGD iter. 187/499: loss=6.44643136296232, w0=74.20000000000014, w1=12.15498123287161\n",
      "SubSGD iter. 188/499: loss=6.1481109984801, w0=74.90000000000015, w1=12.243508746360735\n",
      "SubSGD iter. 189/499: loss=6.182465953822422, w0=74.20000000000014, w1=12.257263942336225\n",
      "SubSGD iter. 190/499: loss=6.1075296383907345, w0=73.50000000000014, w1=13.028087008404412\n",
      "SubSGD iter. 191/499: loss=5.783353261620669, w0=72.80000000000014, w1=13.571103469769511\n",
      "SubSGD iter. 192/499: loss=5.622558989597783, w0=73.50000000000014, w1=14.124309385845624\n",
      "SubSGD iter. 193/499: loss=5.486067898154278, w0=72.80000000000014, w1=14.244201199333956\n",
      "SubSGD iter. 194/499: loss=5.467819997295656, w0=72.10000000000014, w1=14.444950736383207\n",
      "SubSGD iter. 195/499: loss=5.4884162152471925, w0=72.80000000000014, w1=14.482238048001078\n",
      "SubSGD iter. 196/499: loss=5.426840361708593, w0=72.10000000000014, w1=15.318276905273894\n",
      "SubSGD iter. 197/499: loss=5.3677394247855075, w0=72.80000000000014, w1=14.464430634848625\n",
      "SubSGD iter. 198/499: loss=5.429754190285036, w0=73.50000000000014, w1=15.19639522835626\n",
      "SubSGD iter. 199/499: loss=5.380419407293247, w0=72.80000000000014, w1=16.032434085629077\n",
      "SubSGD iter. 200/499: loss=5.313127183092, w0=72.10000000000014, w1=16.559391121397926\n",
      "SubSGD iter. 201/499: loss=5.3565537120541356, w0=72.80000000000014, w1=17.174052102528314\n",
      "SubSGD iter. 202/499: loss=5.422369214623035, w0=73.50000000000014, w1=16.60104201951369\n",
      "SubSGD iter. 203/499: loss=5.401578031421478, w0=72.80000000000014, w1=17.032834597797113\n",
      "SubSGD iter. 204/499: loss=5.401094420589944, w0=72.10000000000014, w1=17.85149296760274\n",
      "SubSGD iter. 205/499: loss=5.563685990035031, w0=72.80000000000014, w1=18.37567351905446\n",
      "SubSGD iter. 206/499: loss=5.679937635317296, w0=72.10000000000014, w1=18.902630554823308\n",
      "SubSGD iter. 207/499: loss=5.875684765739019, w0=71.40000000000013, w1=19.460754258572347\n",
      "SubSGD iter. 208/499: loss=6.164186293355709, w0=72.10000000000014, w1=19.805222505442813\n",
      "SubSGD iter. 209/499: loss=6.203027337503757, w0=71.40000000000013, w1=18.61048366833839\n",
      "SubSGD iter. 210/499: loss=5.875198115965885, w0=72.10000000000014, w1=18.888610637140076\n",
      "SubSGD iter. 211/499: loss=5.871036986148106, w0=72.80000000000014, w1=17.677187749213896\n",
      "SubSGD iter. 212/499: loss=5.509217289887055, w0=73.50000000000014, w1=18.55643571511007\n",
      "SubSGD iter. 213/499: loss=5.78095890292879, w0=74.20000000000014, w1=19.288400308617703\n",
      "SubSGD iter. 214/499: loss=6.1521189602413875, w0=74.90000000000015, w1=19.264434468814613\n",
      "SubSGD iter. 215/499: loss=6.354877636159846, w0=75.60000000000015, w1=18.48020174077529\n",
      "SubSGD iter. 216/499: loss=6.350873303734378, w0=74.90000000000015, w1=19.03217317271297\n",
      "SubSGD iter. 217/499: loss=6.277152962364219, w0=74.20000000000014, w1=19.379257158332962\n",
      "SubSGD iter. 218/499: loss=6.187449019125015, w0=74.90000000000015, w1=18.979799385946382\n",
      "SubSGD iter. 219/499: loss=6.259979152870879, w0=75.60000000000015, w1=18.626683634379066\n",
      "SubSGD iter. 220/499: loss=6.397300897057801, w0=74.90000000000015, w1=19.17384593661294\n",
      "SubSGD iter. 221/499: loss=6.324331219714121, w0=75.60000000000015, w1=17.989843736219047\n",
      "SubSGD iter. 222/499: loss=6.210062980821906, w0=74.90000000000015, w1=18.024534595560517\n",
      "SubSGD iter. 223/499: loss=5.958071633344784, w0=74.20000000000014, w1=18.124811694800727\n",
      "SubSGD iter. 224/499: loss=5.798555022306173, w0=74.90000000000015, w1=17.270965424375458\n",
      "SubSGD iter. 225/499: loss=5.787556411724668, w0=74.20000000000014, w1=17.666402216318463\n",
      "SubSGD iter. 226/499: loss=5.692372663497715, w0=74.90000000000015, w1=18.309331915082257\n",
      "SubSGD iter. 227/499: loss=6.043597661362969, w0=74.20000000000014, w1=18.861303347019938\n",
      "SubSGD iter. 228/499: loss=6.00394884066049, w0=74.90000000000015, w1=18.511365415539622\n",
      "SubSGD iter. 229/499: loss=6.108002949511073, w0=74.20000000000014, w1=18.69552619385794\n",
      "SubSGD iter. 230/499: loss=5.951732405311776, w0=73.50000000000014, w1=19.08081033418607\n",
      "SubSGD iter. 231/499: loss=5.945295941351814, w0=74.20000000000014, w1=18.825378453084497\n",
      "SubSGD iter. 232/499: loss=5.992356161326535, w0=73.50000000000014, w1=17.94620614085369\n",
      "SubSGD iter. 233/499: loss=5.630389771501882, w0=74.20000000000014, w1=18.060026467668003\n",
      "SubSGD iter. 234/499: loss=5.782799886271252, w0=73.50000000000014, w1=17.314040814080606\n",
      "SubSGD iter. 235/499: loss=5.50639407515801, w0=74.20000000000014, w1=17.290074974277516\n",
      "SubSGD iter. 236/499: loss=5.61387890997492, w0=74.90000000000015, w1=17.831032026248657\n",
      "SubSGD iter. 237/499: loss=5.904594197792752, w0=75.60000000000015, w1=17.996484818181283\n",
      "SubSGD iter. 238/499: loss=6.211731987068813, w0=74.90000000000015, w1=18.73059914118227\n",
      "SubSGD iter. 239/499: loss=6.1782642761455, w0=74.20000000000014, w1=19.077683126802263\n",
      "SubSGD iter. 240/499: loss=6.078205405620881, w0=73.50000000000014, w1=19.690481297803675\n",
      "SubSGD iter. 241/499: loss=6.182666923538379, w0=72.80000000000014, w1=20.26044142233966\n",
      "SubSGD iter. 242/499: loss=6.377821449243778, w0=72.10000000000014, w1=20.20828107680995\n",
      "SubSGD iter. 243/499: loss=6.368229770683623, w0=71.40000000000013, w1=20.75544337904382\n",
      "SubSGD iter. 244/499: loss=6.696432163900174, w0=70.70000000000013, w1=20.400415901513995\n",
      "SubSGD iter. 245/499: loss=6.698451715003449, w0=70.00000000000013, w1=19.654430247926598\n",
      "SubSGD iter. 246/499: loss=6.6180718396793266, w0=70.70000000000013, w1=18.80058397750133\n",
      "SubSGD iter. 247/499: loss=6.095961173540328, w0=71.40000000000013, w1=19.029705669788086\n",
      "SubSGD iter. 248/499: loss=6.007995433152667, w0=72.10000000000014, w1=19.195158461720712\n",
      "SubSGD iter. 249/499: loss=5.973941312872869, w0=72.80000000000014, w1=19.53962670859118\n",
      "SubSGD iter. 250/499: loss=6.069844820635156, w0=72.10000000000014, w1=18.706289126031603\n",
      "SubSGD iter. 251/499: loss=5.8116117363163955, w0=72.80000000000014, w1=17.96695632658766\n",
      "SubSGD iter. 252/499: loss=5.573768272652329, w0=72.10000000000014, w1=18.08684814007599\n",
      "SubSGD iter. 253/499: loss=5.625417794249676, w0=71.40000000000013, w1=17.181673036888927\n",
      "SubSGD iter. 254/499: loss=5.506410533724742, w0=72.10000000000014, w1=17.425985760598063\n",
      "SubSGD iter. 255/499: loss=5.472724627751485, w0=72.80000000000014, w1=17.87616550680436\n",
      "SubSGD iter. 256/499: loss=5.552874808338485, w0=72.10000000000014, w1=18.221342594529357\n",
      "SubSGD iter. 257/499: loss=5.662344793671647, w0=71.40000000000013, w1=18.616779386472363\n",
      "SubSGD iter. 258/499: loss=5.877108798951604, w0=70.70000000000013, w1=18.024461425204183\n",
      "SubSGD iter. 259/499: loss=5.867883565690218, w0=70.00000000000013, w1=18.529479157028504\n",
      "SubSGD iter. 260/499: loss=6.243342245925877, w0=70.70000000000013, w1=18.54478682374064\n",
      "SubSGD iter. 261/499: loss=6.016029975163995, w0=70.00000000000013, w1=17.930872410363058\n",
      "SubSGD iter. 262/499: loss=6.086772728459903, w0=70.70000000000013, w1=17.309820836792465\n",
      "SubSGD iter. 263/499: loss=5.716707299044101, w0=71.40000000000013, w1=17.737337922009882\n",
      "SubSGD iter. 264/499: loss=5.628595331291686, w0=70.70000000000013, w1=17.402109866706628\n",
      "SubSGD iter. 265/499: loss=5.733139279119498, w0=71.40000000000013, w1=16.42531238776471\n",
      "SubSGD iter. 266/499: loss=5.442585962598965, w0=72.10000000000014, w1=16.773279434155125\n",
      "SubSGD iter. 267/499: loss=5.374234559936649, w0=71.40000000000013, w1=17.453878611811206\n",
      "SubSGD iter. 268/499: loss=5.560844927550714, w0=72.10000000000014, w1=16.242455723885026\n",
      "SubSGD iter. 269/499: loss=5.341882884759904, w0=71.40000000000013, w1=16.789618026118898\n",
      "SubSGD iter. 270/499: loss=5.4617717882687575, w0=70.70000000000013, w1=17.185054818061904\n",
      "SubSGD iter. 271/499: loss=5.697029739002599, w0=71.40000000000013, w1=17.799715799192292\n",
      "SubSGD iter. 272/499: loss=5.644226199418852, w0=70.70000000000013, w1=17.919607612680622\n",
      "SubSGD iter. 273/499: loss=5.841927068090757, w0=70.00000000000013, w1=18.428108730420455\n",
      "SubSGD iter. 274/499: loss=6.213331307055698, w0=70.70000000000013, w1=18.624453323937683\n",
      "SubSGD iter. 275/499: loss=6.0409150607389694, w0=70.00000000000013, w1=19.102501526363806\n",
      "SubSGD iter. 276/499: loss=6.418898884009562, w0=70.70000000000013, w1=18.20086926195105\n",
      "SubSGD iter. 277/499: loss=5.9149489938623026, w0=71.40000000000013, w1=18.39738934472955\n",
      "SubSGD iter. 278/499: loss=5.81052627237179, w0=72.10000000000014, w1=17.967057756268815\n",
      "SubSGD iter. 279/499: loss=5.593455374706132, w0=72.80000000000014, w1=18.685221080300003\n",
      "SubSGD iter. 280/499: loss=5.770640617929279, w0=73.50000000000014, w1=18.838114837414587\n",
      "SubSGD iter. 281/499: loss=5.864188988716864, w0=74.20000000000014, w1=19.30469696338116\n",
      "SubSGD iter. 282/499: loss=6.158334604037314, w0=73.50000000000014, w1=18.75630245962335\n",
      "SubSGD iter. 283/499: loss=5.839023700856362, w0=74.20000000000014, w1=17.77950498068143\n",
      "SubSGD iter. 284/499: loss=5.7178985538620575, w0=74.90000000000015, w1=18.792183322034198\n",
      "SubSGD iter. 285/499: loss=6.198458252083655, w0=75.60000000000015, w1=18.41659748374763\n",
      "SubSGD iter. 286/499: loss=6.3310493657233184, w0=74.90000000000015, w1=18.763681469367622\n",
      "SubSGD iter. 287/499: loss=6.189112252526957, w0=74.20000000000014, w1=19.114018402736015\n",
      "SubSGD iter. 288/499: loss=6.09080805268021, w0=74.90000000000015, w1=20.29482310343591\n",
      "SubSGD iter. 289/499: loss=6.749893837213385, w0=74.20000000000014, w1=20.09238621339491\n",
      "SubSGD iter. 290/499: loss=6.479700265448945, w0=74.90000000000015, w1=19.05487201958137\n",
      "SubSGD iter. 291/499: loss=6.2845961070345435, w0=74.20000000000014, w1=19.073451478480248\n",
      "SubSGD iter. 292/499: loss=6.076737686746837, w0=73.50000000000014, w1=18.42139458103779\n",
      "SubSGD iter. 293/499: loss=5.743267032978355, w0=72.80000000000014, w1=17.33751918434649\n",
      "SubSGD iter. 294/499: loss=5.447876740431591, w0=73.50000000000014, w1=18.34995752221291\n",
      "SubSGD iter. 295/499: loss=5.723783791725913, w0=74.20000000000014, w1=17.373160043270993\n",
      "SubSGD iter. 296/499: loss=5.629787935531318, w0=74.90000000000015, w1=16.942828454810257\n",
      "SubSGD iter. 297/499: loss=5.735963115201853, w0=74.20000000000014, w1=16.101638286585615\n",
      "SubSGD iter. 298/499: loss=5.481612211362765, w0=73.50000000000014, w1=15.221908156753138\n",
      "SubSGD iter. 299/499: loss=5.37924149110227, w0=72.80000000000014, w1=15.846724256564084\n",
      "SubSGD iter. 300/499: loss=5.312771145509959, w0=72.10000000000014, w1=16.41668438110007\n",
      "SubSGD iter. 301/499: loss=5.3492022923152795, w0=71.40000000000013, w1=15.46977211935764\n",
      "SubSGD iter. 302/499: loss=5.456165455035287, w0=70.70000000000013, w1=15.134544064054385\n",
      "SubSGD iter. 303/499: loss=5.663905458874765, w0=71.40000000000013, w1=15.345708458620589\n",
      "SubSGD iter. 304/499: loss=5.466293916271698, w0=72.10000000000014, w1=15.327128999721712\n",
      "SubSGD iter. 305/499: loss=5.367038022513617, w0=72.80000000000014, w1=16.33980734107448\n",
      "SubSGD iter. 306/499: loss=5.318799404423463, w0=72.10000000000014, w1=17.02040651873056\n",
      "SubSGD iter. 307/499: loss=5.4053171359601935, w0=71.40000000000013, w1=16.401273122785895\n",
      "SubSGD iter. 308/499: loss=5.44165908057956, w0=72.10000000000014, w1=15.970941534325158\n",
      "SubSGD iter. 309/499: loss=5.338534903035549, w0=72.80000000000014, w1=16.58732848316241\n",
      "SubSGD iter. 310/499: loss=5.339590961567471, w0=73.50000000000014, w1=17.305491807193597\n",
      "SubSGD iter. 311/499: loss=5.504870972984045, w0=74.20000000000014, w1=16.278559609601064\n",
      "SubSGD iter. 312/499: loss=5.493412700073134, w0=74.90000000000015, w1=16.023127728499492\n",
      "SubSGD iter. 313/499: loss=5.640046544026854, w0=75.60000000000015, w1=16.576333644575605\n",
      "SubSGD iter. 314/499: loss=5.9391975720023575, w0=74.90000000000015, w1=15.830347990988209\n",
      "SubSGD iter. 315/499: loss=5.628873567511959, w0=75.60000000000015, w1=16.296930116954783\n",
      "SubSGD iter. 316/499: loss=5.9078226491542924, w0=74.90000000000015, w1=16.70800530149701\n",
      "SubSGD iter. 317/499: loss=5.703444617311831, w0=74.20000000000014, w1=17.119080486039238\n",
      "SubSGD iter. 318/499: loss=5.5851938718084355, w0=73.50000000000014, w1=16.472933252652016\n",
      "SubSGD iter. 319/499: loss=5.387470102355797, w0=74.20000000000014, w1=16.751060221453702\n",
      "SubSGD iter. 320/499: loss=5.537137207353947, w0=73.50000000000014, w1=16.490348000071837\n",
      "SubSGD iter. 321/499: loss=5.3893878947701195, w0=72.80000000000014, w1=16.91358742484435\n",
      "SubSGD iter. 322/499: loss=5.38334582809617, w0=72.10000000000014, w1=16.07239725661971\n",
      "SubSGD iter. 323/499: loss=5.338499673101973, w0=71.40000000000013, w1=16.222853933070837\n",
      "SubSGD iter. 324/499: loss=5.435888754684718, w0=70.70000000000013, w1=17.062881504530647\n",
      "SubSGD iter. 325/499: loss=5.678486445087931, w0=70.00000000000013, w1=17.50307675923762\n",
      "SubSGD iter. 326/499: loss=6.0056293255885285, w0=70.70000000000013, w1=17.747389482946755\n",
      "SubSGD iter. 327/499: loss=5.80241600416867, w0=71.40000000000013, w1=17.949826372987754\n",
      "SubSGD iter. 328/499: loss=5.683798659963322, w0=70.70000000000013, w1=18.36090155752998\n",
      "SubSGD iter. 329/499: loss=5.959950031598127, w0=70.00000000000013, w1=18.640830549966445\n",
      "SubSGD iter. 330/499: loss=6.276608950077503, w0=70.70000000000013, w1=18.056518012420554\n",
      "SubSGD iter. 331/499: loss=5.876190286118504, w0=70.00000000000013, w1=18.46759319696278\n",
      "SubSGD iter. 332/499: loss=6.224853551080423, w0=70.70000000000013, w1=18.26684365991353\n",
      "SubSGD iter. 333/499: loss=5.933264956491654, w0=70.00000000000013, w1=17.738787844402882\n",
      "SubSGD iter. 334/499: loss=6.045721844538048, w0=70.70000000000013, w1=18.463808034483687\n",
      "SubSGD iter. 335/499: loss=5.990734975440991, w0=70.00000000000013, w1=17.10105663909194\n",
      "SubSGD iter. 336/499: loss=5.9482072818379725, w0=70.70000000000013, w1=17.127429289716762\n",
      "SubSGD iter. 337/499: loss=5.688283417255186, w0=71.40000000000013, w1=16.225797025304004\n",
      "SubSGD iter. 338/499: loss=5.435970537813892, w0=70.70000000000013, w1=16.66548543038919\n",
      "SubSGD iter. 339/499: loss=5.630285963022149, w0=71.40000000000013, w1=16.092475347374567\n",
      "SubSGD iter. 340/499: loss=5.43418839421267, w0=70.70000000000013, w1=15.435292644179581\n",
      "SubSGD iter. 341/499: loss=5.635234000714275, w0=70.00000000000013, w1=14.829889492735596\n",
      "SubSGD iter. 342/499: loss=5.967434120152754, w0=69.30000000000013, w1=15.638003233777528\n",
      "SubSGD iter. 343/499: loss=6.198599757055027, w0=70.00000000000013, w1=16.72187863046883\n",
      "SubSGD iter. 344/499: loss=5.911292009200555, w0=70.70000000000013, w1=16.521129093419578\n",
      "SubSGD iter. 345/499: loss=5.617844140497796, w0=71.40000000000013, w1=15.97811263205448\n",
      "SubSGD iter. 346/499: loss=5.435674812072652, w0=70.70000000000013, w1=15.971684751176133\n",
      "SubSGD iter. 347/499: loss=5.613290604395818, w0=71.40000000000013, w1=15.936993891834662\n",
      "SubSGD iter. 348/499: loss=5.436388521209126, w0=70.70000000000013, w1=16.36878647011808\n",
      "SubSGD iter. 349/499: loss=5.612129725794148, w0=71.40000000000013, w1=15.18478426972419\n",
      "SubSGD iter. 350/499: loss=5.4826172695688395, w0=72.10000000000014, w1=15.532751316114604\n",
      "SubSGD iter. 351/499: loss=5.351972366898637, w0=71.40000000000013, w1=15.653255867593646\n",
      "SubSGD iter. 352/499: loss=5.4450613672762564, w0=72.10000000000014, w1=15.86442026215985\n",
      "SubSGD iter. 353/499: loss=5.3410459278651885, w0=72.80000000000014, w1=16.5275645792029\n",
      "SubSGD iter. 354/499: loss=5.333073963306412, w0=72.10000000000014, w1=16.812739073752194\n",
      "SubSGD iter. 355/499: loss=5.378682033972003, w0=72.80000000000014, w1=16.780310065984835\n",
      "SubSGD iter. 356/499: loss=5.36423295495181, w0=72.10000000000014, w1=16.647986539630995\n",
      "SubSGD iter. 357/499: loss=5.3623364546293795, w0=71.40000000000013, w1=17.466644909436624\n",
      "SubSGD iter. 358/499: loss=5.56361206855319, w0=72.10000000000014, w1=17.69576660172338\n",
      "SubSGD iter. 359/499: loss=5.527572872213687, w0=71.40000000000013, w1=17.65344004511989\n",
      "SubSGD iter. 360/499: loss=5.607571905855427, w0=72.10000000000014, w1=17.99715266630328\n",
      "SubSGD iter. 361/499: loss=5.601485285873252, w0=71.40000000000013, w1=17.163815083743703\n",
      "SubSGD iter. 362/499: loss=5.5031633221035126, w0=72.10000000000014, w1=18.04298739597451\n",
      "SubSGD iter. 363/499: loss=5.613714889058888, w0=71.40000000000013, w1=17.514931580463863\n",
      "SubSGD iter. 364/499: loss=5.574153003936206, w0=72.10000000000014, w1=17.144663489935066\n",
      "SubSGD iter. 365/499: loss=5.42323516753007, w0=71.40000000000013, w1=17.570449637225806\n",
      "SubSGD iter. 366/499: loss=5.587268974608953, w0=72.10000000000014, w1=18.695579597356115\n",
      "SubSGD iter. 367/499: loss=5.808154218182976, w0=72.80000000000014, w1=18.721952247980937\n",
      "SubSGD iter. 368/499: loss=5.7818187498789895, w0=73.50000000000014, w1=19.262909299952078\n",
      "SubSGD iter. 369/499: loss=6.010985015941786, w0=72.80000000000014, w1=18.528633556460278\n",
      "SubSGD iter. 370/499: loss=5.723689843304101, w0=73.50000000000014, w1=17.687215046734334\n",
      "SubSGD iter. 371/499: loss=5.57709223252569, w0=72.80000000000014, w1=17.807106860222664\n",
      "SubSGD iter. 372/499: loss=5.536982514697334, w0=73.50000000000014, w1=18.33128741167438\n",
      "SubSGD iter. 373/499: loss=5.718980880302457, w0=72.80000000000014, w1=17.497949829114805\n",
      "SubSGD iter. 374/499: loss=5.47542675021297, w0=73.50000000000014, w1=16.286526941188626\n",
      "SubSGD iter. 375/499: loss=5.370892580003443, w0=72.80000000000014, w1=16.436983617639754\n",
      "SubSGD iter. 376/499: loss=5.323401834491749, w0=72.10000000000014, w1=16.557488169118795\n",
      "SubSGD iter. 377/499: loss=5.356453880758731, w0=72.80000000000014, w1=15.818155369674852\n",
      "SubSGD iter. 378/499: loss=5.3129890212480575, w0=72.10000000000014, w1=15.938659921153894\n",
      "SubSGD iter. 379/499: loss=5.339295877163685, w0=72.80000000000014, w1=15.602296963477817\n",
      "SubSGD iter. 380/499: loss=5.32259218293181, w0=72.10000000000014, w1=16.2828961411339\n",
      "SubSGD iter. 381/499: loss=5.343349970664509, w0=71.40000000000013, w1=16.363710446622612\n",
      "SubSGD iter. 382/499: loss=5.440210776350328, w0=72.10000000000014, w1=16.25353632974187\n",
      "SubSGD iter. 383/499: loss=5.342264561449231, w0=72.80000000000014, w1=16.720118455708445\n",
      "SubSGD iter. 384/499: loss=5.355833361227693, w0=73.50000000000014, w1=15.866272185283176\n",
      "SubSGD iter. 385/499: loss=5.36171060838936, w0=74.20000000000014, w1=12.493128507693498\n",
      "SubSGD iter. 386/499: loss=6.015914594370419, w0=74.90000000000015, w1=13.106121004877059\n",
      "SubSGD iter. 387/499: loss=5.92398574516945, w0=74.20000000000014, w1=13.386049997313522\n",
      "SubSGD iter. 388/499: loss=5.723962103384687, w0=73.50000000000014, w1=14.28768226172628\n",
      "SubSGD iter. 389/499: loss=5.460953664017946, w0=72.80000000000014, w1=14.245355705122789\n",
      "SubSGD iter. 390/499: loss=5.467613810032426, w0=72.10000000000014, w1=14.792518007356659\n",
      "SubSGD iter. 391/499: loss=5.435330135897773, w0=72.80000000000014, w1=15.322218981418775\n",
      "SubSGD iter. 392/499: loss=5.337898401542181, w0=73.50000000000014, w1=15.945707103704953\n",
      "SubSGD iter. 393/499: loss=5.3608221023493305, w0=74.20000000000014, w1=16.395886849911246\n",
      "SubSGD iter. 394/499: loss=5.502174478786242, w0=73.50000000000014, w1=16.23008059034375\n",
      "SubSGD iter. 395/499: loss=5.3670779409926945, w0=72.80000000000014, w1=16.78820429409279\n",
      "SubSGD iter. 396/499: loss=5.365334575403031, w0=73.50000000000014, w1=16.38874652170621\n",
      "SubSGD iter. 397/499: loss=5.378760147954944, w0=72.80000000000014, w1=15.774832108328628\n",
      "SubSGD iter. 398/499: loss=5.314421287448487, w0=73.50000000000014, w1=15.790139775040764\n",
      "SubSGD iter. 399/499: loss=5.362562174774602, w0=72.80000000000014, w1=15.940596451491892\n",
      "SubSGD iter. 400/499: loss=5.312909978053523, w0=73.50000000000014, w1=16.481553503463033\n",
      "SubSGD iter. 401/499: loss=5.388419404186963, w0=72.80000000000014, w1=15.747277759971233\n",
      "SubSGD iter. 402/499: loss=5.315641286849757, w0=72.10000000000014, w1=16.404158727724198\n",
      "SubSGD iter. 403/499: loss=5.348610779624305, w0=72.80000000000014, w1=17.237496310283774\n",
      "SubSGD iter. 404/499: loss=5.432269096291723, w0=73.50000000000014, w1=17.218916851384897\n",
      "SubSGD iter. 405/499: loss=5.4894466663779085, w0=74.20000000000014, w1=17.831909348568455\n",
      "SubSGD iter. 406/499: loss=5.729725569380476, w0=73.50000000000014, w1=17.699585822214615\n",
      "SubSGD iter. 407/499: loss=5.579560982078343, w0=72.80000000000014, w1=18.251557254152296\n",
      "SubSGD iter. 408/499: loss=5.6460855212167305, w0=73.50000000000014, w1=17.554972502911532\n",
      "SubSGD iter. 409/499: loss=5.551158687077471, w0=72.80000000000014, w1=16.94956935146755\n",
      "SubSGD iter. 410/499: loss=5.388701330763129, w0=72.10000000000014, w1=16.35725139019937\n",
      "SubSGD iter. 411/499: loss=5.346395622888515, w0=72.80000000000014, w1=16.55968828024037\n",
      "SubSGD iter. 412/499: loss=5.336576915589144, w0=72.10000000000014, w1=16.57344347621586\n",
      "SubSGD iter. 413/499: loss=5.3572981380095195, w0=71.40000000000013, w1=15.210692080824114\n",
      "SubSGD iter. 414/499: loss=5.47989411245078, w0=72.10000000000014, w1=15.827079029661363\n",
      "SubSGD iter. 415/499: loss=5.341926172346568, w0=72.80000000000014, w1=15.396747441200626\n",
      "SubSGD iter. 416/499: loss=5.333825432686404, w0=73.50000000000014, w1=16.01066185457821\n",
      "SubSGD iter. 417/499: loss=5.360095561809903, w0=72.80000000000014, w1=15.811900988759685\n",
      "SubSGD iter. 418/499: loss=5.313097498735551, w0=72.10000000000014, w1=15.207368882905525\n",
      "SubSGD iter. 419/499: loss=5.378791953026319, w0=71.40000000000013, w1=15.685417085331649\n",
      "SubSGD iter. 420/499: loss=5.443600769649071, w0=72.10000000000014, w1=16.3018040341689\n",
      "SubSGD iter. 421/499: loss=5.344048980650221, w0=71.40000000000013, w1=16.545830858878052\n",
      "SubSGD iter. 422/499: loss=5.447884705484319, w0=72.10000000000014, w1=17.15882335606161\n",
      "SubSGD iter. 423/499: loss=5.425507623379354, w0=72.80000000000014, w1=16.419490556617667\n",
      "SubSGD iter. 424/499: loss=5.322326351740926, w0=72.10000000000014, w1=16.845276703908407\n",
      "SubSGD iter. 425/499: loss=5.382768644591082, w0=71.40000000000013, w1=17.230560844236535\n",
      "SubSGD iter. 426/499: loss=5.5153000774963346, w0=70.70000000000013, w1=16.02454455446609\n",
      "SubSGD iter. 427/499: loss=5.612595531902733, w0=71.40000000000013, w1=16.29002825669032\n",
      "SubSGD iter. 428/499: loss=5.437755405795344, w0=70.70000000000013, w1=16.569957249126784\n",
      "SubSGD iter. 429/499: loss=5.621796940662669, w0=70.00000000000013, w1=16.595255014515626\n",
      "SubSGD iter. 430/499: loss=5.89970719469008, w0=69.30000000000013, w1=16.099628754046794\n",
      "SubSGD iter. 431/499: loss=6.190324191129616, w0=68.60000000000012, w1=16.635148436873\n",
      "SubSGD iter. 432/499: loss=6.581002991332483, w0=69.30000000000013, w1=15.81649006706737\n",
      "SubSGD iter. 433/499: loss=6.1926759468454495, w0=70.00000000000013, w1=16.649827649626946\n",
      "SubSGD iter. 434/499: loss=5.904700053647341, w0=70.70000000000013, w1=16.815633909194442\n",
      "SubSGD iter. 435/499: loss=5.645173700514412, w0=71.40000000000013, w1=17.012153991972944\n",
      "SubSGD iter. 436/499: loss=5.481341060542533, w0=70.70000000000013, w1=17.42322917651517\n",
      "SubSGD iter. 437/499: loss=5.737062598162365, w0=70.00000000000013, w1=16.330604662189376\n",
      "SubSGD iter. 438/499: loss=5.876310288549128, w0=70.70000000000013, w1=16.79718678815595\n",
      "SubSGD iter. 439/499: loss=5.643119303623925, w0=70.00000000000013, w1=16.442159310626124\n",
      "SubSGD iter. 440/499: loss=5.885700444696731, w0=70.70000000000013, w1=16.09222137914581\n",
      "SubSGD iter. 441/499: loss=5.611705625086466, w0=71.40000000000013, w1=16.258027638713305\n",
      "SubSGD iter. 442/499: loss=5.4368661673799945, w0=72.10000000000014, w1=16.46919203327951\n",
      "SubSGD iter. 443/499: loss=5.351821753239402, w0=72.80000000000014, w1=16.557719546768634\n",
      "SubSGD iter. 444/499: loss=5.336362233619564, w0=72.10000000000014, w1=16.983505694059374\n",
      "SubSGD iter. 445/499: loss=5.400406053999847, w0=71.40000000000013, w1=16.32632299086439\n",
      "SubSGD iter. 446/499: loss=5.4387692280948325, w0=70.70000000000013, w1=14.963571595472644\n",
      "SubSGD iter. 447/499: loss=5.694978340861119, w0=70.00000000000013, w1=15.490528631241492\n",
      "SubSGD iter. 448/499: loss=5.874434633905839, w0=70.70000000000013, w1=15.940708377447786\n",
      "SubSGD iter. 449/499: loss=5.613697923832783, w0=71.40000000000013, w1=16.389367416232496\n",
      "SubSGD iter. 450/499: loss=5.441200032212549, w0=70.70000000000013, w1=17.229394987692306\n",
      "SubSGD iter. 451/499: loss=5.703803278856803, w0=70.00000000000013, w1=16.572212284497322\n",
      "SubSGD iter. 452/499: loss=5.897599011701723, w0=70.70000000000013, w1=17.451460250393495\n",
      "SubSGD iter. 453/499: loss=5.742307064518106, w0=71.40000000000013, w1=17.72958721919518\n",
      "SubSGD iter. 454/499: loss=5.626653132886392, w0=70.70000000000013, w1=16.771452128293376\n",
      "SubSGD iter. 455/499: loss=5.640322420487274, w0=70.00000000000013, w1=17.39423853511367\n",
      "SubSGD iter. 456/499: loss=5.988676634513773, w0=70.70000000000013, w1=17.02831780438327\n",
      "SubSGD iter. 457/499: loss=5.673240415230134, w0=71.40000000000013, w1=17.181211561497854\n",
      "SubSGD iter. 458/499: loss=5.506326621070216, w0=72.10000000000014, w1=18.037433614002392\n",
      "SubSGD iter. 459/499: loss=5.612233031301992, w0=72.80000000000014, w1=17.060636135060474\n",
      "SubSGD iter. 460/499: loss=5.4052323648562055, w0=73.50000000000014, w1=16.32130333561653\n",
      "SubSGD iter. 461/499: loss=5.373242765424682, w0=74.20000000000014, w1=14.967346441583302\n",
      "SubSGD iter. 462/499: loss=5.485859667080678, w0=74.90000000000015, w1=15.211659165292437\n",
      "SubSGD iter. 463/499: loss=5.628206650795933, w0=74.20000000000014, w1=16.006028509783217\n",
      "SubSGD iter. 464/499: loss=5.476372658029478, w0=73.50000000000014, w1=16.740142832784205\n",
      "SubSGD iter. 465/499: loss=5.416946127344891, w0=74.20000000000014, w1=16.344166589562374\n",
      "SubSGD iter. 466/499: loss=5.498226301217876, w0=73.50000000000014, w1=16.211843063208534\n",
      "SubSGD iter. 467/499: loss=5.366013584325557, w0=72.80000000000014, w1=16.292657368697245\n",
      "SubSGD iter. 468/499: loss=5.31765169594871, w0=72.10000000000014, w1=16.443114045148373\n",
      "SubSGD iter. 469/499: loss=5.350453668834317, w0=71.40000000000013, w1=15.71809385506757\n",
      "SubSGD iter. 470/499: loss=5.442349429395479, w0=72.10000000000014, w1=15.352173124337172\n",
      "SubSGD iter. 471/499: loss=5.36505363271679, w0=72.80000000000014, w1=15.111193209670525\n",
      "SubSGD iter. 472/499: loss=5.351442021051775, w0=72.10000000000014, w1=14.480027869675672\n",
      "SubSGD iter. 473/499: loss=5.482930804480454, w0=72.80000000000014, w1=15.660832570375568\n",
      "SubSGD iter. 474/499: loss=5.319479017896925, w0=72.10000000000014, w1=16.01116950374396\n",
      "SubSGD iter. 475/499: loss=5.3380241256502865, w0=71.40000000000013, w1=15.746384394553193\n",
      "SubSGD iter. 476/499: loss=5.44126605759526, w0=72.10000000000014, w1=16.758822732419613\n",
      "SubSGD iter. 477/499: loss=5.372698039701647, w0=72.80000000000014, w1=16.1550566624531\n",
      "SubSGD iter. 478/499: loss=5.314706154803757, w0=72.10000000000014, w1=15.208144400710669\n",
      "SubSGD iter. 479/499: loss=5.3787102647225975, w0=71.40000000000013, w1=15.45217122541982\n",
      "SubSGD iter. 480/499: loss=5.457526266628473, w0=72.10000000000014, w1=14.765476589913684\n",
      "SubSGD iter. 481/499: loss=5.439449272454406, w0=72.80000000000014, w1=15.015075052600862\n",
      "SubSGD iter. 482/499: loss=5.359470170360584, w0=73.50000000000014, w1=16.14020501273117\n",
      "SubSGD iter. 483/499: loss=5.362564412129469, w0=72.80000000000014, w1=16.420134005167633\n",
      "SubSGD iter. 484/499: loss=5.322363925094422, w0=72.10000000000014, w1=15.327509490841837\n",
      "SubSGD iter. 485/499: loss=5.3670078740173865, w0=72.80000000000014, w1=15.577107953529016\n",
      "SubSGD iter. 486/499: loss=5.32396875844846, w0=73.50000000000014, w1=15.842591655753246\n",
      "SubSGD iter. 487/499: loss=5.361975483005417, w0=74.20000000000014, w1=15.164244659778703\n",
      "SubSGD iter. 488/499: loss=5.477279159461389, w0=73.50000000000014, w1=14.206109568876895\n",
      "SubSGD iter. 489/499: loss=5.473019592584481, w0=72.80000000000014, w1=13.259197307134466\n",
      "SubSGD iter. 490/499: loss=5.717614360208616, w0=73.50000000000014, w1=14.343072703825769\n",
      "SubSGD iter. 491/499: loss=5.452915908278664, w0=72.80000000000014, w1=13.685890000630783\n",
      "SubSGD iter. 492/499: loss=5.59045488216688, w0=73.50000000000014, w1=14.591065103817847\n",
      "SubSGD iter. 493/499: loss=5.424444529763919, w0=72.80000000000014, w1=13.99874714254967\n",
      "SubSGD iter. 494/499: loss=5.515973977428892, w0=72.10000000000014, w1=14.10892125943041\n",
      "SubSGD iter. 495/499: loss=5.56283284375259, w0=71.40000000000013, w1=14.678881383966395\n",
      "SubSGD iter. 496/499: loss=5.552859328595756, w0=72.10000000000014, w1=14.928479846653573\n",
      "SubSGD iter. 497/499: loss=5.415898309248151, w0=72.80000000000014, w1=15.437665690599825\n",
      "SubSGD iter. 498/499: loss=5.33158925665481, w0=73.50000000000014, w1=16.232835415717172\n",
      "SubSGD iter. 499/499: loss=5.367264111828895, w0=74.20000000000014, w1=16.47714813942631\n",
      "SubSGD: execution time=0.016 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1555ad91c04b45c7b99cc9fb6b8b75c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses,\n",
    "        subsgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
