{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "height, weight, gender = load_data(sub_sample=False, add_outlier=False)\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000,), (10000, 2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NB: throughout this laboratory the data has the following format: \n",
    "  * there are **N = 10000** data entries\n",
    "  * **y** represents the column vector containing weight information -- that which we wish to predict/the output (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,)**.\n",
    "  * **tx** represents the matrix $\\tilde{X}$ formed by laterally concatenating a column vector of 1s to the column vector of height information -- the input data (see also the first page of $\\texttt{exercise02.pdf}$). Its **shape** is **(N,2)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Computing the Cost Function\n",
    "Fill in the `compute_loss` function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(y, tx, w):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2,). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        the value of the loss (a scalar), corresponding to the input parameters w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss by MSE\n",
    "    # ***************************************************\n",
    "    #return (0.5*(1/y.shape[0])*np.matmul((y - np.matmul(tx,w)).T,(y - np.matmul(tx,w)))).item() # MSE\n",
    "    return (1/y.shape[0])*np.sum(np.abs(y - np.matmul(tx,w))) #MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the function `grid_search()` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from costs import *\n",
    "\n",
    "\n",
    "def grid_search(y, tx, grid_w0, grid_w1):\n",
    "    \"\"\"Algorithm for grid search.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        grid_w0: numpy array of shape=(num_grid_pts_w0, ). A 1D array containing num_grid_pts_w0 values of parameter w0 to be tested in the grid search.\n",
    "        grid_w1: numpy array of shape=(num_grid_pts_w1, ). A 1D array containing num_grid_pts_w1 values of parameter w1 to be tested in the grid search.\n",
    "\n",
    "    Returns:\n",
    "        losses: numpy array of shape=(num_grid_pts_w0, num_grid_pts_w1). A 2D array containing the loss value for each combination of w0 and w1\n",
    "    \"\"\"\n",
    "\n",
    "    losses = np.zeros((len(grid_w0), len(grid_w1)))\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute loss for each combination of w0 and w1.\n",
    "    # ***************************************************\n",
    "    for i in range(grid_w0.shape[0]):\n",
    "        for j in range(grid_w1.shape[0]):\n",
    "            losses[i,j] = compute_loss(y,tx,np.array([grid_w0[i],grid_w1[j]]))\n",
    "            \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us play with the grid search demo now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search: loss*=7.956595728172087, w0*=66.66666666666669, w1*=16.666666666666686, execution time=0.001 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from grid_search import generate_w, get_best_parameters\n",
    "from plots import grid_visualization\n",
    "\n",
    "# Generate the grid of parameters to be swept\n",
    "grid_w0, grid_w1 = generate_w(num_intervals=10)\n",
    "\n",
    "# Start the grid search\n",
    "start_time = datetime.datetime.now()\n",
    "grid_losses = grid_search(y, tx, grid_w0, grid_w1)\n",
    "\n",
    "# Select the best combinaison\n",
    "loss_star, w0_star, w1_star = get_best_parameters(grid_w0, grid_w1, grid_losses)\n",
    "end_time = datetime.datetime.now()\n",
    "execution_time = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Print the results\n",
    "print(\n",
    "    \"Grid Search: loss*={l}, w0*={w0}, w1*={w1}, execution time={t:.3f} seconds\".format(\n",
    "        l=loss_star, w0=w0_star, w1=w1_star, t=execution_time\n",
    "    )\n",
    ")\n",
    "\n",
    "# Plot the results\n",
    "fig = grid_visualization(grid_losses, grid_w0, grid_w1, mean_x, std_x, height, weight)\n",
    "fig.set_size_inches(10.0, 6.0)\n",
    "fig.savefig(\"grid_plot\")  # Optional saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, please fill in the functions `compute_gradient` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An numpy array of shape (2, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute gradient vector\n",
    "    # ***************************************************\n",
    "    return -(1/y.shape[0])*np.matmul(tx.T,(y - np.matmul(tx,w)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please fill in the functions `gradient_descent` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of GD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of GD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute gradient and loss\n",
    "        # ***************************************************\n",
    "        grad = compute_gradient(y,tx,w)\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by gradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma*grad\n",
    "\n",
    "        # store w and loss\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"GD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your gradient descent function through gradient descent demo shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2792.236712759165, w0=51.305745401473324, w1=9.435798704492441\n",
      "GD iter. 1/49: loss=265.3024621089666, w0=66.69746902191562, w1=12.266538315840048\n",
      "GD iter. 2/49: loss=37.878379550441835, w0=71.31498610804832, w1=13.115760199244335\n",
      "GD iter. 3/49: loss=17.41021212017456, w0=72.70024123388814, w1=13.370526764265632\n",
      "GD iter. 4/49: loss=15.568077051450487, w0=73.11581777164008, w1=13.446956733772023\n",
      "GD iter. 5/49: loss=15.402284895265316, w0=73.24049073296567, w1=13.469885724623941\n",
      "GD iter. 6/49: loss=15.387363601208621, w0=73.27789262136332, w1=13.476764421879516\n",
      "GD iter. 7/49: loss=15.386020684743551, w0=73.28911318788263, w1=13.478828031056189\n",
      "GD iter. 8/49: loss=15.385899822261655, w0=73.29247935783842, w1=13.47944711380919\n",
      "GD iter. 9/49: loss=15.385888944638314, w0=73.29348920882516, w1=13.47963283863509\n",
      "GD iter. 10/49: loss=15.385887965652197, w0=73.29379216412119, w1=13.479688556082861\n",
      "GD iter. 11/49: loss=15.385887877543448, w0=73.29388305071, w1=13.479705271317192\n",
      "GD iter. 12/49: loss=15.3858878696137, w0=73.29391031668663, w1=13.479710285887492\n",
      "GD iter. 13/49: loss=15.38588786890002, w0=73.29391849647962, w1=13.479711790258582\n",
      "GD iter. 14/49: loss=15.385887868835784, w0=73.29392095041752, w1=13.479712241569908\n",
      "GD iter. 15/49: loss=15.385887868829968, w0=73.29392168659889, w1=13.479712376963306\n",
      "GD iter. 16/49: loss=15.385887868829489, w0=73.2939219074533, w1=13.479712417581325\n",
      "GD iter. 17/49: loss=15.385887868829426, w0=73.29392197370963, w1=13.479712429766732\n",
      "GD iter. 18/49: loss=15.385887868829375, w0=73.29392199358652, w1=13.479712433422353\n",
      "GD iter. 19/49: loss=15.385887868829363, w0=73.29392199954958, w1=13.47971243451904\n",
      "GD iter. 20/49: loss=15.385887868829448, w0=73.2939220013385, w1=13.479712434848047\n",
      "GD iter. 21/49: loss=15.385887868829402, w0=73.29392200187519, w1=13.479712434946748\n",
      "GD iter. 22/49: loss=15.385887868829375, w0=73.29392200203618, w1=13.479712434976358\n",
      "GD iter. 23/49: loss=15.385887868829393, w0=73.29392200208449, w1=13.479712434985242\n",
      "GD iter. 24/49: loss=15.385887868829375, w0=73.29392200209898, w1=13.479712434987906\n",
      "GD iter. 25/49: loss=15.385887868829402, w0=73.29392200210333, w1=13.479712434988706\n",
      "GD iter. 26/49: loss=15.385887868829398, w0=73.29392200210462, w1=13.479712434988945\n",
      "GD iter. 27/49: loss=15.385887868829402, w0=73.29392200210502, w1=13.479712434989018\n",
      "GD iter. 28/49: loss=15.38588786882937, w0=73.29392200210513, w1=13.47971243498904\n",
      "GD iter. 29/49: loss=15.385887868829403, w0=73.29392200210518, w1=13.479712434989047\n",
      "GD iter. 30/49: loss=15.38588786882941, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 31/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 32/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 33/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 34/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 35/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 36/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 37/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 38/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 39/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 40/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 41/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 42/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 43/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 44/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 45/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 46/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 47/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 48/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD iter. 49/49: loss=15.385887868829403, w0=73.29392200210519, w1=13.479712434989048\n",
      "GD: execution time=0.059 seconds\n"
     ]
    }
   ],
   "source": [
    "# from gradient_descent import *\n",
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7539803fafb461babfb692920a1f615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stoch_gradient(y, tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the stochastic gradient of the loss at w.\n",
    "    \"\"\"\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: implement stochastic gradient computation. It's the same as the usual gradient.\n",
    "    # ***************************************************\n",
    "    return -(1/y.shape[0])*np.matmul(tx.T,(y - np.matmul(tx,w)))\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD).\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic gradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic gradient descent.\n",
    "        # ***************************************************\n",
    "        for minibatch_y,minibatch_tx in batch_iter(y,tx,batch_size):\n",
    "            grad = compute_stoch_gradient(minibatch_y,minibatch_tx,w)\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        w = w - gamma*grad\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "\n",
    "        print(\n",
    "            \"SGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD iter. 0/49: loss=2792.236712759165, w0=8.699023511775723, w1=5.336111004894752\n",
      "SGD iter. 1/49: loss=2134.7954654829355, w0=15.749902522167769, w1=11.120993602407683\n",
      "SGD iter. 2/49: loss=1673.8247540881223, w0=20.707572864893713, w1=8.58853921173764\n",
      "SGD iter. 3/49: loss=1410.0097334091065, w0=26.970345940802922, w1=9.169867241393051\n",
      "SGD iter. 4/49: loss=1097.6101202188406, w0=31.780160409398057, w1=15.689480947454284\n",
      "SGD iter. 5/49: loss=879.5236270962347, w0=36.56711211965362, w1=10.58966210095257\n",
      "SGD iter. 6/49: loss=693.991365406332, w0=40.278641027802834, w1=11.580141379614524\n",
      "SGD iter. 7/49: loss=562.1944618721042, w0=42.89399783720994, w1=12.486156153978651\n",
      "SGD iter. 8/49: loss=477.95715952628836, w0=46.12258508898759, w1=9.804132454555699\n",
      "SGD iter. 9/49: loss=391.2816067881834, w0=49.465027345637004, w1=18.772774827154727\n",
      "SGD iter. 10/49: loss=313.3022528870373, w0=51.00507210265236, w1=21.77427136168872\n",
      "SGD iter. 11/49: loss=298.18215668324467, w0=52.81368165348978, w1=21.115921021258156\n",
      "SGD iter. 12/49: loss=254.26185102386177, w0=55.83119697600449, w1=17.37806561568256\n",
      "SGD iter. 13/49: loss=175.45784929814292, w0=58.1805184246801, w1=13.297600846475385\n",
      "SGD iter. 14/49: loss=129.60995403122718, w0=59.9498325039201, w1=11.488531103946245\n",
      "SGD iter. 15/49: loss=106.40065168316266, w0=62.80864018940632, w1=10.922307555391377\n",
      "SGD iter. 16/49: loss=73.62661507378117, w0=63.87735484159505, w1=12.060965606998597\n",
      "SGD iter. 17/49: loss=60.72817769399473, w0=65.155302372178, w1=11.39881036127381\n",
      "SGD iter. 18/49: loss=50.66952932934358, w0=66.02996533766995, w1=12.239571642645727\n",
      "SGD iter. 19/49: loss=42.537395672642916, w0=66.75602273062624, w1=12.0917113629802\n",
      "SGD iter. 20/49: loss=37.72122479878072, w0=67.49544040389156, w1=13.368869165358975\n",
      "SGD iter. 21/49: loss=32.20322540645164, w0=68.77914612704545, w1=11.881489516455623\n",
      "SGD iter. 22/49: loss=26.85464671850292, w0=68.66781755611039, w1=11.91440619385645\n",
      "SGD iter. 23/49: loss=27.31140085572023, w0=68.7938181298821, w1=12.105328397124964\n",
      "SGD iter. 24/49: loss=26.455821040995875, w0=69.57114929894931, w1=12.439961760028678\n",
      "SGD iter. 25/49: loss=22.85594690155088, w0=69.73105133208837, w1=12.36163177475941\n",
      "SGD iter. 26/49: loss=22.357963755852246, w0=71.11421767708615, w1=13.7665983158446\n",
      "SGD iter. 27/49: loss=17.802595095399894, w0=71.39928006934183, w1=13.343679967588402\n",
      "SGD iter. 28/49: loss=17.189974311615593, w0=71.4034489208806, w1=13.346301135069206\n",
      "SGD iter. 29/49: loss=17.181731391719897, w0=70.89021515010113, w1=14.227460528331942\n",
      "SGD iter. 30/49: loss=18.554354789564016, w0=71.19354902097182, w1=13.831950997039392\n",
      "SGD iter. 31/49: loss=17.653707201064616, w0=71.32128839439825, w1=13.92498058783298\n",
      "SGD iter. 32/49: loss=17.43066140792535, w0=71.52645447173342, w1=13.614856315543413\n",
      "SGD iter. 33/49: loss=16.956990538514333, w0=72.36478227553765, w1=13.308973477342082\n",
      "SGD iter. 34/49: loss=15.832114080401592, w0=72.94901534061005, w1=12.82538406183568\n",
      "SGD iter. 35/49: loss=15.659440981358046, w0=72.96867539592085, w1=12.80535733849757\n",
      "SGD iter. 36/49: loss=15.666157944328617, w0=72.42386072714467, w1=13.173621701825196\n",
      "SGD iter. 37/49: loss=15.811236948386746, w0=72.12449343872366, w1=13.27415317455247\n",
      "SGD iter. 38/49: loss=16.090796756031313, w0=72.78747630546992, w1=13.767361001330832\n",
      "SGD iter. 39/49: loss=15.555502339508818, w0=73.25948784612775, w1=13.085505414562675\n",
      "SGD iter. 40/49: loss=15.464180311855012, w0=72.7675663213683, w1=13.95372435724507\n",
      "SGD iter. 41/49: loss=15.636756671371836, w0=72.50605100727134, w1=13.755611469192296\n",
      "SGD iter. 42/49: loss=15.734318359616784, w0=72.31826711020561, w1=13.59215104276764\n",
      "SGD iter. 43/49: loss=15.868160323132761, w0=72.45806642575971, w1=13.715769970337991\n",
      "SGD iter. 44/49: loss=15.763076721080827, w0=72.69578267354754, w1=13.966711530562378\n",
      "SGD iter. 45/49: loss=15.6833572565577, w0=73.22445089383815, w1=13.490739966397888\n",
      "SGD iter. 46/49: loss=15.388361789495834, w0=72.50736676420692, w1=11.925130378161983\n",
      "SGD iter. 47/49: loss=16.903585125666357, w0=73.14163741360366, w1=13.13081515592401\n",
      "SGD iter. 48/49: loss=15.458347822446465, w0=73.72506359948873, w1=13.69639921028562\n",
      "SGD iter. 49/49: loss=15.502305986620815, w0=72.81427492534252, w1=14.3064657383906\n",
      "SGD: execution time=0.059 seconds\n"
     ]
    }
   ],
   "source": [
    "# from stochastic_gradient_descent import *\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.1\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SGD.\n",
    "start_time = datetime.datetime.now()\n",
    "sgd_losses, sgd_ws = stochastic_gradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7607e70b6264500b97730329d792a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        sgd_losses,\n",
    "        sgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(sgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Effect of Outliers and MAE Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "# ***************************************************\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=False)\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200,), (200, 2))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2829.272224438416, w0=51.54259072181176, w1=10.132993413506084\n",
      "GD iter. 1/49: loss=267.0500258779429, w0=67.0053679383553, w1=13.172891437557826\n",
      "GD iter. 2/49: loss=36.45002800750046, w0=71.64420110331838, w1=14.084860844773324\n",
      "GD iter. 3/49: loss=15.69602819916064, w0=73.03585105280729, w1=14.358451666937965\n",
      "GD iter. 4/49: loss=13.82816821641008, w0=73.45334603765397, w1=14.440528913587356\n",
      "GD iter. 5/49: loss=13.660060817962526, w0=73.57859453310797, w1=14.46515208758217\n",
      "GD iter. 6/49: loss=13.64493115210224, w0=73.61616908174418, w1=14.472539039780616\n",
      "GD iter. 7/49: loss=13.643569482174815, w0=73.62744144633503, w1=14.474755125440149\n",
      "GD iter. 8/49: loss=13.64344693188135, w0=73.63082315571229, w1=14.47541995113801\n",
      "GD iter. 9/49: loss=13.643435902354938, w0=73.63183766852546, w1=14.475619398847368\n",
      "GD iter. 10/49: loss=13.643434909697556, w0=73.63214202236942, w1=14.475679233160175\n",
      "GD iter. 11/49: loss=13.643434820358403, w0=73.6322333285226, w1=14.475697183454017\n",
      "GD iter. 12/49: loss=13.643434812317876, w0=73.63226072036856, w1=14.47570256854217\n",
      "GD iter. 13/49: loss=13.643434811594227, w0=73.63226893792235, w1=14.475704184068615\n",
      "GD iter. 14/49: loss=13.643434811529096, w0=73.63227140318848, w1=14.475704668726548\n",
      "GD iter. 15/49: loss=13.643434811523235, w0=73.63227214276833, w1=14.47570481412393\n",
      "GD iter. 16/49: loss=13.64343481152271, w0=73.63227236464228, w1=14.475704857743143\n",
      "GD iter. 17/49: loss=13.643434811522654, w0=73.63227243120447, w1=14.475704870828908\n",
      "GD iter. 18/49: loss=13.643434811522654, w0=73.63227245117312, w1=14.475704874754637\n",
      "GD iter. 19/49: loss=13.643434811522654, w0=73.63227245716372, w1=14.475704875932355\n",
      "GD iter. 20/49: loss=13.643434811522656, w0=73.6322724589609, w1=14.47570487628567\n",
      "GD iter. 21/49: loss=13.643434811522653, w0=73.63227245950004, w1=14.475704876391665\n",
      "GD iter. 22/49: loss=13.643434811522656, w0=73.63227245966179, w1=14.475704876423464\n",
      "GD iter. 23/49: loss=13.643434811522662, w0=73.63227245971032, w1=14.475704876433003\n",
      "GD iter. 24/49: loss=13.643434811522662, w0=73.63227245972487, w1=14.475704876435866\n",
      "GD iter. 25/49: loss=13.643434811522656, w0=73.63227245972924, w1=14.475704876436724\n",
      "GD iter. 26/49: loss=13.643434811522653, w0=73.63227245973054, w1=14.475704876436982\n",
      "GD iter. 27/49: loss=13.643434811522656, w0=73.63227245973094, w1=14.475704876437058\n",
      "GD iter. 28/49: loss=13.643434811522653, w0=73.63227245973106, w1=14.475704876437081\n",
      "GD iter. 29/49: loss=13.643434811522651, w0=73.6322724597311, w1=14.475704876437089\n",
      "GD iter. 30/49: loss=13.643434811522653, w0=73.63227245973111, w1=14.47570487643709\n",
      "GD iter. 31/49: loss=13.64343481152266, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 32/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 33/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 34/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 35/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 36/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 37/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 38/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 39/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 40/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 41/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 42/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 43/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 44/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 45/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 46/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 47/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 48/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD iter. 49/49: loss=13.643434811522656, w0=73.63227245973111, w1=14.475704876437092\n",
      "GD: execution time=0.002 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points\n",
    "#       and the model fit\n",
    "# ***************************************************\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ad0f9b54e734b8181bafb94296d0a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from helpers import *\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: reload the data by subsampling first, then by subsampling and adding outliers\n",
    "height, weight, gender = load_data(sub_sample=True, add_outlier=True)\n",
    "# ***************************************************\n",
    "\n",
    "x, mean_x, std_x = standardize(height)\n",
    "y, tx = build_model_data(x, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((202,), (202, 2))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape, tx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD iter. 0/49: loss=2869.8351145358524, w0=51.84746409844842, w1=7.7244264061924195\n",
      "GD iter. 1/49: loss=318.28212470159644, w0=67.40170332798297, w1=10.041754328050114\n",
      "GD iter. 2/49: loss=88.64235561651283, w0=72.06797509684336, w1=10.736952704607411\n",
      "GD iter. 3/49: loss=67.97477639885523, w0=73.46785662750146, w1=10.945512217574597\n",
      "GD iter. 4/49: loss=66.11469426926604, w0=73.88782108669889, w1=11.00808007146475\n",
      "GD iter. 5/49: loss=65.94728687760302, w0=74.01381042445813, w1=11.026850427631798\n",
      "GD iter. 6/49: loss=65.93222021235334, w0=74.05160722578589, w1=11.032481534481914\n",
      "GD iter. 7/49: loss=65.93086421248087, w0=74.06294626618423, w1=11.034170866536943\n",
      "GD iter. 8/49: loss=65.93074217249234, w0=74.06634797830372, w1=11.034677666153454\n",
      "GD iter. 9/49: loss=65.93073118889339, w0=74.06736849193958, w1=11.034829706038408\n",
      "GD iter. 10/49: loss=65.93073020036948, w0=74.06767464603033, w1=11.034875318003895\n",
      "GD iter. 11/49: loss=65.93073011140233, w0=74.06776649225755, w1=11.034889001593541\n",
      "GD iter. 12/49: loss=65.93073010339529, w0=74.06779404612573, w1=11.034893106670433\n",
      "GD iter. 13/49: loss=65.93073010267464, w0=74.06780231228618, w1=11.034894338193501\n",
      "GD iter. 14/49: loss=65.93073010260979, w0=74.06780479213431, w1=11.034894707650421\n",
      "GD iter. 15/49: loss=65.93073010260395, w0=74.06780553608874, w1=11.034894818487496\n",
      "GD iter. 16/49: loss=65.93073010260343, w0=74.06780575927507, w1=11.03489485173862\n",
      "GD iter. 17/49: loss=65.93073010260339, w0=74.06780582623098, w1=11.034894861713955\n",
      "GD iter. 18/49: loss=65.93073010260338, w0=74.06780584631775, w1=11.034894864706557\n",
      "GD iter. 19/49: loss=65.93073010260338, w0=74.06780585234378, w1=11.034894865604338\n",
      "GD iter. 20/49: loss=65.93073010260339, w0=74.06780585415159, w1=11.034894865873675\n",
      "GD iter. 21/49: loss=65.93073010260338, w0=74.06780585469393, w1=11.034894865954474\n",
      "GD iter. 22/49: loss=65.93073010260338, w0=74.06780585485663, w1=11.034894865978712\n",
      "GD iter. 23/49: loss=65.93073010260338, w0=74.06780585490544, w1=11.034894865985985\n",
      "GD iter. 24/49: loss=65.93073010260338, w0=74.06780585492008, w1=11.034894865988164\n",
      "GD iter. 25/49: loss=65.93073010260338, w0=74.06780585492449, w1=11.03489486598882\n",
      "GD iter. 26/49: loss=65.93073010260339, w0=74.06780585492581, w1=11.034894865989015\n",
      "GD iter. 27/49: loss=65.93073010260338, w0=74.06780585492619, w1=11.034894865989076\n",
      "GD iter. 28/49: loss=65.93073010260338, w0=74.0678058549263, w1=11.034894865989099\n",
      "GD iter. 29/49: loss=65.93073010260339, w0=74.06780585492635, w1=11.0348948659891\n",
      "GD iter. 30/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 31/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 32/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 33/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 34/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 35/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 36/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 37/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 38/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 39/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 40/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 41/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 42/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 43/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 44/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 45/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 46/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 47/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 48/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD iter. 49/49: loss=65.93073010260338, w0=74.06780585492636, w1=11.0348948659891\n",
      "GD: execution time=0.001 seconds\n"
     ]
    }
   ],
   "source": [
    "from plots import gradient_descent_visualization\n",
    "\n",
    "# Define the parameters of the algorithm.\n",
    "max_iters = 50\n",
    "gamma = 0.7\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start gradient descent.\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# ***************************************************\n",
    "# INSERT YOUR CODE HERE\n",
    "# TODO: fit the model to the subsampled data / subsampled data with outliers and visualize the cloud of points\n",
    "#       and the model fit\n",
    "gd_losses, gd_ws = gradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "# ***************************************************\n",
    "\n",
    "\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"GD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3198a4c0673e4f3a8f7ff246a6ef84b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=51, min=1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time Visualization\n",
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        gd_losses,\n",
    "        gd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(gd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 6. Subgradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_subgradient_mae(y, tx, w):\n",
    "    \"\"\"Compute a subgradient of the MAE at w.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        w: numpy array of shape=(2, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        A numpy array of shape (2, ) (same shape as w), containing the subgradient of the MAE at w.\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO: compute subgradient gradient vector for MAE\n",
    "    # ***************************************************\n",
    "    r = np.random.randint(-1,2) #Return a random no. from [-1,1]\n",
    "    del_h = y - np.matmul(tx,w)\n",
    "    del_h[del_h < 0] = -1\n",
    "    del_h[del_h == 0] = r\n",
    "    del_h[del_h > 0] = 1\n",
    "    return -(1/y.shape[0])*np.matmul(tx.T,del_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subgradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The SubGradient Descent (SubGD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(N, )\n",
    "        tx: numpy array of shape=(N,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubGD\n",
    "    \"\"\"\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: compute subgradient and loss\n",
    "        # ***************************************************\n",
    "        subgrad = compute_subgradient_mae(y, tx, w)\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: update w by subgradient\n",
    "        # ***************************************************\n",
    "        w = w - gamma*subgrad\n",
    "\n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "        print(\n",
    "            \"SubGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=6.109524327590712e-16\n",
      "SubGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=1.2219048655181425e-15\n",
      "SubGD iter. 2/499: loss=72.66780585492637, w0=2.0999999999999996, w1=1.832857298277214e-15\n",
      "SubGD iter. 3/499: loss=71.96780585492638, w0=2.8, w1=2.443809731036285e-15\n",
      "SubGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=3.054762163795356e-15\n",
      "SubGD iter. 5/499: loss=70.56780585492639, w0=4.2, w1=3.665714596554428e-15\n",
      "SubGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=4.276667029313499e-15\n",
      "SubGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=4.887619462072571e-15\n",
      "SubGD iter. 8/499: loss=68.46780585492637, w0=6.300000000000001, w1=5.498571894831642e-15\n",
      "SubGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=6.109524327590714e-15\n",
      "SubGD iter. 10/499: loss=67.06780585492639, w0=7.700000000000001, w1=6.720476760349785e-15\n",
      "SubGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=7.331429193108857e-15\n",
      "SubGD iter. 12/499: loss=65.66780585492639, w0=9.1, w1=7.942381625867928e-15\n",
      "SubGD iter. 13/499: loss=64.96780585492638, w0=9.799999999999999, w1=8.553334058627e-15\n",
      "SubGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=9.164286491386072e-15\n",
      "SubGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=9.775238924145143e-15\n",
      "SubGD iter. 16/499: loss=62.867805854926374, w0=11.899999999999997, w1=1.0386191356904215e-14\n",
      "SubGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=1.0997143789663286e-14\n",
      "SubGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=1.1608096222422358e-14\n",
      "SubGD iter. 19/499: loss=60.76780585492638, w0=13.999999999999995, w1=1.2219048655181429e-14\n",
      "SubGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=1.28300010879405e-14\n",
      "SubGD iter. 21/499: loss=59.36780585492639, w0=15.399999999999993, w1=1.3440953520699572e-14\n",
      "SubGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=1.4051905953458644e-14\n",
      "SubGD iter. 23/499: loss=57.96780585492638, w0=16.799999999999994, w1=1.4662858386217714e-14\n",
      "SubGD iter. 24/499: loss=57.26780585492638, w0=17.499999999999993, w1=1.5273810818976784e-14\n",
      "SubGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=1.5884763251735854e-14\n",
      "SubGD iter. 26/499: loss=55.867805854926395, w0=18.89999999999999, w1=1.6495715684494924e-14\n",
      "SubGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=1.7106668117253994e-14\n",
      "SubGD iter. 28/499: loss=54.46780585492638, w0=20.29999999999999, w1=1.7717620550013064e-14\n",
      "SubGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=1.8328572982772134e-14\n",
      "SubGD iter. 30/499: loss=53.06780585492639, w0=21.69999999999999, w1=1.8939525415531204e-14\n",
      "SubGD iter. 31/499: loss=52.367805854926395, w0=22.399999999999988, w1=1.9550477848290273e-14\n",
      "SubGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=2.0161430281049343e-14\n",
      "SubGD iter. 33/499: loss=50.96780585492639, w0=23.799999999999986, w1=2.0772382713808413e-14\n",
      "SubGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=2.1383335146567483e-14\n",
      "SubGD iter. 35/499: loss=49.56780585492639, w0=25.199999999999985, w1=2.1994287579326553e-14\n",
      "SubGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=2.2605240012085623e-14\n",
      "SubGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=2.3216192444844693e-14\n",
      "SubGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=2.3827144877603763e-14\n",
      "SubGD iter. 39/499: loss=46.7678058549264, w0=27.999999999999982, w1=2.4438097310362833e-14\n",
      "SubGD iter. 40/499: loss=46.067805854926405, w0=28.69999999999998, w1=2.5049049743121903e-14\n",
      "SubGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=2.5660002175880973e-14\n",
      "SubGD iter. 42/499: loss=44.6678058549264, w0=30.09999999999998, w1=2.6270954608640043e-14\n",
      "SubGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=2.6881907041399113e-14\n",
      "SubGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=2.7492859474158183e-14\n",
      "SubGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=2.8103811906917253e-14\n",
      "SubGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=2.871476433967632e-14\n",
      "SubGD iter. 47/499: loss=41.167805854926385, w0=33.59999999999999, w1=2.9325716772435396e-14\n",
      "SubGD iter. 48/499: loss=40.46780585492639, w0=34.29999999999999, w1=2.993666920519447e-14\n",
      "SubGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=3.054762163795354e-14\n",
      "SubGD iter. 50/499: loss=39.067805854926384, w0=35.699999999999996, w1=3.1158574070712615e-14\n",
      "SubGD iter. 51/499: loss=38.36780585492638, w0=36.4, w1=3.176952650347169e-14\n",
      "SubGD iter. 52/499: loss=37.66780585492638, w0=37.1, w1=3.238047893623076e-14\n",
      "SubGD iter. 53/499: loss=36.96780585492638, w0=37.800000000000004, w1=3.2991431368989835e-14\n",
      "SubGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=3.360238380174891e-14\n",
      "SubGD iter. 55/499: loss=35.56780585492637, w0=39.20000000000001, w1=3.421333623450798e-14\n",
      "SubGD iter. 56/499: loss=34.86780585492637, w0=39.90000000000001, w1=3.4824288667267054e-14\n",
      "SubGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=3.543524110002613e-14\n",
      "SubGD iter. 58/499: loss=33.46780585492636, w0=41.30000000000002, w1=3.60461935327852e-14\n",
      "SubGD iter. 59/499: loss=32.767805854926365, w0=42.00000000000002, w1=3.6657145965544273e-14\n",
      "SubGD iter. 60/499: loss=32.067805854926355, w0=42.700000000000024, w1=3.7268098398303347e-14\n",
      "SubGD iter. 61/499: loss=31.36780585492636, w0=43.40000000000003, w1=3.787905083106242e-14\n",
      "SubGD iter. 62/499: loss=30.667805854926353, w0=44.10000000000003, w1=3.849000326382149e-14\n",
      "SubGD iter. 63/499: loss=29.96780585492635, w0=44.80000000000003, w1=3.9100955696580566e-14\n",
      "SubGD iter. 64/499: loss=29.267805854926348, w0=45.500000000000036, w1=3.971190812933964e-14\n",
      "SubGD iter. 65/499: loss=28.567805854926345, w0=46.20000000000004, w1=4.032286056209871e-14\n",
      "SubGD iter. 66/499: loss=27.867805854926342, w0=46.90000000000004, w1=4.0933812994857785e-14\n",
      "SubGD iter. 67/499: loss=27.17327020966892, w0=47.59306930693074, w1=0.011147845678271063\n",
      "SubGD iter. 68/499: loss=26.490451563751197, w0=48.279207920792125, w1=0.03308574108989941\n",
      "SubGD iter. 69/499: loss=25.817212322770175, w0=48.96534653465351, w1=0.05502363650152776\n",
      "SubGD iter. 70/499: loss=25.15503943465645, w0=49.63069306930698, w1=0.10538326388307814\n",
      "SubGD iter. 71/499: loss=24.524103413894778, w0=50.28910891089114, w1=0.16746568532793435\n",
      "SubGD iter. 72/499: loss=23.899295346035593, w0=50.947524752475296, w1=0.22954810677279056\n",
      "SubGD iter. 73/499: loss=23.284392925657148, w0=51.59207920792084, w1=0.31242512932747524\n",
      "SubGD iter. 74/499: loss=22.686876444181845, w0=52.22277227722777, w1=0.4119501328839991\n",
      "SubGD iter. 75/499: loss=22.10626756964055, w0=52.84653465346539, w1=0.5208167847923756\n",
      "SubGD iter. 76/499: loss=21.537818828008433, w0=53.4564356435644, w1=0.6457900912635992\n",
      "SubGD iter. 77/499: loss=20.986339874628463, w0=54.0594059405941, w1=0.7796904498577214\n",
      "SubGD iter. 78/499: loss=20.445560936620446, w0=54.655445544554496, w1=0.9197570104995693\n",
      "SubGD iter. 79/499: loss=19.91191015895785, w0=55.24455445544559, w1=1.0670920297849913\n",
      "SubGD iter. 80/499: loss=19.389644090563234, w0=55.819801980198065, w1=1.2261255948210765\n",
      "SubGD iter. 81/499: loss=18.887989064395885, w0=56.36732673267331, w1=1.410709342622213\n",
      "SubGD iter. 82/499: loss=18.415960501854236, w0=56.900990099009945, w1=1.605853732220269\n",
      "SubGD iter. 83/499: loss=17.954898543040386, w0=57.42772277227727, w1=1.808762802293962\n",
      "SubGD iter. 84/499: loss=17.505757656579824, w0=57.933663366336674, w1=2.0285064197514697\n",
      "SubGD iter. 85/499: loss=17.07495742693161, w0=58.43267326732677, w1=2.2494370848672776\n",
      "SubGD iter. 86/499: loss=16.652967297509903, w0=58.91089108910895, w1=2.4837982986028337\n",
      "SubGD iter. 87/499: loss=16.24854073149673, w0=59.382178217821824, w1=2.7260245553531504\n",
      "SubGD iter. 88/499: loss=15.849105212654159, w0=59.83960396039608, w1=2.978742333469136\n",
      "SubGD iter. 89/499: loss=15.46691979123133, w0=60.262376237623805, w1=3.251528669355438\n",
      "SubGD iter. 90/499: loss=15.108294621512217, w0=60.67821782178222, w1=3.5270865794242794\n",
      "SubGD iter. 91/499: loss=14.754896345922832, w0=61.087128712871326, w1=3.806459183951815\n",
      "SubGD iter. 92/499: loss=14.40452896162028, w0=61.49603960396043, w1=4.085831788479351\n",
      "SubGD iter. 93/499: loss=14.055787028127279, w0=61.891089108910926, w1=4.373839384328607\n",
      "SubGD iter. 94/499: loss=13.714620911605637, w0=62.27920792079211, w1=4.666037469532047\n",
      "SubGD iter. 95/499: loss=13.381236307284155, w0=62.65346534653469, w1=4.959829093241769\n",
      "SubGD iter. 96/499: loss=13.058821615166238, w0=63.02079207920796, w1=5.25705719205664\n",
      "SubGD iter. 97/499: loss=12.74025172433924, w0=63.38118811881192, w1=5.560434316352406\n",
      "SubGD iter. 98/499: loss=12.423218888756113, w0=63.74158415841588, w1=5.863811440648173\n",
      "SubGD iter. 99/499: loss=12.107561731901173, w0=64.08811881188123, w1=6.172402175278548\n",
      "SubGD iter. 100/499: loss=11.800622097398135, w0=64.42772277227726, w1=6.486369310516498\n",
      "SubGD iter. 101/499: loss=11.49504179464643, w0=64.7673267326733, w1=6.800336445754448\n",
      "SubGD iter. 102/499: loss=11.189461491894715, w0=65.10693069306933, w1=7.114303580992399\n",
      "SubGD iter. 103/499: loss=10.883881189143004, w0=65.44653465346536, w1=7.428270716230349\n",
      "SubGD iter. 104/499: loss=10.584593408313204, w0=65.76534653465349, w1=7.747893210218626\n",
      "SubGD iter. 105/499: loss=10.295816534318941, w0=66.070297029703, w1=8.073669686866905\n",
      "SubGD iter. 106/499: loss=10.01135208122136, w0=66.37524752475251, w1=8.399446163515185\n",
      "SubGD iter. 107/499: loss=9.728084326668132, w0=66.6663366336634, w1=8.73297028041739\n",
      "SubGD iter. 108/499: loss=9.44812546112251, w0=66.9574257425743, w1=9.066494397319596\n",
      "SubGD iter. 109/499: loss=9.17104110409667, w0=67.23465346534658, w1=9.39863031947029\n",
      "SubGD iter. 110/499: loss=8.903656131158963, w0=67.51188118811886, w1=9.730766241620982\n",
      "SubGD iter. 111/499: loss=8.636271158221255, w0=67.78910891089114, w1=10.062902163771675\n",
      "SubGD iter. 112/499: loss=8.376151920302375, w0=68.06633663366343, w1=10.363999289979422\n",
      "SubGD iter. 113/499: loss=8.140540838751498, w0=68.32970297029709, w1=10.660466909273612\n",
      "SubGD iter. 114/499: loss=7.918544501597273, w0=68.59306930693076, w1=10.943174379960814\n",
      "SubGD iter. 115/499: loss=7.7052797283770005, w0=68.85643564356442, w1=11.225881850648015\n",
      "SubGD iter. 116/499: loss=7.493695831178641, w0=69.11287128712878, w1=11.504395843582206\n",
      "SubGD iter. 117/499: loss=7.289992405743416, w0=69.35544554455453, w1=11.78820189306775\n",
      "SubGD iter. 118/499: loss=7.097234035781543, w0=69.58415841584166, w1=12.060911465190971\n",
      "SubGD iter. 119/499: loss=6.919905294668923, w0=69.80594059405948, w1=12.324245668386048\n",
      "SubGD iter. 120/499: loss=6.750573527315454, w0=70.0277227722773, w1=12.587579871581125\n",
      "SubGD iter. 121/499: loss=6.584744810805664, w0=70.25643564356443, w1=12.824765405096484\n",
      "SubGD iter. 122/499: loss=6.430343276347806, w0=70.47821782178225, w1=13.065616959310148\n",
      "SubGD iter. 123/499: loss=6.278071481890353, w0=70.69306930693077, w1=13.302953389983912\n",
      "SubGD iter. 124/499: loss=6.133663329263324, w0=70.89405940594067, w1=13.525403099312918\n",
      "SubGD iter. 125/499: loss=6.00584079834303, w0=71.08811881188126, w1=13.742945617944212\n",
      "SubGD iter. 126/499: loss=5.885021825223219, w0=71.27524752475254, w1=13.953548196006844\n",
      "SubGD iter. 127/499: loss=5.771635252269659, w0=71.46237623762383, w1=14.164150774069476\n",
      "SubGD iter. 128/499: loss=5.667162061790258, w0=71.62178217821788, w1=14.349779559473173\n",
      "SubGD iter. 129/499: loss=5.586726765993147, w0=71.75346534653471, w1=14.51689010761231\n",
      "SubGD iter. 130/499: loss=5.523847812160388, w0=71.87128712871292, w1=14.670791185324186\n",
      "SubGD iter. 131/499: loss=5.480093708591872, w0=71.95445544554461, w1=14.780276456654521\n",
      "SubGD iter. 132/499: loss=5.4530880035020255, w0=72.0376237623763, w1=14.889761727984856\n",
      "SubGD iter. 133/499: loss=5.427392630862905, w0=72.10693069306937, w1=14.985916181776727\n",
      "SubGD iter. 134/499: loss=5.407322445682752, w0=72.17623762376245, w1=15.082070635568597\n",
      "SubGD iter. 135/499: loss=5.387252260502599, w0=72.24554455445552, w1=15.178225089360467\n",
      "SubGD iter. 136/499: loss=5.3704607803386955, w0=72.30099009900998, w1=15.25972348971591\n",
      "SubGD iter. 137/499: loss=5.357406523334741, w0=72.34950495049513, w1=15.335091856448138\n",
      "SubGD iter. 138/499: loss=5.345929264022583, w0=72.39801980198028, w1=15.410460223180365\n",
      "SubGD iter. 139/499: loss=5.335714659517474, w0=72.43267326732682, w1=15.469961786755725\n",
      "SubGD iter. 140/499: loss=5.330043910465361, w0=72.46039603960405, w1=15.51864528583281\n",
      "SubGD iter. 141/499: loss=5.325676428273225, w0=72.48811881188128, w1=15.561592159086487\n",
      "SubGD iter. 142/499: loss=5.322176726526591, w0=72.5019801980199, w1=15.597828332032526\n",
      "SubGD iter. 143/499: loss=5.320111309643112, w0=72.52277227722782, w1=15.624722856626713\n",
      "SubGD iter. 144/499: loss=5.318478284898438, w0=72.55049504950505, w1=15.642690329098\n",
      "SubGD iter. 145/499: loss=5.3172400485651465, w0=72.56435643564366, w1=15.664356578291091\n",
      "SubGD iter. 146/499: loss=5.316406547951546, w0=72.58514851485158, w1=15.677095775361284\n",
      "SubGD iter. 147/499: loss=5.315557122666144, w0=72.6059405940595, w1=15.689834972431477\n",
      "SubGD iter. 148/499: loss=5.314707697380741, w0=72.62673267326743, w1=15.70257416950167\n",
      "SubGD iter. 149/499: loss=5.313876880922167, w0=72.64059405940604, w1=15.72424041869476\n",
      "SubGD iter. 150/499: loss=5.313052246871384, w0=72.66138613861396, w1=15.736979615764954\n",
      "SubGD iter. 151/499: loss=5.312377839024388, w0=72.66831683168327, w1=15.74811029423128\n",
      "SubGD iter. 152/499: loss=5.312132229725043, w0=72.67524752475258, w1=15.759240972697606\n",
      "SubGD iter. 153/499: loss=5.311886620425697, w0=72.68217821782189, w1=15.770371651163932\n",
      "SubGD iter. 154/499: loss=5.311683566098434, w0=72.68217821782189, w1=15.774323911906686\n",
      "SubGD iter. 155/499: loss=5.311661251291322, w0=72.68217821782189, w1=15.77827617264944\n",
      "SubGD iter. 156/499: loss=5.31163893648421, w0=72.68217821782189, w1=15.782228433392193\n",
      "SubGD iter. 157/499: loss=5.311616621677096, w0=72.68217821782189, w1=15.786180694134947\n",
      "SubGD iter. 158/499: loss=5.311594306869984, w0=72.68217821782189, w1=15.7901329548777\n",
      "SubGD iter. 159/499: loss=5.311571992062871, w0=72.68217821782189, w1=15.794085215620454\n",
      "SubGD iter. 160/499: loss=5.31154967725576, w0=72.68217821782189, w1=15.798037476363207\n",
      "SubGD iter. 161/499: loss=5.311527362448647, w0=72.68217821782189, w1=15.801989737105961\n",
      "SubGD iter. 162/499: loss=5.3115050476415355, w0=72.68217821782189, w1=15.805941997848715\n",
      "SubGD iter. 163/499: loss=5.311482732834422, w0=72.68217821782189, w1=15.809894258591468\n",
      "SubGD iter. 164/499: loss=5.311460418027309, w0=72.68217821782189, w1=15.813846519334222\n",
      "SubGD iter. 165/499: loss=5.311438103220198, w0=72.68217821782189, w1=15.817798780076975\n",
      "SubGD iter. 166/499: loss=5.311415788413085, w0=72.68217821782189, w1=15.821751040819729\n",
      "SubGD iter. 167/499: loss=5.311393473605974, w0=72.68217821782189, w1=15.825703301562482\n",
      "SubGD iter. 168/499: loss=5.31137115879886, w0=72.68217821782189, w1=15.829655562305236\n",
      "SubGD iter. 169/499: loss=5.3113488439917464, w0=72.68217821782189, w1=15.83360782304799\n",
      "SubGD iter. 170/499: loss=5.311326529184636, w0=72.68217821782189, w1=15.837560083790743\n",
      "SubGD iter. 171/499: loss=5.311304214377523, w0=72.68217821782189, w1=15.841512344533497\n",
      "SubGD iter. 172/499: loss=5.311281899570409, w0=72.68217821782189, w1=15.84546460527625\n",
      "SubGD iter. 173/499: loss=5.311259584763298, w0=72.68217821782189, w1=15.849416866019004\n",
      "SubGD iter. 174/499: loss=5.311237269956186, w0=72.68217821782189, w1=15.853369126761757\n",
      "SubGD iter. 175/499: loss=5.311214955149073, w0=72.68217821782189, w1=15.857321387504511\n",
      "SubGD iter. 176/499: loss=5.31119264034196, w0=72.68217821782189, w1=15.861273648247264\n",
      "SubGD iter. 177/499: loss=5.311170325534848, w0=72.68217821782189, w1=15.865225908990018\n",
      "SubGD iter. 178/499: loss=5.311148010727736, w0=72.68217821782189, w1=15.869178169732772\n",
      "SubGD iter. 179/499: loss=5.311125695920623, w0=72.68217821782189, w1=15.873130430475525\n",
      "SubGD iter. 180/499: loss=5.31110338111351, w0=72.68217821782189, w1=15.877082691218279\n",
      "SubGD iter. 181/499: loss=5.311081066306398, w0=72.68217821782189, w1=15.881034951961032\n",
      "SubGD iter. 182/499: loss=5.311058751499285, w0=72.68217821782189, w1=15.884987212703786\n",
      "SubGD iter. 183/499: loss=5.3110364366921745, w0=72.68217821782189, w1=15.88893947344654\n",
      "SubGD iter. 184/499: loss=5.311014121885061, w0=72.68217821782189, w1=15.892891734189293\n",
      "SubGD iter. 185/499: loss=5.310991807077948, w0=72.68217821782189, w1=15.896843994932047\n",
      "SubGD iter. 186/499: loss=5.310969492270837, w0=72.68217821782189, w1=15.9007962556748\n",
      "SubGD iter. 187/499: loss=5.310947177463723, w0=72.68217821782189, w1=15.904748516417554\n",
      "SubGD iter. 188/499: loss=5.31092486265661, w0=72.68217821782189, w1=15.908700777160307\n",
      "SubGD iter. 189/499: loss=5.310902547849499, w0=72.68217821782189, w1=15.91265303790306\n",
      "SubGD iter. 190/499: loss=5.310913706061381, w0=72.67524752475258, w1=15.910526938117323\n",
      "SubGD iter. 191/499: loss=5.31089223718627, w0=72.67524752475258, w1=15.914479198860077\n",
      "SubGD iter. 192/499: loss=5.3108699223791564, w0=72.67524752475258, w1=15.91843145960283\n",
      "SubGD iter. 193/499: loss=5.310862636053835, w0=72.66831683168327, w1=15.916305359817093\n",
      "SubGD iter. 194/499: loss=5.310859611715927, w0=72.66831683168327, w1=15.920257620559847\n",
      "SubGD iter. 195/499: loss=5.310837296908816, w0=72.66831683168327, w1=15.9242098813026\n",
      "SubGD iter. 196/499: loss=5.310814982101703, w0=72.66831683168327, w1=15.928162142045354\n",
      "SubGD iter. 197/499: loss=5.310823570190169, w0=72.66138613861396, w1=15.926036042259616\n",
      "SubGD iter. 198/499: loss=5.310804671438475, w0=72.66138613861396, w1=15.92998830300237\n",
      "SubGD iter. 199/499: loss=5.3107823566313614, w0=72.66138613861396, w1=15.933940563745123\n",
      "SubGD iter. 200/499: loss=5.310772500182623, w0=72.65445544554466, w1=15.931814463959386\n",
      "SubGD iter. 201/499: loss=5.3107720459681325, w0=72.65445544554466, w1=15.93576672470214\n",
      "SubGD iter. 202/499: loss=5.310749731161019, w0=72.65445544554466, w1=15.939718985444893\n",
      "SubGD iter. 203/499: loss=5.310727416353908, w0=72.65445544554466, w1=15.943671246187646\n",
      "SubGD iter. 204/499: loss=5.310733434318959, w0=72.64752475247535, w1=15.941545146401909\n",
      "SubGD iter. 205/499: loss=5.310717105690678, w0=72.64752475247535, w1=15.945497407144662\n",
      "SubGD iter. 206/499: loss=5.3106947908835656, w0=72.64752475247535, w1=15.949449667887416\n",
      "SubGD iter. 207/499: loss=5.310682364311412, w0=72.64059405940604, w1=15.947323568101679\n",
      "SubGD iter. 208/499: loss=5.3106844802203375, w0=72.64059405940604, w1=15.951275828844432\n",
      "SubGD iter. 209/499: loss=5.310662165413224, w0=72.64059405940604, w1=15.955228089587186\n",
      "SubGD iter. 210/499: loss=5.310639850606112, w0=72.64059405940604, w1=15.95918035032994\n",
      "SubGD iter. 211/499: loss=5.310643298447746, w0=72.63366336633673, w1=15.957054250544202\n",
      "SubGD iter. 212/499: loss=5.310629539942882, w0=72.63366336633673, w1=15.961006511286955\n",
      "SubGD iter. 213/499: loss=5.3106072251357705, w0=72.63366336633673, w1=15.964958772029709\n",
      "SubGD iter. 214/499: loss=5.310592228440201, w0=72.62673267326743, w1=15.962832672243971\n",
      "SubGD iter. 215/499: loss=5.310633183099028, w0=72.63366336633673, w1=15.967301372051375\n",
      "SubGD iter. 216/499: loss=5.310599343585063, w0=72.62673267326743, w1=15.965175272265638\n",
      "SubGD iter. 217/499: loss=5.31061822827579, w0=72.63366336633673, w1=15.969643972073042\n",
      "SubGD iter. 218/499: loss=5.310606458729926, w0=72.62673267326743, w1=15.967517872287305\n",
      "SubGD iter. 219/499: loss=5.3106032734525535, w0=72.63366336633673, w1=15.971986572094709\n",
      "SubGD iter. 220/499: loss=5.31061357387479, w0=72.62673267326743, w1=15.969860472308971\n",
      "SubGD iter. 221/499: loss=5.310588318629316, w0=72.63366336633673, w1=15.974329172116375\n",
      "SubGD iter. 222/499: loss=5.310620689019651, w0=72.62673267326743, w1=15.972203072330638\n",
      "SubGD iter. 223/499: loss=5.310574966149887, w0=72.62673267326743, w1=15.970593411609551\n",
      "SubGD iter. 224/499: loss=5.310583639649728, w0=72.63366336633673, w1=15.975062111416955\n",
      "SubGD iter. 225/499: loss=5.310622915165495, w0=72.62673267326743, w1=15.972936011631218\n",
      "SubGD iter. 226/499: loss=5.310576651555033, w0=72.62673267326743, w1=15.97132635091013\n",
      "SubGD iter. 227/499: loss=5.31057896067014, w0=72.63366336633673, w1=15.975795050717535\n",
      "SubGD iter. 228/499: loss=5.310625141311338, w0=72.62673267326743, w1=15.973668950931797\n",
      "SubGD iter. 229/499: loss=5.310578336960181, w0=72.62673267326743, w1=15.97205929021071\n",
      "SubGD iter. 230/499: loss=5.310574635520699, w0=72.62673267326743, w1=15.970449629489623\n",
      "SubGD iter. 231/499: loss=5.310584557534204, w0=72.63366336633673, w1=15.974918329297028\n",
      "SubGD iter. 232/499: loss=5.31062247845816, w0=72.62673267326743, w1=15.97279222951129\n",
      "SubGD iter. 233/499: loss=5.310576320925847, w0=72.62673267326743, w1=15.971182568790203\n",
      "SubGD iter. 234/499: loss=5.3105798785546146, w0=72.63366336633673, w1=15.975651268597607\n",
      "SubGD iter. 235/499: loss=5.310624704604003, w0=72.62673267326743, w1=15.97352516881187\n",
      "SubGD iter. 236/499: loss=5.310578006330994, w0=72.62673267326743, w1=15.971915508090783\n",
      "SubGD iter. 237/499: loss=5.310575199575027, w0=72.63366336633673, w1=15.976384207898187\n",
      "SubGD iter. 238/499: loss=5.310626930749845, w0=72.62673267326743, w1=15.97425810811245\n",
      "SubGD iter. 239/499: loss=5.310579691736141, w0=72.62673267326743, w1=15.972648447391363\n",
      "SubGD iter. 240/499: loss=5.310575990296659, w0=72.62673267326743, w1=15.971038786670276\n",
      "SubGD iter. 241/499: loss=5.310580796439089, w0=72.63366336633673, w1=15.97550748647768\n",
      "SubGD iter. 242/499: loss=5.310624267896667, w0=72.62673267326743, w1=15.973381386691942\n",
      "SubGD iter. 243/499: loss=5.310577675701806, w0=72.62673267326743, w1=15.971771725970855\n",
      "SubGD iter. 244/499: loss=5.310576117459501, w0=72.63366336633673, w1=15.97624042577826\n",
      "SubGD iter. 245/499: loss=5.31062649404251, w0=72.62673267326743, w1=15.974114325992522\n",
      "SubGD iter. 246/499: loss=5.310579361106954, w0=72.62673267326743, w1=15.972504665271435\n",
      "SubGD iter. 247/499: loss=5.310575659667472, w0=72.62673267326743, w1=15.970895004550348\n",
      "SubGD iter. 248/499: loss=5.310581714323563, w0=72.63366336633673, w1=15.975363704357752\n",
      "SubGD iter. 249/499: loss=5.310623831189332, w0=72.62673267326743, w1=15.973237604572015\n",
      "SubGD iter. 250/499: loss=5.31057734507262, w0=72.62673267326743, w1=15.971627943850928\n",
      "SubGD iter. 251/499: loss=5.310577035343975, w0=72.63366336633673, w1=15.976096643658332\n",
      "SubGD iter. 252/499: loss=5.310626057335176, w0=72.62673267326743, w1=15.973970543872595\n",
      "SubGD iter. 253/499: loss=5.310579030477766, w0=72.62673267326743, w1=15.972360883151508\n",
      "SubGD iter. 254/499: loss=5.3105753290382856, w0=72.62673267326743, w1=15.97075122243042\n",
      "SubGD iter. 255/499: loss=5.310582632208036, w0=72.63366336633673, w1=15.975219922237825\n",
      "SubGD iter. 256/499: loss=5.310623394481998, w0=72.62673267326743, w1=15.973093822452087\n",
      "SubGD iter. 257/499: loss=5.3105770144434326, w0=72.62673267326743, w1=15.971484161731\n",
      "SubGD iter. 258/499: loss=5.3105779532284485, w0=72.63366336633673, w1=15.975952861538405\n",
      "SubGD iter. 259/499: loss=5.3106256206278415, w0=72.62673267326743, w1=15.973826761752667\n",
      "SubGD iter. 260/499: loss=5.310578699848581, w0=72.62673267326743, w1=15.97221710103158\n",
      "SubGD iter. 261/499: loss=5.310574998409099, w0=72.62673267326743, w1=15.970607440310493\n",
      "SubGD iter. 262/499: loss=5.3105835500925105, w0=72.63366336633673, w1=15.975076140117897\n",
      "SubGD iter. 263/499: loss=5.3106229577746635, w0=72.62673267326743, w1=15.97295004033216\n",
      "SubGD iter. 264/499: loss=5.310576683814246, w0=72.62673267326743, w1=15.971340379611073\n",
      "SubGD iter. 265/499: loss=5.310578871112924, w0=72.63366336633673, w1=15.975809079418477\n",
      "SubGD iter. 266/499: loss=5.310625183920507, w0=72.62673267326743, w1=15.97368297963274\n",
      "SubGD iter. 267/499: loss=5.310578369219393, w0=72.62673267326743, w1=15.972073318911653\n",
      "SubGD iter. 268/499: loss=5.310574667779911, w0=72.62673267326743, w1=15.970463658190566\n",
      "SubGD iter. 269/499: loss=5.310584467976985, w0=72.63366336633673, w1=15.97493235799797\n",
      "SubGD iter. 270/499: loss=5.310622521067329, w0=72.62673267326743, w1=15.972806258212232\n",
      "SubGD iter. 271/499: loss=5.310576353185058, w0=72.62673267326743, w1=15.971196597491145\n",
      "SubGD iter. 272/499: loss=5.310579788997397, w0=72.63366336633673, w1=15.97566529729855\n",
      "SubGD iter. 273/499: loss=5.310624747213171, w0=72.62673267326743, w1=15.973539197512812\n",
      "SubGD iter. 274/499: loss=5.310578038590206, w0=72.62673267326743, w1=15.971929536791725\n",
      "SubGD iter. 275/499: loss=5.310575110017809, w0=72.63366336633673, w1=15.97639823659913\n",
      "SubGD iter. 276/499: loss=5.310626973359014, w0=72.62673267326743, w1=15.974272136813392\n",
      "SubGD iter. 277/499: loss=5.310579723995353, w0=72.62673267326743, w1=15.972662476092305\n",
      "SubGD iter. 278/499: loss=5.310576022555872, w0=72.62673267326743, w1=15.971052815371218\n",
      "SubGD iter. 279/499: loss=5.31058070688187, w0=72.63366336633673, w1=15.975521515178622\n",
      "SubGD iter. 280/499: loss=5.310624310505837, w0=72.62673267326743, w1=15.973395415392885\n",
      "SubGD iter. 281/499: loss=5.31057770796102, w0=72.62673267326743, w1=15.971785754671798\n",
      "SubGD iter. 282/499: loss=5.310576027902282, w0=72.63366336633673, w1=15.976254454479202\n",
      "SubGD iter. 283/499: loss=5.31062653665168, w0=72.62673267326743, w1=15.974128354693464\n",
      "SubGD iter. 284/499: loss=5.310579393366166, w0=72.62673267326743, w1=15.972518693972377\n",
      "SubGD iter. 285/499: loss=5.310575691926684, w0=72.62673267326743, w1=15.97090903325129\n",
      "SubGD iter. 286/499: loss=5.3105816247663435, w0=72.63366336633673, w1=15.975377733058695\n",
      "SubGD iter. 287/499: loss=5.310623873798502, w0=72.62673267326743, w1=15.973251633272957\n",
      "SubGD iter. 288/499: loss=5.310577377331832, w0=72.62673267326743, w1=15.97164197255187\n",
      "SubGD iter. 289/499: loss=5.310576945786757, w0=72.63366336633673, w1=15.976110672359274\n",
      "SubGD iter. 290/499: loss=5.310626099944344, w0=72.62673267326743, w1=15.973984572573537\n",
      "SubGD iter. 291/499: loss=5.310579062736979, w0=72.62673267326743, w1=15.97237491185245\n",
      "SubGD iter. 292/499: loss=5.310575361297499, w0=72.62673267326743, w1=15.970765251131363\n",
      "SubGD iter. 293/499: loss=5.310582542650818, w0=72.63366336633673, w1=15.975233950938767\n",
      "SubGD iter. 294/499: loss=5.310623437091167, w0=72.62673267326743, w1=15.97310785115303\n",
      "SubGD iter. 295/499: loss=5.310577046702645, w0=72.62673267326743, w1=15.971498190431943\n",
      "SubGD iter. 296/499: loss=5.31057786367123, w0=72.63366336633673, w1=15.975966890239347\n",
      "SubGD iter. 297/499: loss=5.310625663237009, w0=72.62673267326743, w1=15.97384079045361\n",
      "SubGD iter. 298/499: loss=5.310578732107792, w0=72.62673267326743, w1=15.972231129732522\n",
      "SubGD iter. 299/499: loss=5.310575030668311, w0=72.62673267326743, w1=15.970621469011435\n",
      "SubGD iter. 300/499: loss=5.310583460535291, w0=72.63366336633673, w1=15.97509016881884\n",
      "SubGD iter. 301/499: loss=5.310623000383832, w0=72.62673267326743, w1=15.972964069033102\n",
      "SubGD iter. 302/499: loss=5.3105767160734585, w0=72.62673267326743, w1=15.971354408312015\n",
      "SubGD iter. 303/499: loss=5.310578781555703, w0=72.63366336633673, w1=15.97582310811942\n",
      "SubGD iter. 304/499: loss=5.310625226529674, w0=72.62673267326743, w1=15.973697008333682\n",
      "SubGD iter. 305/499: loss=5.3105784014786055, w0=72.62673267326743, w1=15.972087347612595\n",
      "SubGD iter. 306/499: loss=5.3105747000391235, w0=72.62673267326743, w1=15.970477686891508\n",
      "SubGD iter. 307/499: loss=5.310584378419766, w0=72.63366336633673, w1=15.974946386698912\n",
      "SubGD iter. 308/499: loss=5.310622563676496, w0=72.62673267326743, w1=15.972820286913175\n",
      "SubGD iter. 309/499: loss=5.3105763854442705, w0=72.62673267326743, w1=15.971210626192088\n",
      "SubGD iter. 310/499: loss=5.310579699440178, w0=72.63366336633673, w1=15.975679325999492\n",
      "SubGD iter. 311/499: loss=5.31062478982234, w0=72.62673267326743, w1=15.973553226213754\n",
      "SubGD iter. 312/499: loss=5.310578070849419, w0=72.62673267326743, w1=15.971943565492667\n",
      "SubGD iter. 313/499: loss=5.310575020460591, w0=72.63366336633673, w1=15.976412265300072\n",
      "SubGD iter. 314/499: loss=5.3106270159681825, w0=72.62673267326743, w1=15.974286165514334\n",
      "SubGD iter. 315/499: loss=5.310579756254565, w0=72.62673267326743, w1=15.972676504793247\n",
      "SubGD iter. 316/499: loss=5.310576054815085, w0=72.62673267326743, w1=15.97106684407216\n",
      "SubGD iter. 317/499: loss=5.310580617324651, w0=72.63366336633673, w1=15.975535543879564\n",
      "SubGD iter. 318/499: loss=5.3106243531150055, w0=72.62673267326743, w1=15.973409444093827\n",
      "SubGD iter. 319/499: loss=5.310577740220231, w0=72.62673267326743, w1=15.97179978337274\n",
      "SubGD iter. 320/499: loss=5.310575938345063, w0=72.63366336633673, w1=15.976268483180144\n",
      "SubGD iter. 321/499: loss=5.310626579260848, w0=72.62673267326743, w1=15.974142383394407\n",
      "SubGD iter. 322/499: loss=5.310579425625379, w0=72.62673267326743, w1=15.97253272267332\n",
      "SubGD iter. 323/499: loss=5.310575724185897, w0=72.62673267326743, w1=15.970923061952233\n",
      "SubGD iter. 324/499: loss=5.310581535209124, w0=72.63366336633673, w1=15.975391761759637\n",
      "SubGD iter. 325/499: loss=5.310623916407671, w0=72.62673267326743, w1=15.9732656619739\n",
      "SubGD iter. 326/499: loss=5.310577409591045, w0=72.62673267326743, w1=15.971656001252812\n",
      "SubGD iter. 327/499: loss=5.310576856229538, w0=72.63366336633673, w1=15.976124701060217\n",
      "SubGD iter. 328/499: loss=5.310626142553512, w0=72.62673267326743, w1=15.973998601274479\n",
      "SubGD iter. 329/499: loss=5.310579094996193, w0=72.62673267326743, w1=15.972388940553392\n",
      "SubGD iter. 330/499: loss=5.310575393556711, w0=72.62673267326743, w1=15.970779279832305\n",
      "SubGD iter. 331/499: loss=5.310582453093599, w0=72.63366336633673, w1=15.97524797963971\n",
      "SubGD iter. 332/499: loss=5.310623479700335, w0=72.62673267326743, w1=15.973121879853972\n",
      "SubGD iter. 333/499: loss=5.310577078961859, w0=72.62673267326743, w1=15.971512219132885\n",
      "SubGD iter. 334/499: loss=5.3105777741140106, w0=72.63366336633673, w1=15.975980918940289\n",
      "SubGD iter. 335/499: loss=5.310625705846178, w0=72.62673267326743, w1=15.973854819154552\n",
      "SubGD iter. 336/499: loss=5.310578764367005, w0=72.62673267326743, w1=15.972245158433465\n",
      "SubGD iter. 337/499: loss=5.3105750629275255, w0=72.62673267326743, w1=15.970635497712378\n",
      "SubGD iter. 338/499: loss=5.3105833709780725, w0=72.63366336633673, w1=15.975104197519782\n",
      "SubGD iter. 339/499: loss=5.310623042993001, w0=72.62673267326743, w1=15.972978097734044\n",
      "SubGD iter. 340/499: loss=5.310576748332672, w0=72.62673267326743, w1=15.971368437012957\n",
      "SubGD iter. 341/499: loss=5.310578691998486, w0=72.63366336633673, w1=15.975837136820362\n",
      "SubGD iter. 342/499: loss=5.310625269138844, w0=72.62673267326743, w1=15.973711037034624\n",
      "SubGD iter. 343/499: loss=5.310578433737818, w0=72.62673267326743, w1=15.972101376313537\n",
      "SubGD iter. 344/499: loss=5.3105747322983365, w0=72.62673267326743, w1=15.97049171559245\n",
      "SubGD iter. 345/499: loss=5.310584288862547, w0=72.63366336633673, w1=15.974960415399854\n",
      "SubGD iter. 346/499: loss=5.3106226062856665, w0=72.62673267326743, w1=15.972834315614117\n",
      "SubGD iter. 347/499: loss=5.310576417703483, w0=72.62673267326743, w1=15.97122465489303\n",
      "SubGD iter. 348/499: loss=5.310579609882957, w0=72.63366336633673, w1=15.975693354700434\n",
      "SubGD iter. 349/499: loss=5.310624832431509, w0=72.62673267326743, w1=15.973567254914697\n",
      "SubGD iter. 350/499: loss=5.310578103108631, w0=72.62673267326743, w1=15.97195759419361\n",
      "SubGD iter. 351/499: loss=5.310574930903371, w0=72.63366336633673, w1=15.976426294001014\n",
      "SubGD iter. 352/499: loss=5.310627058577351, w0=72.62673267326743, w1=15.974300194215276\n",
      "SubGD iter. 353/499: loss=5.3105797885137775, w0=72.62673267326743, w1=15.97269053349419\n",
      "SubGD iter. 354/499: loss=5.310576087074297, w0=72.62673267326743, w1=15.971080872773102\n",
      "SubGD iter. 355/499: loss=5.310580527767432, w0=72.63366336633673, w1=15.975549572580507\n",
      "SubGD iter. 356/499: loss=5.310624395724175, w0=72.62673267326743, w1=15.973423472794769\n",
      "SubGD iter. 357/499: loss=5.310577772479444, w0=72.62673267326743, w1=15.971813812073682\n",
      "SubGD iter. 358/499: loss=5.3105758487878445, w0=72.63366336633673, w1=15.976282511881086\n",
      "SubGD iter. 359/499: loss=5.310626621870016, w0=72.62673267326743, w1=15.974156412095349\n",
      "SubGD iter. 360/499: loss=5.310579457884592, w0=72.62673267326743, w1=15.972546751374262\n",
      "SubGD iter. 361/499: loss=5.31057575644511, w0=72.62673267326743, w1=15.970937090653175\n",
      "SubGD iter. 362/499: loss=5.310581445651906, w0=72.63366336633673, w1=15.975405790460579\n",
      "SubGD iter. 363/499: loss=5.310623959016839, w0=72.62673267326743, w1=15.973279690674842\n",
      "SubGD iter. 364/499: loss=5.310577441850257, w0=72.62673267326743, w1=15.971670029953755\n",
      "SubGD iter. 365/499: loss=5.310576766672319, w0=72.63366336633673, w1=15.976138729761159\n",
      "SubGD iter. 366/499: loss=5.310626185162682, w0=72.62673267326743, w1=15.974012629975421\n",
      "SubGD iter. 367/499: loss=5.310579127255404, w0=72.62673267326743, w1=15.972402969254334\n",
      "SubGD iter. 368/499: loss=5.310575425815924, w0=72.62673267326743, w1=15.970793308533247\n",
      "SubGD iter. 369/499: loss=5.31058236353638, w0=72.63366336633673, w1=15.975262008340652\n",
      "SubGD iter. 370/499: loss=5.310623522309504, w0=72.62673267326743, w1=15.973135908554914\n",
      "SubGD iter. 371/499: loss=5.31057711122107, w0=72.62673267326743, w1=15.971526247833827\n",
      "SubGD iter. 372/499: loss=5.310577684556793, w0=72.63366336633673, w1=15.975994947641231\n",
      "SubGD iter. 373/499: loss=5.3106257484553465, w0=72.62673267326743, w1=15.973868847855494\n",
      "SubGD iter. 374/499: loss=5.310578796626218, w0=72.62673267326743, w1=15.972259187134407\n",
      "SubGD iter. 375/499: loss=5.310575095186736, w0=72.62673267326743, w1=15.97064952641332\n",
      "SubGD iter. 376/499: loss=5.310583281420854, w0=72.63366336633673, w1=15.975118226220724\n",
      "SubGD iter. 377/499: loss=5.3106230856021694, w0=72.62673267326743, w1=15.972992126434987\n",
      "SubGD iter. 378/499: loss=5.310576780591885, w0=72.62673267326743, w1=15.9713824657139\n",
      "SubGD iter. 379/499: loss=5.310578602441266, w0=72.63366336633673, w1=15.975851165521304\n",
      "SubGD iter. 380/499: loss=5.310625311748012, w0=72.62673267326743, w1=15.973725065735566\n",
      "SubGD iter. 381/499: loss=5.310578465997031, w0=72.62673267326743, w1=15.97211540501448\n",
      "SubGD iter. 382/499: loss=5.31057476455755, w0=72.62673267326743, w1=15.970505744293392\n",
      "SubGD iter. 383/499: loss=5.310584199305326, w0=72.63366336633673, w1=15.974974444100797\n",
      "SubGD iter. 384/499: loss=5.310622648894835, w0=72.62673267326743, w1=15.972848344315059\n",
      "SubGD iter. 385/499: loss=5.310576449962697, w0=72.62673267326743, w1=15.971238683593972\n",
      "SubGD iter. 386/499: loss=5.310579520325739, w0=72.63366336633673, w1=15.975707383401376\n",
      "SubGD iter. 387/499: loss=5.310624875040678, w0=72.62673267326743, w1=15.973581283615639\n",
      "SubGD iter. 388/499: loss=5.3105781353678445, w0=72.62673267326743, w1=15.971971622894552\n",
      "SubGD iter. 389/499: loss=5.310574841346153, w0=72.63366336633673, w1=15.976440322701956\n",
      "SubGD iter. 390/499: loss=5.31062710118652, w0=72.62673267326743, w1=15.974314222916218\n",
      "SubGD iter. 391/499: loss=5.3105798207729915, w0=72.62673267326743, w1=15.972704562195132\n",
      "SubGD iter. 392/499: loss=5.3105761193335095, w0=72.62673267326743, w1=15.971094901474045\n",
      "SubGD iter. 393/499: loss=5.310580438210215, w0=72.63366336633673, w1=15.975563601281449\n",
      "SubGD iter. 394/499: loss=5.310624438333342, w0=72.62673267326743, w1=15.973437501495711\n",
      "SubGD iter. 395/499: loss=5.3105778047386565, w0=72.62673267326743, w1=15.971827840774624\n",
      "SubGD iter. 396/499: loss=5.310575759230627, w0=72.63366336633673, w1=15.976296540582029\n",
      "SubGD iter. 397/499: loss=5.3106266644791855, w0=72.62673267326743, w1=15.974170440796291\n",
      "SubGD iter. 398/499: loss=5.310579490143804, w0=72.62673267326743, w1=15.972560780075204\n",
      "SubGD iter. 399/499: loss=5.310575788704323, w0=72.62673267326743, w1=15.970951119354117\n",
      "SubGD iter. 400/499: loss=5.310581356094687, w0=72.63366336633673, w1=15.975419819161521\n",
      "SubGD iter. 401/499: loss=5.310624001626008, w0=72.62673267326743, w1=15.973293719375784\n",
      "SubGD iter. 402/499: loss=5.31057747410947, w0=72.62673267326743, w1=15.971684058654697\n",
      "SubGD iter. 403/499: loss=5.310576677115099, w0=72.63366336633673, w1=15.976152758462101\n",
      "SubGD iter. 404/499: loss=5.31062622777185, w0=72.62673267326743, w1=15.974026658676364\n",
      "SubGD iter. 405/499: loss=5.310579159514617, w0=72.62673267326743, w1=15.972416997955277\n",
      "SubGD iter. 406/499: loss=5.310575458075137, w0=72.62673267326743, w1=15.97080733723419\n",
      "SubGD iter. 407/499: loss=5.310582273979161, w0=72.63366336633673, w1=15.975276037041594\n",
      "SubGD iter. 408/499: loss=5.310623564918673, w0=72.62673267326743, w1=15.973149937255856\n",
      "SubGD iter. 409/499: loss=5.310577143480284, w0=72.62673267326743, w1=15.97154027653477\n",
      "SubGD iter. 410/499: loss=5.310577594999573, w0=72.63366336633673, w1=15.976008976342174\n",
      "SubGD iter. 411/499: loss=5.310625791064516, w0=72.62673267326743, w1=15.973882876556436\n",
      "SubGD iter. 412/499: loss=5.31057882888543, w0=72.62673267326743, w1=15.972273215835349\n",
      "SubGD iter. 413/499: loss=5.310575127445949, w0=72.62673267326743, w1=15.970663555114262\n",
      "SubGD iter. 414/499: loss=5.3105831918636355, w0=72.63366336633673, w1=15.975132254921666\n",
      "SubGD iter. 415/499: loss=5.310623128211338, w0=72.62673267326743, w1=15.973006155135929\n",
      "SubGD iter. 416/499: loss=5.310576812851097, w0=72.62673267326743, w1=15.971396494414842\n",
      "SubGD iter. 417/499: loss=5.310578512884048, w0=72.63366336633673, w1=15.975865194222246\n",
      "SubGD iter. 418/499: loss=5.31062535435718, w0=72.62673267326743, w1=15.973739094436509\n",
      "SubGD iter. 419/499: loss=5.310578498256244, w0=72.62673267326743, w1=15.972129433715422\n",
      "SubGD iter. 420/499: loss=5.310574796816762, w0=72.62673267326743, w1=15.970519772994335\n",
      "SubGD iter. 421/499: loss=5.31058410974811, w0=72.63366336633673, w1=15.974988472801739\n",
      "SubGD iter. 422/499: loss=5.310622691504004, w0=72.62673267326743, w1=15.972862373016001\n",
      "SubGD iter. 423/499: loss=5.310576482221909, w0=72.62673267326743, w1=15.971252712294914\n",
      "SubGD iter. 424/499: loss=5.310579430768521, w0=72.63366336633673, w1=15.975721412102319\n",
      "SubGD iter. 425/499: loss=5.310624917649847, w0=72.62673267326743, w1=15.973595312316581\n",
      "SubGD iter. 426/499: loss=5.310578167627056, w0=72.62673267326743, w1=15.971985651595494\n",
      "SubGD iter. 427/499: loss=5.310574751788934, w0=72.63366336633673, w1=15.976454351402898\n",
      "SubGD iter. 428/499: loss=5.310627143795689, w0=72.62673267326743, w1=15.97432825161716\n",
      "SubGD iter. 429/499: loss=5.310579853032204, w0=72.62673267326743, w1=15.972718590896074\n",
      "SubGD iter. 430/499: loss=5.310576151592722, w0=72.62673267326743, w1=15.971108930174987\n",
      "SubGD iter. 431/499: loss=5.310580348652995, w0=72.63366336633673, w1=15.975577629982391\n",
      "SubGD iter. 432/499: loss=5.310624480942511, w0=72.62673267326743, w1=15.973451530196654\n",
      "SubGD iter. 433/499: loss=5.31057783699787, w0=72.62673267326743, w1=15.971841869475567\n",
      "SubGD iter. 434/499: loss=5.310575669673408, w0=72.63366336633673, w1=15.97631056928297\n",
      "SubGD iter. 435/499: loss=5.310626707088354, w0=72.62673267326743, w1=15.974184469497233\n",
      "SubGD iter. 436/499: loss=5.3105795224030174, w0=72.62673267326743, w1=15.972574808776146\n",
      "SubGD iter. 437/499: loss=5.310575820963536, w0=72.62673267326743, w1=15.97096514805506\n",
      "SubGD iter. 438/499: loss=5.3105812665374685, w0=72.63366336633673, w1=15.975433847862464\n",
      "SubGD iter. 439/499: loss=5.310624044235177, w0=72.62673267326743, w1=15.973307748076726\n",
      "SubGD iter. 440/499: loss=5.310577506368683, w0=72.62673267326743, w1=15.97169808735564\n",
      "SubGD iter. 441/499: loss=5.310576587557881, w0=72.63366336633673, w1=15.976166787163043\n",
      "SubGD iter. 442/499: loss=5.31062627038102, w0=72.62673267326743, w1=15.974040687377306\n",
      "SubGD iter. 443/499: loss=5.31057919177383, w0=72.62673267326743, w1=15.972431026656219\n",
      "SubGD iter. 444/499: loss=5.310575490334348, w0=72.62673267326743, w1=15.970821365935132\n",
      "SubGD iter. 445/499: loss=5.310582184421943, w0=72.63366336633673, w1=15.975290065742536\n",
      "SubGD iter. 446/499: loss=5.310623607527843, w0=72.62673267326743, w1=15.973163965956799\n",
      "SubGD iter. 447/499: loss=5.310577175739496, w0=72.62673267326743, w1=15.971554305235712\n",
      "SubGD iter. 448/499: loss=5.310577505442355, w0=72.63366336633673, w1=15.976023005043116\n",
      "SubGD iter. 449/499: loss=5.310625833673684, w0=72.62673267326743, w1=15.973896905257378\n",
      "SubGD iter. 450/499: loss=5.310578861144643, w0=72.62673267326743, w1=15.972287244536291\n",
      "SubGD iter. 451/499: loss=5.310575159705161, w0=72.62673267326743, w1=15.970677583815204\n",
      "SubGD iter. 452/499: loss=5.310583102306416, w0=72.63366336633673, w1=15.975146283622609\n",
      "SubGD iter. 453/499: loss=5.310623170820506, w0=72.62673267326743, w1=15.973020183836871\n",
      "SubGD iter. 454/499: loss=5.310576845110308, w0=72.62673267326743, w1=15.971410523115784\n",
      "SubGD iter. 455/499: loss=5.310578423326828, w0=72.63366336633673, w1=15.975879222923188\n",
      "SubGD iter. 456/499: loss=5.3106253969663495, w0=72.62673267326743, w1=15.97375312313745\n",
      "SubGD iter. 457/499: loss=5.310578530515456, w0=72.62673267326743, w1=15.972143462416364\n",
      "SubGD iter. 458/499: loss=5.310574829075976, w0=72.62673267326743, w1=15.970533801695277\n",
      "SubGD iter. 459/499: loss=5.31058402019089, w0=72.63366336633673, w1=15.975002501502681\n",
      "SubGD iter. 460/499: loss=5.3106227341131715, w0=72.62673267326743, w1=15.972876401716944\n",
      "SubGD iter. 461/499: loss=5.310576514481123, w0=72.62673267326743, w1=15.971266740995857\n",
      "SubGD iter. 462/499: loss=5.3105793412113025, w0=72.63366336633673, w1=15.97573544080326\n",
      "SubGD iter. 463/499: loss=5.310624960259015, w0=72.62673267326743, w1=15.973609341017523\n",
      "SubGD iter. 464/499: loss=5.31057819988627, w0=72.62673267326743, w1=15.971999680296436\n",
      "SubGD iter. 465/499: loss=5.310574662231714, w0=72.63366336633673, w1=15.97646838010384\n",
      "SubGD iter. 466/499: loss=5.310627186404858, w0=72.62673267326743, w1=15.974342280318103\n",
      "SubGD iter. 467/499: loss=5.310579885291418, w0=72.62673267326743, w1=15.972732619597016\n",
      "SubGD iter. 468/499: loss=5.310576183851936, w0=72.62673267326743, w1=15.97112295887593\n",
      "SubGD iter. 469/499: loss=5.310580259095775, w0=72.63366336633673, w1=15.975591658683333\n",
      "SubGD iter. 470/499: loss=5.31062452355168, w0=72.62673267326743, w1=15.973465558897596\n",
      "SubGD iter. 471/499: loss=5.310577869257083, w0=72.62673267326743, w1=15.971855898176509\n",
      "SubGD iter. 472/499: loss=5.310575580116187, w0=72.63366336633673, w1=15.976324597983913\n",
      "SubGD iter. 473/499: loss=5.310626749697522, w0=72.62673267326743, w1=15.974198498198175\n",
      "SubGD iter. 474/499: loss=5.3105795546622305, w0=72.62673267326743, w1=15.972588837477089\n",
      "SubGD iter. 475/499: loss=5.3105758532227485, w0=72.62673267326743, w1=15.970979176756002\n",
      "SubGD iter. 476/499: loss=5.310581176980251, w0=72.63366336633673, w1=15.975447876563406\n",
      "SubGD iter. 477/499: loss=5.310624086844346, w0=72.62673267326743, w1=15.973321776777668\n",
      "SubGD iter. 478/499: loss=5.3105775386278955, w0=72.62673267326743, w1=15.971712116056581\n",
      "SubGD iter. 479/499: loss=5.3105764980006605, w0=72.63366336633673, w1=15.976180815863986\n",
      "SubGD iter. 480/499: loss=5.310626312990188, w0=72.62673267326743, w1=15.974054716078248\n",
      "SubGD iter. 481/499: loss=5.3105792240330425, w0=72.62673267326743, w1=15.972445055357161\n",
      "SubGD iter. 482/499: loss=5.3105755225935605, w0=72.62673267326743, w1=15.970835394636074\n",
      "SubGD iter. 483/499: loss=5.310582094864723, w0=72.63366336633673, w1=15.975304094443478\n",
      "SubGD iter. 484/499: loss=5.31062365013701, w0=72.62673267326743, w1=15.97317799465774\n",
      "SubGD iter. 485/499: loss=5.310577207998708, w0=72.62673267326743, w1=15.971568333936654\n",
      "SubGD iter. 486/499: loss=5.3105774158851355, w0=72.63366336633673, w1=15.976037033744058\n",
      "SubGD iter. 487/499: loss=5.310625876282852, w0=72.62673267326743, w1=15.97391093395832\n",
      "SubGD iter. 488/499: loss=5.310578893403856, w0=72.62673267326743, w1=15.972301273237234\n",
      "SubGD iter. 489/499: loss=5.310575191964374, w0=72.62673267326743, w1=15.970691612516147\n",
      "SubGD iter. 490/499: loss=5.3105830127491975, w0=72.63366336633673, w1=15.97516031232355\n",
      "SubGD iter. 491/499: loss=5.310623213429675, w0=72.62673267326743, w1=15.973034212537813\n",
      "SubGD iter. 492/499: loss=5.310576877369521, w0=72.62673267326743, w1=15.971424551816726\n",
      "SubGD iter. 493/499: loss=5.31057833376961, w0=72.63366336633673, w1=15.97589325162413\n",
      "SubGD iter. 494/499: loss=5.310625439575518, w0=72.62673267326743, w1=15.973767151838393\n",
      "SubGD iter. 495/499: loss=5.310578562774669, w0=72.62673267326743, w1=15.972157491117306\n",
      "SubGD iter. 496/499: loss=5.310574861335188, w0=72.62673267326743, w1=15.97054783039622\n",
      "SubGD iter. 497/499: loss=5.310583930633671, w0=72.63366336633673, w1=15.975016530203623\n",
      "SubGD iter. 498/499: loss=5.310622776722341, w0=72.62673267326743, w1=15.972890430417886\n",
      "SubGD iter. 499/499: loss=5.310576546740335, w0=72.62673267326743, w1=15.971280769696799\n",
      "SubGD: execution time=0.013 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subgd_losses, subgd_ws = subgradient_descent(y, tx, w_initial, max_iters, gamma)\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f546fdeba8d74ffbad828fa08926995e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subgd_losses,\n",
    "        subgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subgd_ws)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Subgradient Descent\n",
    "\n",
    "**NB** for the computation of the subgradient you can reuse the `compute_subgradient` method that you implemented above, just making sure that you pass in a minibatch as opposed to the full data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_subgradient_descent(y, tx, initial_w, batch_size, max_iters, gamma):\n",
    "    \"\"\"Compute a stochastic subgradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "\n",
    "    Args:\n",
    "        y: numpy array of shape=(B, )\n",
    "        tx: numpy array of shape=(B,2)\n",
    "        initial_w: numpy array of shape=(2, ). The initial guess (or the initialization) for the model parameters\n",
    "        batch_size: a scalar denoting the number of data points in a mini-batch used for computing the stochastic subgradient\n",
    "        max_iters: a scalar denoting the total number of iterations of SubSGD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        losses: a list of length max_iters containing the loss value (scalar) for each iteration of SubSGD\n",
    "        ws: a list of length max_iters containing the model parameters as numpy arrays of shape (2, ), for each iteration of SubSGD\n",
    "    \"\"\"\n",
    "\n",
    "    # Define parameters to store w and loss\n",
    "    ws = [initial_w]\n",
    "    losses = []\n",
    "    w = initial_w\n",
    "\n",
    "    for n_iter in range(max_iters):\n",
    "        # ***************************************************\n",
    "        # INSERT YOUR CODE HERE\n",
    "        # TODO: implement stochastic subgradient descent.\n",
    "        # ***************************************************\n",
    "        for minibatch_y,minibatch_tx in batch_iter(y,tx,batch_size):\n",
    "            subgrad = compute_subgradient_mae(minibatch_y,minibatch_tx,w)\n",
    "        loss = compute_loss(y,tx,w)\n",
    "        w = w - gamma*subgrad\n",
    "        \n",
    "        ws.append(w)\n",
    "        losses.append(loss)\n",
    "\n",
    "        print(\n",
    "            \"SubSGD iter. {bi}/{ti}: loss={l}, w0={w0}, w1={w1}\".format(\n",
    "                bi=n_iter, ti=max_iters - 1, l=loss, w0=w[0], w1=w[1]\n",
    "            )\n",
    "        )\n",
    "    return losses, ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SubSGD iter. 0/499: loss=74.06780585492638, w0=0.7, w1=-0.678346995974543\n",
      "SubSGD iter. 1/499: loss=73.36780585492637, w0=1.4, w1=-0.07294384453055835\n",
      "SubSGD iter. 2/499: loss=72.66780585492639, w0=2.0999999999999996, w1=1.2898075508611884\n",
      "SubSGD iter. 3/499: loss=71.96780585492637, w0=2.8, w1=1.1056467725428685\n",
      "SubSGD iter. 4/499: loss=71.26780585492638, w0=3.5, w1=1.8670037231619379\n",
      "SubSGD iter. 5/499: loss=70.56780585492639, w0=4.2, w1=1.1803090876558011\n",
      "SubSGD iter. 6/499: loss=69.86780585492637, w0=4.9, w1=1.5353365651856272\n",
      "SubSGD iter. 7/499: loss=69.16780585492639, w0=5.6000000000000005, w1=1.330503045310404\n",
      "SubSGD iter. 8/499: loss=68.46780585492638, w0=6.300000000000001, w1=1.9616683853052566\n",
      "SubSGD iter. 9/499: loss=67.76780585492638, w0=7.000000000000001, w1=0.9347361877127229\n",
      "SubSGD iter. 10/499: loss=67.06780585492638, w0=7.700000000000001, w1=1.5477286848962826\n",
      "SubSGD iter. 11/499: loss=66.36780585492637, w0=8.4, w1=1.8178671351859526\n",
      "SubSGD iter. 12/499: loss=65.66780585492637, w0=9.1, w1=1.9501906615397913\n",
      "SubSGD iter. 13/499: loss=64.96780585492638, w0=9.799999999999999, w1=2.7453603866571386\n",
      "SubSGD iter. 14/499: loss=64.26780585492638, w0=10.499999999999998, w1=2.3632344021068037\n",
      "SubSGD iter. 15/499: loss=63.567805854926384, w0=11.199999999999998, w1=2.006508883750967\n",
      "SubSGD iter. 16/499: loss=62.867805854926374, w0=11.899999999999997, w1=0.8225066833570753\n",
      "SubSGD iter. 17/499: loss=62.167805854926385, w0=12.599999999999996, w1=-0.017520888102735555\n",
      "SubSGD iter. 18/499: loss=61.46780585492638, w0=13.299999999999995, w1=-0.9191531525154932\n",
      "SubSGD iter. 19/499: loss=60.76780585492637, w0=13.999999999999995, w1=-1.7729994229407624\n",
      "SubSGD iter. 20/499: loss=60.067805854926384, w0=14.699999999999994, w1=-2.3160158843058607\n",
      "SubSGD iter. 21/499: loss=59.367805854926395, w0=15.399999999999993, w1=-2.747808462589281\n",
      "SubSGD iter. 22/499: loss=58.667805854926385, w0=16.099999999999994, w1=-2.1243203403031035\n",
      "SubSGD iter. 23/499: loss=57.96780585492639, w0=16.799999999999994, w1=-1.9278002575246007\n",
      "SubSGD iter. 24/499: loss=57.26780585492638, w0=17.499999999999993, w1=-1.3980992834624846\n",
      "SubSGD iter. 25/499: loss=56.567805854926384, w0=18.199999999999992, w1=-0.8683983094003684\n",
      "SubSGD iter. 26/499: loss=55.86780585492639, w0=18.89999999999999, w1=-0.7029455174677426\n",
      "SubSGD iter. 27/499: loss=55.167805854926385, w0=19.59999999999999, w1=-0.5245933960879282\n",
      "SubSGD iter. 28/499: loss=54.46780585492639, w0=20.29999999999999, w1=-0.9001792343744979\n",
      "SubSGD iter. 29/499: loss=53.767805854926394, w0=20.99999999999999, w1=-1.0820430779496668\n",
      "SubSGD iter. 30/499: loss=53.067805854926384, w0=21.69999999999999, w1=-1.7855479963405294\n",
      "SubSGD iter. 31/499: loss=52.36780585492639, w0=22.399999999999988, w1=-1.9674118399156981\n",
      "SubSGD iter. 32/499: loss=51.667805854926385, w0=23.099999999999987, w1=-1.426454787944556\n",
      "SubSGD iter. 33/499: loss=50.9678058549264, w0=23.799999999999986, w1=-1.631288307819779\n",
      "SubSGD iter. 34/499: loss=50.267805854926394, w0=24.499999999999986, w1=-1.872268222486427\n",
      "SubSGD iter. 35/499: loss=49.567805854926405, w0=25.199999999999985, w1=-1.3238737187286163\n",
      "SubSGD iter. 36/499: loss=48.867805854926395, w0=25.899999999999984, w1=-0.9886456634253613\n",
      "SubSGD iter. 37/499: loss=48.1678058549264, w0=26.599999999999984, w1=-1.1156601086533864\n",
      "SubSGD iter. 38/499: loss=47.4678058549264, w0=27.299999999999983, w1=-2.241592522154557\n",
      "SubSGD iter. 39/499: loss=46.767805854926394, w0=27.999999999999982, w1=-1.8978799009711667\n",
      "SubSGD iter. 40/499: loss=46.06780585492639, w0=28.69999999999998, w1=-2.1419067256803177\n",
      "SubSGD iter. 41/499: loss=45.367805854926395, w0=29.39999999999998, w1=-2.4889907113003087\n",
      "SubSGD iter. 42/499: loss=44.66780585492641, w0=30.09999999999998, w1=-2.2598690190135504\n",
      "SubSGD iter. 43/499: loss=43.9678058549264, w0=30.79999999999998, w1=-1.6468765218299906\n",
      "SubSGD iter. 44/499: loss=43.2678058549264, w0=31.49999999999998, w1=-2.590383005367111\n",
      "SubSGD iter. 45/499: loss=42.567805854926405, w0=32.19999999999998, w1=-1.577704664014344\n",
      "SubSGD iter. 46/499: loss=41.867805854926395, w0=32.899999999999984, w1=-1.9771624364009228\n",
      "SubSGD iter. 47/499: loss=41.1678058549264, w0=33.59999999999999, w1=-1.7123773272101546\n",
      "SubSGD iter. 48/499: loss=40.4678058549264, w0=34.29999999999999, w1=-2.749891521023695\n",
      "SubSGD iter. 49/499: loss=39.767805854926394, w0=34.99999999999999, w1=-2.7845823803651664\n",
      "SubSGD iter. 50/499: loss=39.067805854926384, w0=35.699999999999996, w1=-2.9680481594155856\n",
      "SubSGD iter. 51/499: loss=38.36780585492639, w0=36.4, w1=-3.1185048358667133\n",
      "SubSGD iter. 52/499: loss=37.667805854926385, w0=37.1, w1=-3.7412912426870055\n",
      "SubSGD iter. 53/499: loss=36.96780585492638, w0=37.800000000000004, w1=-4.0264657372362995\n",
      "SubSGD iter. 54/499: loss=36.26780585492637, w0=38.50000000000001, w1=-3.602924484182284\n",
      "SubSGD iter. 55/499: loss=35.56780585492638, w0=39.20000000000001, w1=-3.3526433677202276\n",
      "SubSGD iter. 56/499: loss=34.867805854926374, w0=39.90000000000001, w1=-3.977459467531174\n",
      "SubSGD iter. 57/499: loss=34.16780585492637, w0=40.600000000000016, w1=-3.8116532079636767\n",
      "SubSGD iter. 58/499: loss=33.47159643167563, w0=41.30000000000002, w1=-3.388111954909661\n",
      "SubSGD iter. 59/499: loss=32.77178202113434, w0=42.00000000000002, w1=-3.7351959405296515\n",
      "SubSGD iter. 60/499: loss=32.098433834614994, w0=42.700000000000024, w1=-2.722757602663231\n",
      "SubSGD iter. 61/499: loss=31.375047310644113, w0=43.40000000000003, w1=-3.5171269471540114\n",
      "SubSGD iter. 62/499: loss=30.73307871726833, w0=44.10000000000003, w1=-3.228629682596596\n",
      "SubSGD iter. 63/499: loss=30.041403673976145, w0=44.80000000000003, w1=-4.172136166133717\n",
      "SubSGD iter. 64/499: loss=29.46710715792251, w0=45.500000000000036, w1=-3.159697828267296\n",
      "SubSGD iter. 65/499: loss=28.70848186857995, w0=46.20000000000004, w1=-3.403724652976447\n",
      "SubSGD iter. 66/499: loss=28.086040154232844, w0=46.90000000000004, w1=-4.174547719044634\n",
      "SubSGD iter. 67/499: loss=27.562641496697335, w0=46.20000000000004, w1=-3.0486153055434633\n",
      "SubSGD iter. 68/499: loss=28.04524153786485, w0=46.90000000000004, w1=-3.8886428770032744\n",
      "SubSGD iter. 69/499: loss=27.51817649252957, w0=47.600000000000044, w1=-4.461652960017898\n",
      "SubSGD iter. 70/499: loss=27.00838180878963, w0=48.30000000000005, w1=-3.5147406982754674\n",
      "SubSGD iter. 71/499: loss=26.223874297879487, w0=49.00000000000005, w1=-2.9017482010919076\n",
      "SubSGD iter. 72/499: loss=25.50364697915208, w0=49.70000000000005, w1=-3.3249876258644218\n",
      "SubSGD iter. 73/499: loss=24.987130305068092, w0=50.400000000000055, w1=-3.525737162913673\n",
      "SubSGD iter. 74/499: loss=24.445476234944735, w0=51.10000000000006, w1=-3.170709685383847\n",
      "SubSGD iter. 75/499: loss=23.78706520910693, w0=51.80000000000006, w1=-2.6426538698731985\n",
      "SubSGD iter. 76/499: loss=23.088123116565285, w0=51.10000000000006, w1=-1.9620546922171158\n",
      "SubSGD iter. 77/499: loss=23.515510396756625, w0=51.80000000000006, w1=-2.3615124646036945\n",
      "SubSGD iter. 78/499: loss=23.022267077125935, w0=52.500000000000064, w1=-2.708596450223685\n",
      "SubSGD iter. 79/499: loss=22.535237522786936, w0=53.20000000000007, w1=-2.99114426930991\n",
      "SubSGD iter. 80/499: loss=22.095161307880335, w0=52.500000000000064, w1=-2.2876393509190476\n",
      "SubSGD iter. 81/499: loss=22.429715126212315, w0=53.20000000000007, w1=-0.9222698982554365\n",
      "SubSGD iter. 82/499: loss=21.53463628939213, w0=53.90000000000007, w1=0.1341313331234535\n",
      "SubSGD iter. 83/499: loss=20.71303855369687, w0=54.60000000000007, w1=-0.04773251045171534\n",
      "SubSGD iter. 84/499: loss=20.181335096318133, w0=55.300000000000075, w1=0.14878757232678738\n",
      "SubSGD iter. 85/499: loss=19.582026459700344, w0=56.00000000000008, w1=-0.20115035915352983\n",
      "SubSGD iter. 86/499: loss=19.175992842019102, w0=55.300000000000075, w1=0.9828518412403616\n",
      "SubSGD iter. 87/499: loss=19.36464032896185, w0=56.00000000000008, w1=0.18848249674958129\n",
      "SubSGD iter. 88/499: loss=19.047366817177753, w0=56.70000000000008, w1=0.8314121955133769\n",
      "SubSGD iter. 89/499: loss=18.33907204788707, w0=57.400000000000084, w1=1.4743418942771724\n",
      "SubSGD iter. 90/499: loss=17.630960528673317, w0=58.10000000000009, w1=0.9846583608805841\n",
      "SubSGD iter. 91/499: loss=17.31770339913522, w0=58.80000000000009, w1=0.6482954032045074\n",
      "SubSGD iter. 92/499: loss=17.0178886645967, w0=58.10000000000009, w1=1.4426647476952876\n",
      "SubSGD iter. 93/499: loss=17.15475846849454, w0=57.400000000000084, w1=2.1461696660861502\n",
      "SubSGD iter. 94/499: loss=17.423645004620788, w0=58.10000000000009, w1=3.2387941804119444\n",
      "SubSGD iter. 95/499: loss=16.590861929620004, w0=58.80000000000009, w1=3.2041033210704732\n",
      "SubSGD iter. 96/499: loss=16.08792045334357, w0=59.50000000000009, w1=3.1903481250949826\n",
      "SubSGD iter. 97/499: loss=15.609013026771079, w0=60.200000000000095, w1=2.845171037369984\n",
      "SubSGD iter. 98/499: loss=15.306406341195883, w0=60.9000000000001, w1=2.6279140331902338\n",
      "SubSGD iter. 99/499: loss=14.995419524354556, w0=61.6000000000001, w1=3.094496159156809\n",
      "SubSGD iter. 100/499: loss=14.416951928790569, w0=62.300000000000104, w1=4.262826959674277\n",
      "SubSGD iter. 101/499: loss=13.542247895071364, w0=63.00000000000011, w1=4.908974193061498\n",
      "SubSGD iter. 102/499: loss=12.90181429898643, w0=63.70000000000011, w1=4.324661655515607\n",
      "SubSGD iter. 103/499: loss=12.831197655985207, w0=64.4000000000001, w1=5.16585182374025\n",
      "SubSGD iter. 104/499: loss=12.114140704245822, w0=65.10000000000011, w1=5.789339946026428\n",
      "SubSGD iter. 105/499: loss=11.508610445796755, w0=64.4000000000001, w1=6.523454269027416\n",
      "SubSGD iter. 106/499: loss=11.491857944752699, w0=63.7000000000001, w1=7.028472000851739\n",
      "SubSGD iter. 107/499: loss=11.634458401098406, w0=64.4000000000001, w1=7.293257110042507\n",
      "SubSGD iter. 108/499: loss=11.155289783311767, w0=63.7000000000001, w1=7.782940643439096\n",
      "SubSGD iter. 109/499: loss=11.356297787120326, w0=64.4000000000001, w1=8.662112955669903\n",
      "SubSGD iter. 110/499: loss=10.639664827747843, w0=65.10000000000011, w1=8.775933282484216\n",
      "SubSGD iter. 111/499: loss=10.190026811885742, w0=65.80000000000011, w1=5.402789604894538\n",
      "SubSGD iter. 112/499: loss=11.433495043265392, w0=66.50000000000011, w1=4.663456805450594\n",
      "SubSGD iter. 113/499: loss=11.618386163061627, w0=67.20000000000012, w1=5.364390802885347\n",
      "SubSGD iter. 114/499: loss=11.000761648399207, w0=67.90000000000012, w1=5.593512495172105\n",
      "SubSGD iter. 115/499: loss=10.66773339949761, w0=67.20000000000012, w1=6.004587679714331\n",
      "SubSGD iter. 116/499: loss=10.639653385763655, w0=67.90000000000012, w1=7.367339075106077\n",
      "SubSGD iter. 117/499: loss=9.665896734880434, w0=68.60000000000012, w1=8.425254381933636\n",
      "SubSGD iter. 118/499: loss=8.897646254530887, w0=69.30000000000013, w1=8.949434933385353\n",
      "SubSGD iter. 119/499: loss=8.434950822746611, w0=70.00000000000013, w1=8.424802008673614\n",
      "SubSGD iter. 120/499: loss=8.532550449591849, w0=69.30000000000013, w1=7.81939885722963\n",
      "SubSGD iter. 121/499: loss=9.031920468164643, w0=70.00000000000013, w1=8.26805789601434\n",
      "SubSGD iter. 122/499: loss=8.617572027046977, w0=69.30000000000013, w1=7.71966339225653\n",
      "SubSGD iter. 123/499: loss=9.087539335256023, w0=68.60000000000012, w1=8.503896120295854\n",
      "SubSGD iter. 124/499: loss=8.856450573550516, w0=67.90000000000012, w1=8.30513525447733\n",
      "SubSGD iter. 125/499: loss=9.181772666140512, w0=68.60000000000012, w1=8.834836228539446\n",
      "SubSGD iter. 126/499: loss=8.687331499077331, w0=69.30000000000013, w1=9.258377481593461\n",
      "SubSGD iter. 127/499: loss=8.278935571282677, w0=68.60000000000012, w1=10.066491222635394\n",
      "SubSGD iter. 128/499: loss=8.088812077918506, w0=69.30000000000013, w1=9.462725152668881\n",
      "SubSGD iter. 129/499: loss=8.176663345602423, w0=70.00000000000013, w1=10.519126384047771\n",
      "SubSGD iter. 130/499: loss=7.46691466842625, w0=70.70000000000013, w1=11.699931084747668\n",
      "SubSGD iter. 131/499: loss=6.737274498906757, w0=70.00000000000013, w1=12.58876290891416\n",
      "SubSGD iter. 132/499: loss=6.593401890100731, w0=69.30000000000013, w1=11.931580205719175\n",
      "SubSGD iter. 133/499: loss=7.061202934974981, w0=68.60000000000012, w1=12.371775460426147\n",
      "SubSGD iter. 134/499: loss=7.1899929531758735, w0=69.30000000000013, w1=11.693428464451603\n",
      "SubSGD iter. 135/499: loss=7.152398587133978, w0=68.60000000000012, w1=12.877430664845495\n",
      "SubSGD iter. 136/499: loss=7.051574757427522, w0=67.90000000000012, w1=13.28850584938772\n",
      "SubSGD iter. 137/499: loss=7.288940385774248, w0=68.60000000000012, w1=13.945688552582705\n",
      "SubSGD iter. 138/499: loss=6.7941378881332355, w0=69.30000000000013, w1=13.595351619214314\n",
      "SubSGD iter. 139/499: loss=6.551559711067015, w0=68.60000000000012, w1=14.431390476487131\n",
      "SubSGD iter. 140/499: loss=6.703744982764539, w0=69.30000000000013, w1=15.487791707866021\n",
      "SubSGD iter. 141/499: loss=6.205496931108905, w0=68.60000000000012, w1=14.293052870761596\n",
      "SubSGD iter. 142/499: loss=6.728041480696037, w0=69.30000000000013, w1=14.425376397115434\n",
      "SubSGD iter. 143/499: loss=6.359757187051851, w0=70.00000000000013, w1=14.0294001538936\n",
      "SubSGD iter. 144/499: loss=6.149659768448155, w0=70.70000000000013, w1=15.08731546072116\n",
      "SubSGD iter. 145/499: loss=5.671535448895827, w0=71.40000000000013, w1=15.615371276231809\n",
      "SubSGD iter. 146/499: loss=5.446803113749359, w0=72.10000000000014, w1=15.245103185703012\n",
      "SubSGD iter. 147/499: loss=5.374817252294419, w0=71.40000000000013, w1=15.527651004789238\n",
      "SubSGD iter. 148/499: loss=5.45169055401184, w0=70.70000000000013, w1=16.15043741160953\n",
      "SubSGD iter. 149/499: loss=5.610978846039656, w0=70.00000000000013, w1=16.65545514343385\n",
      "SubSGD iter. 150/499: loss=5.905214913939561, w0=70.70000000000013, w1=17.1835109589445\n",
      "SubSGD iter. 151/499: loss=5.696795414234371, w0=71.40000000000013, w1=16.743822553859314\n",
      "SubSGD iter. 152/499: loss=5.458759552177837, w0=72.10000000000014, w1=16.06547555788477\n",
      "SubSGD iter. 153/499: loss=5.338445913229364, w0=72.80000000000014, w1=16.60643260985591\n",
      "SubSGD iter. 154/499: loss=5.341674184921235, w0=73.50000000000014, w1=16.081799685144173\n",
      "SubSGD iter. 155/499: loss=5.360028041340268, w0=74.20000000000014, w1=16.24760594471167\n",
      "SubSGD iter. 156/499: loss=5.491141621647735, w0=74.90000000000015, w1=16.68753038759585\n",
      "SubSGD iter. 157/499: loss=5.700850278816659, w0=74.20000000000014, w1=17.24565409134489\n",
      "SubSGD iter. 158/499: loss=5.606195502853136, w0=73.50000000000014, w1=16.750027830876057\n",
      "SubSGD iter. 159/499: loss=5.418057891371896, w0=74.20000000000014, w1=17.62927579677223\n",
      "SubSGD iter. 160/499: loss=5.683993691327623, w0=74.90000000000015, w1=16.83490645228145\n",
      "SubSGD iter. 161/499: loss=5.720356771499135, w0=74.20000000000014, w1=16.70258292592761\n",
      "SubSGD iter. 162/499: loss=5.532340982075695, w0=73.50000000000014, w1=17.125822350700126\n",
      "SubSGD iter. 163/499: loss=5.472860829186377, w0=74.20000000000014, w1=17.843985674731314\n",
      "SubSGD iter. 164/499: loss=5.7324510463338765, w0=74.90000000000015, w1=18.283910117615495\n",
      "SubSGD iter. 165/499: loss=6.035493568787316, w0=74.20000000000014, w1=17.378735014428432\n",
      "SubSGD iter. 166/499: loss=5.630926947893896, w0=73.50000000000014, w1=17.948695138964418\n",
      "SubSGD iter. 167/499: loss=5.630909231185022, w0=74.20000000000014, w1=16.764692938570526\n",
      "SubSGD iter. 168/499: loss=5.538485995105768, w0=74.90000000000015, w1=17.6439409044667\n",
      "SubSGD iter. 169/499: loss=5.858676295621309, w0=75.60000000000015, w1=18.769070864597005\n",
      "SubSGD iter. 170/499: loss=6.4429061593160695, w0=76.30000000000015, w1=18.38694488004667\n",
      "SubSGD iter. 171/499: loss=6.625329170242416, w0=75.60000000000015, w1=17.180928590276224\n",
      "SubSGD iter. 172/499: loss=6.031312746936071, w0=76.30000000000015, w1=16.410105524208035\n",
      "SubSGD iter. 173/499: loss=6.226937310876193, w0=77.00000000000016, w1=13.036961846618357\n",
      "SubSGD iter. 174/499: loss=6.679403733378976, w0=76.30000000000015, w1=12.90463832026452\n",
      "SubSGD iter. 175/499: loss=6.407603249445106, w0=75.60000000000015, w1=13.031652765492545\n",
      "SubSGD iter. 176/499: loss=6.131452839684689, w0=74.90000000000015, w1=13.850311135298174\n",
      "SubSGD iter. 177/499: loss=5.780336704913036, w0=74.20000000000014, w1=14.355328867122497\n",
      "SubSGD iter. 178/499: loss=5.545928875515241, w0=73.50000000000014, w1=14.020100811819242\n",
      "SubSGD iter. 179/499: loss=5.504731077396614, w0=72.80000000000014, w1=15.109895834892463\n",
      "SubSGD iter. 180/499: loss=5.351542662550371, w0=73.50000000000014, w1=14.582707196145236\n",
      "SubSGD iter. 181/499: loss=5.425265512022052, w0=72.80000000000014, w1=15.418746053418053\n",
      "SubSGD iter. 182/499: loss=5.332623211914714, w0=73.50000000000014, w1=15.69687302221974\n",
      "SubSGD iter. 183/499: loss=5.363605394492854, w0=74.20000000000014, w1=12.323729344630063\n",
      "SubSGD iter. 184/499: loss=6.08154204399861, w0=74.90000000000015, w1=11.987366386953987\n",
      "SubSGD iter. 185/499: loss=6.282424585545489, w0=74.20000000000014, w1=11.652138331650733\n",
      "SubSGD iter. 186/499: loss=6.358661673076767, w0=73.50000000000014, w1=11.423016639363974\n",
      "SubSGD iter. 187/499: loss=6.44643136296232, w0=74.20000000000014, w1=12.15498123287161\n",
      "SubSGD iter. 188/499: loss=6.1481109984801, w0=74.90000000000015, w1=12.243508746360735\n",
      "SubSGD iter. 189/499: loss=6.182465953822422, w0=74.20000000000014, w1=12.257263942336225\n",
      "SubSGD iter. 190/499: loss=6.1075296383907345, w0=73.50000000000014, w1=13.028087008404412\n",
      "SubSGD iter. 191/499: loss=5.783353261620669, w0=72.80000000000014, w1=13.571103469769511\n",
      "SubSGD iter. 192/499: loss=5.622558989597783, w0=73.50000000000014, w1=14.124309385845624\n",
      "SubSGD iter. 193/499: loss=5.486067898154278, w0=72.80000000000014, w1=14.244201199333956\n",
      "SubSGD iter. 194/499: loss=5.467819997295656, w0=72.10000000000014, w1=14.444950736383207\n",
      "SubSGD iter. 195/499: loss=5.4884162152471925, w0=72.80000000000014, w1=14.482238048001078\n",
      "SubSGD iter. 196/499: loss=5.426840361708593, w0=72.10000000000014, w1=15.318276905273894\n",
      "SubSGD iter. 197/499: loss=5.3677394247855075, w0=72.80000000000014, w1=14.464430634848625\n",
      "SubSGD iter. 198/499: loss=5.429754190285036, w0=73.50000000000014, w1=15.19639522835626\n",
      "SubSGD iter. 199/499: loss=5.380419407293247, w0=72.80000000000014, w1=16.032434085629077\n",
      "SubSGD iter. 200/499: loss=5.313127183092, w0=72.10000000000014, w1=16.559391121397926\n",
      "SubSGD iter. 201/499: loss=5.3565537120541356, w0=72.80000000000014, w1=17.174052102528314\n",
      "SubSGD iter. 202/499: loss=5.422369214623035, w0=73.50000000000014, w1=16.60104201951369\n",
      "SubSGD iter. 203/499: loss=5.401578031421478, w0=72.80000000000014, w1=17.032834597797113\n",
      "SubSGD iter. 204/499: loss=5.401094420589944, w0=72.10000000000014, w1=17.85149296760274\n",
      "SubSGD iter. 205/499: loss=5.563685990035031, w0=72.80000000000014, w1=18.37567351905446\n",
      "SubSGD iter. 206/499: loss=5.679937635317296, w0=72.10000000000014, w1=18.902630554823308\n",
      "SubSGD iter. 207/499: loss=5.875684765739019, w0=71.40000000000013, w1=19.460754258572347\n",
      "SubSGD iter. 208/499: loss=6.164186293355709, w0=72.10000000000014, w1=19.805222505442813\n",
      "SubSGD iter. 209/499: loss=6.203027337503757, w0=71.40000000000013, w1=18.61048366833839\n",
      "SubSGD iter. 210/499: loss=5.875198115965885, w0=72.10000000000014, w1=18.888610637140076\n",
      "SubSGD iter. 211/499: loss=5.871036986148106, w0=72.80000000000014, w1=17.677187749213896\n",
      "SubSGD iter. 212/499: loss=5.509217289887055, w0=73.50000000000014, w1=18.55643571511007\n",
      "SubSGD iter. 213/499: loss=5.78095890292879, w0=74.20000000000014, w1=19.288400308617703\n",
      "SubSGD iter. 214/499: loss=6.1521189602413875, w0=74.90000000000015, w1=19.264434468814613\n",
      "SubSGD iter. 215/499: loss=6.354877636159846, w0=75.60000000000015, w1=18.48020174077529\n",
      "SubSGD iter. 216/499: loss=6.350873303734378, w0=74.90000000000015, w1=19.03217317271297\n",
      "SubSGD iter. 217/499: loss=6.277152962364219, w0=74.20000000000014, w1=19.379257158332962\n",
      "SubSGD iter. 218/499: loss=6.187449019125015, w0=74.90000000000015, w1=18.979799385946382\n",
      "SubSGD iter. 219/499: loss=6.259979152870879, w0=75.60000000000015, w1=18.626683634379066\n",
      "SubSGD iter. 220/499: loss=6.397300897057801, w0=74.90000000000015, w1=19.17384593661294\n",
      "SubSGD iter. 221/499: loss=6.324331219714121, w0=75.60000000000015, w1=17.989843736219047\n",
      "SubSGD iter. 222/499: loss=6.210062980821906, w0=74.90000000000015, w1=18.024534595560517\n",
      "SubSGD iter. 223/499: loss=5.958071633344784, w0=74.20000000000014, w1=18.124811694800727\n",
      "SubSGD iter. 224/499: loss=5.798555022306173, w0=74.90000000000015, w1=17.270965424375458\n",
      "SubSGD iter. 225/499: loss=5.787556411724668, w0=74.20000000000014, w1=17.666402216318463\n",
      "SubSGD iter. 226/499: loss=5.692372663497715, w0=74.90000000000015, w1=18.309331915082257\n",
      "SubSGD iter. 227/499: loss=6.043597661362969, w0=74.20000000000014, w1=18.861303347019938\n",
      "SubSGD iter. 228/499: loss=6.00394884066049, w0=74.90000000000015, w1=18.511365415539622\n",
      "SubSGD iter. 229/499: loss=6.108002949511073, w0=74.20000000000014, w1=18.69552619385794\n",
      "SubSGD iter. 230/499: loss=5.951732405311776, w0=73.50000000000014, w1=19.08081033418607\n",
      "SubSGD iter. 231/499: loss=5.945295941351814, w0=74.20000000000014, w1=18.825378453084497\n",
      "SubSGD iter. 232/499: loss=5.992356161326535, w0=73.50000000000014, w1=17.94620614085369\n",
      "SubSGD iter. 233/499: loss=5.630389771501882, w0=74.20000000000014, w1=18.060026467668003\n",
      "SubSGD iter. 234/499: loss=5.782799886271252, w0=73.50000000000014, w1=17.314040814080606\n",
      "SubSGD iter. 235/499: loss=5.50639407515801, w0=74.20000000000014, w1=17.290074974277516\n",
      "SubSGD iter. 236/499: loss=5.61387890997492, w0=74.90000000000015, w1=17.831032026248657\n",
      "SubSGD iter. 237/499: loss=5.904594197792752, w0=75.60000000000015, w1=17.996484818181283\n",
      "SubSGD iter. 238/499: loss=6.211731987068813, w0=74.90000000000015, w1=18.73059914118227\n",
      "SubSGD iter. 239/499: loss=6.1782642761455, w0=74.20000000000014, w1=19.077683126802263\n",
      "SubSGD iter. 240/499: loss=6.078205405620881, w0=73.50000000000014, w1=19.690481297803675\n",
      "SubSGD iter. 241/499: loss=6.182666923538379, w0=72.80000000000014, w1=20.26044142233966\n",
      "SubSGD iter. 242/499: loss=6.377821449243778, w0=72.10000000000014, w1=20.20828107680995\n",
      "SubSGD iter. 243/499: loss=6.368229770683623, w0=71.40000000000013, w1=20.75544337904382\n",
      "SubSGD iter. 244/499: loss=6.696432163900174, w0=70.70000000000013, w1=20.400415901513995\n",
      "SubSGD iter. 245/499: loss=6.698451715003449, w0=70.00000000000013, w1=19.654430247926598\n",
      "SubSGD iter. 246/499: loss=6.6180718396793266, w0=70.70000000000013, w1=18.80058397750133\n",
      "SubSGD iter. 247/499: loss=6.095961173540328, w0=71.40000000000013, w1=19.029705669788086\n",
      "SubSGD iter. 248/499: loss=6.007995433152667, w0=72.10000000000014, w1=19.195158461720712\n",
      "SubSGD iter. 249/499: loss=5.973941312872869, w0=72.80000000000014, w1=19.53962670859118\n",
      "SubSGD iter. 250/499: loss=6.069844820635156, w0=72.10000000000014, w1=18.706289126031603\n",
      "SubSGD iter. 251/499: loss=5.8116117363163955, w0=72.80000000000014, w1=17.96695632658766\n",
      "SubSGD iter. 252/499: loss=5.573768272652329, w0=72.10000000000014, w1=18.08684814007599\n",
      "SubSGD iter. 253/499: loss=5.625417794249676, w0=71.40000000000013, w1=17.181673036888927\n",
      "SubSGD iter. 254/499: loss=5.506410533724742, w0=72.10000000000014, w1=17.425985760598063\n",
      "SubSGD iter. 255/499: loss=5.472724627751485, w0=72.80000000000014, w1=17.87616550680436\n",
      "SubSGD iter. 256/499: loss=5.552874808338485, w0=72.10000000000014, w1=18.221342594529357\n",
      "SubSGD iter. 257/499: loss=5.662344793671647, w0=71.40000000000013, w1=18.616779386472363\n",
      "SubSGD iter. 258/499: loss=5.877108798951604, w0=70.70000000000013, w1=18.024461425204183\n",
      "SubSGD iter. 259/499: loss=5.867883565690218, w0=70.00000000000013, w1=18.529479157028504\n",
      "SubSGD iter. 260/499: loss=6.243342245925877, w0=70.70000000000013, w1=18.54478682374064\n",
      "SubSGD iter. 261/499: loss=6.016029975163995, w0=70.00000000000013, w1=17.930872410363058\n",
      "SubSGD iter. 262/499: loss=6.086772728459903, w0=70.70000000000013, w1=17.309820836792465\n",
      "SubSGD iter. 263/499: loss=5.716707299044101, w0=71.40000000000013, w1=17.737337922009882\n",
      "SubSGD iter. 264/499: loss=5.628595331291686, w0=70.70000000000013, w1=17.402109866706628\n",
      "SubSGD iter. 265/499: loss=5.733139279119498, w0=71.40000000000013, w1=16.42531238776471\n",
      "SubSGD iter. 266/499: loss=5.442585962598965, w0=72.10000000000014, w1=16.773279434155125\n",
      "SubSGD iter. 267/499: loss=5.374234559936649, w0=71.40000000000013, w1=17.453878611811206\n",
      "SubSGD iter. 268/499: loss=5.560844927550714, w0=72.10000000000014, w1=16.242455723885026\n",
      "SubSGD iter. 269/499: loss=5.341882884759904, w0=71.40000000000013, w1=16.789618026118898\n",
      "SubSGD iter. 270/499: loss=5.4617717882687575, w0=70.70000000000013, w1=17.185054818061904\n",
      "SubSGD iter. 271/499: loss=5.697029739002599, w0=71.40000000000013, w1=17.799715799192292\n",
      "SubSGD iter. 272/499: loss=5.644226199418852, w0=70.70000000000013, w1=17.919607612680622\n",
      "SubSGD iter. 273/499: loss=5.841927068090757, w0=70.00000000000013, w1=18.428108730420455\n",
      "SubSGD iter. 274/499: loss=6.213331307055698, w0=70.70000000000013, w1=18.624453323937683\n",
      "SubSGD iter. 275/499: loss=6.0409150607389694, w0=70.00000000000013, w1=19.102501526363806\n",
      "SubSGD iter. 276/499: loss=6.418898884009562, w0=70.70000000000013, w1=18.20086926195105\n",
      "SubSGD iter. 277/499: loss=5.9149489938623026, w0=71.40000000000013, w1=18.39738934472955\n",
      "SubSGD iter. 278/499: loss=5.81052627237179, w0=72.10000000000014, w1=17.967057756268815\n",
      "SubSGD iter. 279/499: loss=5.593455374706132, w0=72.80000000000014, w1=18.685221080300003\n",
      "SubSGD iter. 280/499: loss=5.770640617929279, w0=73.50000000000014, w1=18.838114837414587\n",
      "SubSGD iter. 281/499: loss=5.864188988716864, w0=74.20000000000014, w1=19.30469696338116\n",
      "SubSGD iter. 282/499: loss=6.158334604037314, w0=73.50000000000014, w1=18.75630245962335\n",
      "SubSGD iter. 283/499: loss=5.839023700856362, w0=74.20000000000014, w1=17.77950498068143\n",
      "SubSGD iter. 284/499: loss=5.7178985538620575, w0=74.90000000000015, w1=18.792183322034198\n",
      "SubSGD iter. 285/499: loss=6.198458252083655, w0=75.60000000000015, w1=18.41659748374763\n",
      "SubSGD iter. 286/499: loss=6.3310493657233184, w0=74.90000000000015, w1=18.763681469367622\n",
      "SubSGD iter. 287/499: loss=6.189112252526957, w0=74.20000000000014, w1=19.114018402736015\n",
      "SubSGD iter. 288/499: loss=6.09080805268021, w0=74.90000000000015, w1=20.29482310343591\n",
      "SubSGD iter. 289/499: loss=6.749893837213385, w0=74.20000000000014, w1=20.09238621339491\n",
      "SubSGD iter. 290/499: loss=6.479700265448945, w0=74.90000000000015, w1=19.05487201958137\n",
      "SubSGD iter. 291/499: loss=6.2845961070345435, w0=74.20000000000014, w1=19.073451478480248\n",
      "SubSGD iter. 292/499: loss=6.076737686746837, w0=73.50000000000014, w1=18.42139458103779\n",
      "SubSGD iter. 293/499: loss=5.743267032978355, w0=72.80000000000014, w1=17.33751918434649\n",
      "SubSGD iter. 294/499: loss=5.447876740431591, w0=73.50000000000014, w1=18.34995752221291\n",
      "SubSGD iter. 295/499: loss=5.723783791725913, w0=74.20000000000014, w1=17.373160043270993\n",
      "SubSGD iter. 296/499: loss=5.629787935531318, w0=74.90000000000015, w1=16.942828454810257\n",
      "SubSGD iter. 297/499: loss=5.735963115201853, w0=74.20000000000014, w1=16.101638286585615\n",
      "SubSGD iter. 298/499: loss=5.481612211362765, w0=73.50000000000014, w1=15.221908156753138\n",
      "SubSGD iter. 299/499: loss=5.37924149110227, w0=72.80000000000014, w1=15.846724256564084\n",
      "SubSGD iter. 300/499: loss=5.312771145509959, w0=72.10000000000014, w1=16.41668438110007\n",
      "SubSGD iter. 301/499: loss=5.3492022923152795, w0=71.40000000000013, w1=15.46977211935764\n",
      "SubSGD iter. 302/499: loss=5.456165455035287, w0=70.70000000000013, w1=15.134544064054385\n",
      "SubSGD iter. 303/499: loss=5.663905458874765, w0=71.40000000000013, w1=15.345708458620589\n",
      "SubSGD iter. 304/499: loss=5.466293916271698, w0=72.10000000000014, w1=15.327128999721712\n",
      "SubSGD iter. 305/499: loss=5.367038022513617, w0=72.80000000000014, w1=16.33980734107448\n",
      "SubSGD iter. 306/499: loss=5.318799404423463, w0=72.10000000000014, w1=17.02040651873056\n",
      "SubSGD iter. 307/499: loss=5.4053171359601935, w0=71.40000000000013, w1=16.401273122785895\n",
      "SubSGD iter. 308/499: loss=5.44165908057956, w0=72.10000000000014, w1=15.970941534325158\n",
      "SubSGD iter. 309/499: loss=5.338534903035549, w0=72.80000000000014, w1=16.58732848316241\n",
      "SubSGD iter. 310/499: loss=5.339590961567471, w0=73.50000000000014, w1=17.305491807193597\n",
      "SubSGD iter. 311/499: loss=5.504870972984045, w0=74.20000000000014, w1=16.278559609601064\n",
      "SubSGD iter. 312/499: loss=5.493412700073134, w0=74.90000000000015, w1=16.023127728499492\n",
      "SubSGD iter. 313/499: loss=5.640046544026854, w0=75.60000000000015, w1=16.576333644575605\n",
      "SubSGD iter. 314/499: loss=5.9391975720023575, w0=74.90000000000015, w1=15.830347990988209\n",
      "SubSGD iter. 315/499: loss=5.628873567511959, w0=75.60000000000015, w1=16.296930116954783\n",
      "SubSGD iter. 316/499: loss=5.9078226491542924, w0=74.90000000000015, w1=16.70800530149701\n",
      "SubSGD iter. 317/499: loss=5.703444617311831, w0=74.20000000000014, w1=17.119080486039238\n",
      "SubSGD iter. 318/499: loss=5.5851938718084355, w0=73.50000000000014, w1=16.472933252652016\n",
      "SubSGD iter. 319/499: loss=5.387470102355797, w0=74.20000000000014, w1=16.751060221453702\n",
      "SubSGD iter. 320/499: loss=5.537137207353947, w0=73.50000000000014, w1=16.490348000071837\n",
      "SubSGD iter. 321/499: loss=5.3893878947701195, w0=72.80000000000014, w1=16.91358742484435\n",
      "SubSGD iter. 322/499: loss=5.38334582809617, w0=72.10000000000014, w1=16.07239725661971\n",
      "SubSGD iter. 323/499: loss=5.338499673101973, w0=71.40000000000013, w1=16.222853933070837\n",
      "SubSGD iter. 324/499: loss=5.435888754684718, w0=70.70000000000013, w1=17.062881504530647\n",
      "SubSGD iter. 325/499: loss=5.678486445087931, w0=70.00000000000013, w1=17.50307675923762\n",
      "SubSGD iter. 326/499: loss=6.0056293255885285, w0=70.70000000000013, w1=17.747389482946755\n",
      "SubSGD iter. 327/499: loss=5.80241600416867, w0=71.40000000000013, w1=17.949826372987754\n",
      "SubSGD iter. 328/499: loss=5.683798659963322, w0=70.70000000000013, w1=18.36090155752998\n",
      "SubSGD iter. 329/499: loss=5.959950031598127, w0=70.00000000000013, w1=18.640830549966445\n",
      "SubSGD iter. 330/499: loss=6.276608950077503, w0=70.70000000000013, w1=18.056518012420554\n",
      "SubSGD iter. 331/499: loss=5.876190286118504, w0=70.00000000000013, w1=18.46759319696278\n",
      "SubSGD iter. 332/499: loss=6.224853551080423, w0=70.70000000000013, w1=18.26684365991353\n",
      "SubSGD iter. 333/499: loss=5.933264956491654, w0=70.00000000000013, w1=17.738787844402882\n",
      "SubSGD iter. 334/499: loss=6.045721844538048, w0=70.70000000000013, w1=18.463808034483687\n",
      "SubSGD iter. 335/499: loss=5.990734975440991, w0=70.00000000000013, w1=17.10105663909194\n",
      "SubSGD iter. 336/499: loss=5.9482072818379725, w0=70.70000000000013, w1=17.127429289716762\n",
      "SubSGD iter. 337/499: loss=5.688283417255186, w0=71.40000000000013, w1=16.225797025304004\n",
      "SubSGD iter. 338/499: loss=5.435970537813892, w0=70.70000000000013, w1=16.66548543038919\n",
      "SubSGD iter. 339/499: loss=5.630285963022149, w0=71.40000000000013, w1=16.092475347374567\n",
      "SubSGD iter. 340/499: loss=5.43418839421267, w0=70.70000000000013, w1=15.435292644179581\n",
      "SubSGD iter. 341/499: loss=5.635234000714275, w0=70.00000000000013, w1=14.829889492735596\n",
      "SubSGD iter. 342/499: loss=5.967434120152754, w0=69.30000000000013, w1=15.638003233777528\n",
      "SubSGD iter. 343/499: loss=6.198599757055027, w0=70.00000000000013, w1=16.72187863046883\n",
      "SubSGD iter. 344/499: loss=5.911292009200555, w0=70.70000000000013, w1=16.521129093419578\n",
      "SubSGD iter. 345/499: loss=5.617844140497796, w0=71.40000000000013, w1=15.97811263205448\n",
      "SubSGD iter. 346/499: loss=5.435674812072652, w0=70.70000000000013, w1=15.971684751176133\n",
      "SubSGD iter. 347/499: loss=5.613290604395818, w0=71.40000000000013, w1=15.936993891834662\n",
      "SubSGD iter. 348/499: loss=5.436388521209126, w0=70.70000000000013, w1=16.36878647011808\n",
      "SubSGD iter. 349/499: loss=5.612129725794148, w0=71.40000000000013, w1=15.18478426972419\n",
      "SubSGD iter. 350/499: loss=5.4826172695688395, w0=72.10000000000014, w1=15.532751316114604\n",
      "SubSGD iter. 351/499: loss=5.351972366898637, w0=71.40000000000013, w1=15.653255867593646\n",
      "SubSGD iter. 352/499: loss=5.4450613672762564, w0=72.10000000000014, w1=15.86442026215985\n",
      "SubSGD iter. 353/499: loss=5.3410459278651885, w0=72.80000000000014, w1=16.5275645792029\n",
      "SubSGD iter. 354/499: loss=5.333073963306412, w0=72.10000000000014, w1=16.812739073752194\n",
      "SubSGD iter. 355/499: loss=5.378682033972003, w0=72.80000000000014, w1=16.780310065984835\n",
      "SubSGD iter. 356/499: loss=5.36423295495181, w0=72.10000000000014, w1=16.647986539630995\n",
      "SubSGD iter. 357/499: loss=5.3623364546293795, w0=71.40000000000013, w1=17.466644909436624\n",
      "SubSGD iter. 358/499: loss=5.56361206855319, w0=72.10000000000014, w1=17.69576660172338\n",
      "SubSGD iter. 359/499: loss=5.527572872213687, w0=71.40000000000013, w1=17.65344004511989\n",
      "SubSGD iter. 360/499: loss=5.607571905855427, w0=72.10000000000014, w1=17.99715266630328\n",
      "SubSGD iter. 361/499: loss=5.601485285873252, w0=71.40000000000013, w1=17.163815083743703\n",
      "SubSGD iter. 362/499: loss=5.5031633221035126, w0=72.10000000000014, w1=18.04298739597451\n",
      "SubSGD iter. 363/499: loss=5.613714889058888, w0=71.40000000000013, w1=17.514931580463863\n",
      "SubSGD iter. 364/499: loss=5.574153003936206, w0=72.10000000000014, w1=17.144663489935066\n",
      "SubSGD iter. 365/499: loss=5.42323516753007, w0=71.40000000000013, w1=17.570449637225806\n",
      "SubSGD iter. 366/499: loss=5.587268974608953, w0=72.10000000000014, w1=18.695579597356115\n",
      "SubSGD iter. 367/499: loss=5.808154218182976, w0=72.80000000000014, w1=18.721952247980937\n",
      "SubSGD iter. 368/499: loss=5.7818187498789895, w0=73.50000000000014, w1=19.262909299952078\n",
      "SubSGD iter. 369/499: loss=6.010985015941786, w0=72.80000000000014, w1=18.528633556460278\n",
      "SubSGD iter. 370/499: loss=5.723689843304101, w0=73.50000000000014, w1=17.687215046734334\n",
      "SubSGD iter. 371/499: loss=5.57709223252569, w0=72.80000000000014, w1=17.807106860222664\n",
      "SubSGD iter. 372/499: loss=5.536982514697334, w0=73.50000000000014, w1=18.33128741167438\n",
      "SubSGD iter. 373/499: loss=5.718980880302457, w0=72.80000000000014, w1=17.497949829114805\n",
      "SubSGD iter. 374/499: loss=5.47542675021297, w0=73.50000000000014, w1=16.286526941188626\n",
      "SubSGD iter. 375/499: loss=5.370892580003443, w0=72.80000000000014, w1=16.436983617639754\n",
      "SubSGD iter. 376/499: loss=5.323401834491749, w0=72.10000000000014, w1=16.557488169118795\n",
      "SubSGD iter. 377/499: loss=5.356453880758731, w0=72.80000000000014, w1=15.818155369674852\n",
      "SubSGD iter. 378/499: loss=5.3129890212480575, w0=72.10000000000014, w1=15.938659921153894\n",
      "SubSGD iter. 379/499: loss=5.339295877163685, w0=72.80000000000014, w1=15.602296963477817\n",
      "SubSGD iter. 380/499: loss=5.32259218293181, w0=72.10000000000014, w1=16.2828961411339\n",
      "SubSGD iter. 381/499: loss=5.343349970664509, w0=71.40000000000013, w1=16.363710446622612\n",
      "SubSGD iter. 382/499: loss=5.440210776350328, w0=72.10000000000014, w1=16.25353632974187\n",
      "SubSGD iter. 383/499: loss=5.342264561449231, w0=72.80000000000014, w1=16.720118455708445\n",
      "SubSGD iter. 384/499: loss=5.355833361227693, w0=73.50000000000014, w1=15.866272185283176\n",
      "SubSGD iter. 385/499: loss=5.36171060838936, w0=74.20000000000014, w1=12.493128507693498\n",
      "SubSGD iter. 386/499: loss=6.015914594370419, w0=74.90000000000015, w1=13.106121004877059\n",
      "SubSGD iter. 387/499: loss=5.92398574516945, w0=74.20000000000014, w1=13.386049997313522\n",
      "SubSGD iter. 388/499: loss=5.723962103384687, w0=73.50000000000014, w1=14.28768226172628\n",
      "SubSGD iter. 389/499: loss=5.460953664017946, w0=72.80000000000014, w1=14.245355705122789\n",
      "SubSGD iter. 390/499: loss=5.467613810032426, w0=72.10000000000014, w1=14.792518007356659\n",
      "SubSGD iter. 391/499: loss=5.435330135897773, w0=72.80000000000014, w1=15.322218981418775\n",
      "SubSGD iter. 392/499: loss=5.337898401542181, w0=73.50000000000014, w1=15.945707103704953\n",
      "SubSGD iter. 393/499: loss=5.3608221023493305, w0=74.20000000000014, w1=16.395886849911246\n",
      "SubSGD iter. 394/499: loss=5.502174478786242, w0=73.50000000000014, w1=16.23008059034375\n",
      "SubSGD iter. 395/499: loss=5.3670779409926945, w0=72.80000000000014, w1=16.78820429409279\n",
      "SubSGD iter. 396/499: loss=5.365334575403031, w0=73.50000000000014, w1=16.38874652170621\n",
      "SubSGD iter. 397/499: loss=5.378760147954944, w0=72.80000000000014, w1=15.774832108328628\n",
      "SubSGD iter. 398/499: loss=5.314421287448487, w0=73.50000000000014, w1=15.790139775040764\n",
      "SubSGD iter. 399/499: loss=5.362562174774602, w0=72.80000000000014, w1=15.940596451491892\n",
      "SubSGD iter. 400/499: loss=5.312909978053523, w0=73.50000000000014, w1=16.481553503463033\n",
      "SubSGD iter. 401/499: loss=5.388419404186963, w0=72.80000000000014, w1=15.747277759971233\n",
      "SubSGD iter. 402/499: loss=5.315641286849757, w0=72.10000000000014, w1=16.404158727724198\n",
      "SubSGD iter. 403/499: loss=5.348610779624305, w0=72.80000000000014, w1=17.237496310283774\n",
      "SubSGD iter. 404/499: loss=5.432269096291723, w0=73.50000000000014, w1=17.218916851384897\n",
      "SubSGD iter. 405/499: loss=5.4894466663779085, w0=74.20000000000014, w1=17.831909348568455\n",
      "SubSGD iter. 406/499: loss=5.729725569380476, w0=73.50000000000014, w1=17.699585822214615\n",
      "SubSGD iter. 407/499: loss=5.579560982078343, w0=72.80000000000014, w1=18.251557254152296\n",
      "SubSGD iter. 408/499: loss=5.6460855212167305, w0=73.50000000000014, w1=17.554972502911532\n",
      "SubSGD iter. 409/499: loss=5.551158687077471, w0=72.80000000000014, w1=16.94956935146755\n",
      "SubSGD iter. 410/499: loss=5.388701330763129, w0=72.10000000000014, w1=16.35725139019937\n",
      "SubSGD iter. 411/499: loss=5.346395622888515, w0=72.80000000000014, w1=16.55968828024037\n",
      "SubSGD iter. 412/499: loss=5.336576915589144, w0=72.10000000000014, w1=16.57344347621586\n",
      "SubSGD iter. 413/499: loss=5.3572981380095195, w0=71.40000000000013, w1=15.210692080824114\n",
      "SubSGD iter. 414/499: loss=5.47989411245078, w0=72.10000000000014, w1=15.827079029661363\n",
      "SubSGD iter. 415/499: loss=5.341926172346568, w0=72.80000000000014, w1=15.396747441200626\n",
      "SubSGD iter. 416/499: loss=5.333825432686404, w0=73.50000000000014, w1=16.01066185457821\n",
      "SubSGD iter. 417/499: loss=5.360095561809903, w0=72.80000000000014, w1=15.811900988759685\n",
      "SubSGD iter. 418/499: loss=5.313097498735551, w0=72.10000000000014, w1=15.207368882905525\n",
      "SubSGD iter. 419/499: loss=5.378791953026319, w0=71.40000000000013, w1=15.685417085331649\n",
      "SubSGD iter. 420/499: loss=5.443600769649071, w0=72.10000000000014, w1=16.3018040341689\n",
      "SubSGD iter. 421/499: loss=5.344048980650221, w0=71.40000000000013, w1=16.545830858878052\n",
      "SubSGD iter. 422/499: loss=5.447884705484319, w0=72.10000000000014, w1=17.15882335606161\n",
      "SubSGD iter. 423/499: loss=5.425507623379354, w0=72.80000000000014, w1=16.419490556617667\n",
      "SubSGD iter. 424/499: loss=5.322326351740926, w0=72.10000000000014, w1=16.845276703908407\n",
      "SubSGD iter. 425/499: loss=5.382768644591082, w0=71.40000000000013, w1=17.230560844236535\n",
      "SubSGD iter. 426/499: loss=5.5153000774963346, w0=70.70000000000013, w1=16.02454455446609\n",
      "SubSGD iter. 427/499: loss=5.612595531902733, w0=71.40000000000013, w1=16.29002825669032\n",
      "SubSGD iter. 428/499: loss=5.437755405795344, w0=70.70000000000013, w1=16.569957249126784\n",
      "SubSGD iter. 429/499: loss=5.621796940662669, w0=70.00000000000013, w1=16.595255014515626\n",
      "SubSGD iter. 430/499: loss=5.89970719469008, w0=69.30000000000013, w1=16.099628754046794\n",
      "SubSGD iter. 431/499: loss=6.190324191129616, w0=68.60000000000012, w1=16.635148436873\n",
      "SubSGD iter. 432/499: loss=6.581002991332483, w0=69.30000000000013, w1=15.81649006706737\n",
      "SubSGD iter. 433/499: loss=6.1926759468454495, w0=70.00000000000013, w1=16.649827649626946\n",
      "SubSGD iter. 434/499: loss=5.904700053647341, w0=70.70000000000013, w1=16.815633909194442\n",
      "SubSGD iter. 435/499: loss=5.645173700514412, w0=71.40000000000013, w1=17.012153991972944\n",
      "SubSGD iter. 436/499: loss=5.481341060542533, w0=70.70000000000013, w1=17.42322917651517\n",
      "SubSGD iter. 437/499: loss=5.737062598162365, w0=70.00000000000013, w1=16.330604662189376\n",
      "SubSGD iter. 438/499: loss=5.876310288549128, w0=70.70000000000013, w1=16.79718678815595\n",
      "SubSGD iter. 439/499: loss=5.643119303623925, w0=70.00000000000013, w1=16.442159310626124\n",
      "SubSGD iter. 440/499: loss=5.885700444696731, w0=70.70000000000013, w1=16.09222137914581\n",
      "SubSGD iter. 441/499: loss=5.611705625086466, w0=71.40000000000013, w1=16.258027638713305\n",
      "SubSGD iter. 442/499: loss=5.4368661673799945, w0=72.10000000000014, w1=16.46919203327951\n",
      "SubSGD iter. 443/499: loss=5.351821753239402, w0=72.80000000000014, w1=16.557719546768634\n",
      "SubSGD iter. 444/499: loss=5.336362233619564, w0=72.10000000000014, w1=16.983505694059374\n",
      "SubSGD iter. 445/499: loss=5.400406053999847, w0=71.40000000000013, w1=16.32632299086439\n",
      "SubSGD iter. 446/499: loss=5.4387692280948325, w0=70.70000000000013, w1=14.963571595472644\n",
      "SubSGD iter. 447/499: loss=5.694978340861119, w0=70.00000000000013, w1=15.490528631241492\n",
      "SubSGD iter. 448/499: loss=5.874434633905839, w0=70.70000000000013, w1=15.940708377447786\n",
      "SubSGD iter. 449/499: loss=5.613697923832783, w0=71.40000000000013, w1=16.389367416232496\n",
      "SubSGD iter. 450/499: loss=5.441200032212549, w0=70.70000000000013, w1=17.229394987692306\n",
      "SubSGD iter. 451/499: loss=5.703803278856803, w0=70.00000000000013, w1=16.572212284497322\n",
      "SubSGD iter. 452/499: loss=5.897599011701723, w0=70.70000000000013, w1=17.451460250393495\n",
      "SubSGD iter. 453/499: loss=5.742307064518106, w0=71.40000000000013, w1=17.72958721919518\n",
      "SubSGD iter. 454/499: loss=5.626653132886392, w0=70.70000000000013, w1=16.771452128293376\n",
      "SubSGD iter. 455/499: loss=5.640322420487274, w0=70.00000000000013, w1=17.39423853511367\n",
      "SubSGD iter. 456/499: loss=5.988676634513773, w0=70.70000000000013, w1=17.02831780438327\n",
      "SubSGD iter. 457/499: loss=5.673240415230134, w0=71.40000000000013, w1=17.181211561497854\n",
      "SubSGD iter. 458/499: loss=5.506326621070216, w0=72.10000000000014, w1=18.037433614002392\n",
      "SubSGD iter. 459/499: loss=5.612233031301992, w0=72.80000000000014, w1=17.060636135060474\n",
      "SubSGD iter. 460/499: loss=5.4052323648562055, w0=73.50000000000014, w1=16.32130333561653\n",
      "SubSGD iter. 461/499: loss=5.373242765424682, w0=74.20000000000014, w1=14.967346441583302\n",
      "SubSGD iter. 462/499: loss=5.485859667080678, w0=74.90000000000015, w1=15.211659165292437\n",
      "SubSGD iter. 463/499: loss=5.628206650795933, w0=74.20000000000014, w1=16.006028509783217\n",
      "SubSGD iter. 464/499: loss=5.476372658029478, w0=73.50000000000014, w1=16.740142832784205\n",
      "SubSGD iter. 465/499: loss=5.416946127344891, w0=74.20000000000014, w1=16.344166589562374\n",
      "SubSGD iter. 466/499: loss=5.498226301217876, w0=73.50000000000014, w1=16.211843063208534\n",
      "SubSGD iter. 467/499: loss=5.366013584325557, w0=72.80000000000014, w1=16.292657368697245\n",
      "SubSGD iter. 468/499: loss=5.31765169594871, w0=72.10000000000014, w1=16.443114045148373\n",
      "SubSGD iter. 469/499: loss=5.350453668834317, w0=71.40000000000013, w1=15.71809385506757\n",
      "SubSGD iter. 470/499: loss=5.442349429395479, w0=72.10000000000014, w1=15.352173124337172\n",
      "SubSGD iter. 471/499: loss=5.36505363271679, w0=72.80000000000014, w1=15.111193209670525\n",
      "SubSGD iter. 472/499: loss=5.351442021051775, w0=72.10000000000014, w1=14.480027869675672\n",
      "SubSGD iter. 473/499: loss=5.482930804480454, w0=72.80000000000014, w1=15.660832570375568\n",
      "SubSGD iter. 474/499: loss=5.319479017896925, w0=72.10000000000014, w1=16.01116950374396\n",
      "SubSGD iter. 475/499: loss=5.3380241256502865, w0=71.40000000000013, w1=15.746384394553193\n",
      "SubSGD iter. 476/499: loss=5.44126605759526, w0=72.10000000000014, w1=16.758822732419613\n",
      "SubSGD iter. 477/499: loss=5.372698039701647, w0=72.80000000000014, w1=16.1550566624531\n",
      "SubSGD iter. 478/499: loss=5.314706154803757, w0=72.10000000000014, w1=15.208144400710669\n",
      "SubSGD iter. 479/499: loss=5.3787102647225975, w0=71.40000000000013, w1=15.45217122541982\n",
      "SubSGD iter. 480/499: loss=5.457526266628473, w0=72.10000000000014, w1=14.765476589913684\n",
      "SubSGD iter. 481/499: loss=5.439449272454406, w0=72.80000000000014, w1=15.015075052600862\n",
      "SubSGD iter. 482/499: loss=5.359470170360584, w0=73.50000000000014, w1=16.14020501273117\n",
      "SubSGD iter. 483/499: loss=5.362564412129469, w0=72.80000000000014, w1=16.420134005167633\n",
      "SubSGD iter. 484/499: loss=5.322363925094422, w0=72.10000000000014, w1=15.327509490841837\n",
      "SubSGD iter. 485/499: loss=5.3670078740173865, w0=72.80000000000014, w1=15.577107953529016\n",
      "SubSGD iter. 486/499: loss=5.32396875844846, w0=73.50000000000014, w1=15.842591655753246\n",
      "SubSGD iter. 487/499: loss=5.361975483005417, w0=74.20000000000014, w1=15.164244659778703\n",
      "SubSGD iter. 488/499: loss=5.477279159461389, w0=73.50000000000014, w1=14.206109568876895\n",
      "SubSGD iter. 489/499: loss=5.473019592584481, w0=72.80000000000014, w1=13.259197307134466\n",
      "SubSGD iter. 490/499: loss=5.717614360208616, w0=73.50000000000014, w1=14.343072703825769\n",
      "SubSGD iter. 491/499: loss=5.452915908278664, w0=72.80000000000014, w1=13.685890000630783\n",
      "SubSGD iter. 492/499: loss=5.59045488216688, w0=73.50000000000014, w1=14.591065103817847\n",
      "SubSGD iter. 493/499: loss=5.424444529763919, w0=72.80000000000014, w1=13.99874714254967\n",
      "SubSGD iter. 494/499: loss=5.515973977428892, w0=72.10000000000014, w1=14.10892125943041\n",
      "SubSGD iter. 495/499: loss=5.56283284375259, w0=71.40000000000013, w1=14.678881383966395\n",
      "SubSGD iter. 496/499: loss=5.552859328595756, w0=72.10000000000014, w1=14.928479846653573\n",
      "SubSGD iter. 497/499: loss=5.415898309248151, w0=72.80000000000014, w1=15.437665690599825\n",
      "SubSGD iter. 498/499: loss=5.33158925665481, w0=73.50000000000014, w1=16.232835415717172\n",
      "SubSGD iter. 499/499: loss=5.367264111828895, w0=74.20000000000014, w1=16.47714813942631\n",
      "SubSGD: execution time=0.016 seconds\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters of the algorithm.\n",
    "max_iters = 500\n",
    "gamma = 0.7\n",
    "batch_size = 1\n",
    "\n",
    "# Initialization\n",
    "w_initial = np.array([0, 0])\n",
    "\n",
    "# Start SubSGD.\n",
    "start_time = datetime.datetime.now()\n",
    "subsgd_losses, subsgd_ws = stochastic_subgradient_descent(\n",
    "    y, tx, w_initial, batch_size, max_iters, gamma\n",
    ")\n",
    "end_time = datetime.datetime.now()\n",
    "\n",
    "# Print result\n",
    "exection_time = (end_time - start_time).total_seconds()\n",
    "print(\"SubSGD: execution time={t:.3f} seconds\".format(t=exection_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1555ad91c04b45c7b99cc9fb6b8b75c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='n_iter', max=501, min=1), Output()), _dom_classes=('widg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_figure(n_iter)>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import IntSlider, interact\n",
    "\n",
    "\n",
    "def plot_figure(n_iter):\n",
    "    fig = gradient_descent_visualization(\n",
    "        subsgd_losses,\n",
    "        subsgd_ws,\n",
    "        grid_losses,\n",
    "        grid_w0,\n",
    "        grid_w1,\n",
    "        mean_x,\n",
    "        std_x,\n",
    "        height,\n",
    "        weight,\n",
    "        n_iter,\n",
    "    )\n",
    "    fig.set_size_inches(10.0, 6.0)\n",
    "\n",
    "\n",
    "interact(plot_figure, n_iter=IntSlider(min=1, max=len(subsgd_ws)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
